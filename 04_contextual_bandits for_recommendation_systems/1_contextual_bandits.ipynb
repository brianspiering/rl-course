{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Contextual-Bandits\" data-toc-modified-id=\"Contextual-Bandits-1\">Contextual Bandits</a></span></li><li><span><a href=\"#Imagine-you-are-running-a-ecommerce-website-…\" data-toc-modified-id=\"Imagine-you-are-running-a-ecommerce-website-…-2\">Imagine you are running a ecommerce website …</a></span></li><li><span><a href=\"#Why-is-context-useful?\" data-toc-modified-id=\"Why-is-context-useful?-3\">Why is context useful?</a></span></li><li><span><a href=\"#Common-Applied-Domains-for-contextual-bandits\" data-toc-modified-id=\"Common-Applied-Domains-for-contextual-bandits-4\">Common Applied Domains for contextual bandits</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-5\">Learning Outcomes</a></span></li><li><span><a href=\"#Model-for--Contextual-Bandits\" data-toc-modified-id=\"Model-for--Contextual-Bandits-6\">Model for  Contextual Bandits</a></span></li><li><span><a href=\"#Contextual-bandits-(CB)-extend-multi-armed-bandits-(MAB)-towards-full-Reinforcement-Learning-(RL)\" data-toc-modified-id=\"Contextual-bandits-(CB)-extend-multi-armed-bandits-(MAB)-towards-full-Reinforcement-Learning-(RL)-7\">Contextual bandits (CB) extend multi-armed bandits (MAB) towards full Reinforcement Learning (RL)</a></span></li><li><span><a href=\"#Example:-Watch-News\" data-toc-modified-id=\"Example:-Watch-News-8\">Example: Watch News</a></span></li><li><span><a href=\"#Naive-Solution-to-CB\" data-toc-modified-id=\"Naive-Solution-to-CB-9\">Naive Solution to CB</a></span></li><li><span><a href=\"#Building-better-contexts\" data-toc-modified-id=\"Building-better-contexts-10\">Building better contexts</a></span></li><li><span><a href=\"#Student-Activity\" data-toc-modified-id=\"Student-Activity-11\">Student Activity</a></span></li><li><span><a href=\"#Recommender-System-should-have-a-notion-of-time-base-state\" data-toc-modified-id=\"Recommender-System-should-have-a-notion-of-time-base-state-12\">Recommender System should have a notion of time-base state</a></span></li><li><span><a href=\"#Algorithms-for-contextual-bandits\" data-toc-modified-id=\"Algorithms-for-contextual-bandits-13\">Algorithms for contextual bandits</a></span></li><li><span><a href=\"#Contextual-Adaptive-Greedy-Algorithm\" data-toc-modified-id=\"Contextual-Adaptive-Greedy-Algorithm-14\">Contextual Adaptive Greedy Algorithm</a></span></li><li><span><a href=\"#Case-Study:-Accelerated-learning-from-recommender-systems-using-multi-armed-bandit\" data-toc-modified-id=\"Case-Study:-Accelerated-learning-from-recommender-systems-using-multi-armed-bandit-15\">Case Study: Accelerated learning from recommender systems using multi-armed bandit</a></span><ul class=\"toc-item\"><li><span><a href=\"#Metrics:\" data-toc-modified-id=\"Metrics:-15.1\">Metrics:</a></span></li><li><span><a href=\"#MAB-Arms:\" data-toc-modified-id=\"MAB-Arms:-15.2\">MAB Arms:</a></span></li><li><span><a href=\"#Algorithm:\" data-toc-modified-id=\"Algorithm:-15.3\">Algorithm:</a></span></li><li><span><a href=\"#Results:\" data-toc-modified-id=\"Results:-15.4\">Results:</a></span></li><li><span><a href=\"#Future-Directions:\" data-toc-modified-id=\"Future-Directions:-15.5\">Future Directions:</a></span></li></ul></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-16\">Takeaways</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-17\">Sources of Inspiration</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-18\">Bonus Material</a></span></li><li><span><a href=\"#Hierarchy-of-personalization\" data-toc-modified-id=\"Hierarchy-of-personalization-19\">Hierarchy of personalization</a></span></li><li><span><a href=\"#Contextual-MAB---your-preferences-keep-winning,-thus-creating-filter-bubbles\" data-toc-modified-id=\"Contextual-MAB---your-preferences-keep-winning,-thus-creating-filter-bubbles-20\">Contextual MAB - your preferences keep winning, thus creating filter bubbles</a></span></li><li><span><a href=\"#Since-contextual-bandits-track-state,-users-don't-have-to-see-the-same-items-over-and-over\" data-toc-modified-id=\"Since-contextual-bandits-track-state,-users-don't-have-to-see-the-same-items-over-and-over-21\">Since contextual bandits track state, users don't have to see the same items over-and-over</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Contextual Bandits</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Imagine you are running a ecommerce website …</h2></center>\n",
    "\n",
    "<center><img src=\"images/pizza.png\" width=\"55%\"/></center>\n",
    "\n",
    "You are running a food delivery website.\n",
    "\n",
    "A person comes to your website. What food are going to recommend? Pizza or Chinese or Thai or Indian?\n",
    "\n",
    "Would it help if you knew something about the person?\n",
    "\n",
    "Context matters!\n",
    "\n",
    "Context is also called side information or covariates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Why is context useful?</h2></center>\n",
    "\n",
    "\n",
    "- We often know a lot about users and their preferences.\n",
    "- Context is better model of the real-world.\n",
    "- Context helps in highly non-stationary environments. It puts the \"online\" in online learning algorithms. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Applied Domains for contextual bandits\n",
    "-----\n",
    "\n",
    "- E-commerce\n",
    "- News\n",
    "- Movies \n",
    "- Ads\n",
    "- Dynamic pricing\n",
    "\n",
    "These tend to be platform-based where the focus on is a relationship (beyond just one-time transactions) and maximizing lifetime value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Define contextual bandits in your own words.\n",
    "- Explain the connection between multi-armed bandit, contextual bandits, and full reinforcement learning.\n",
    "- Explain the connection between contextual bandits and recommender systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model for  Contextual Bandits\n",
    "------\n",
    "\n",
    "<center><img src=\"images/cb_model.png\" width=\"75%\"/></center>\n",
    "\n",
    "For each round:\n",
    "\n",
    "1. Environment knows about a context  $ x_t ∈ X $ (typically a vector feature)\n",
    "2. Agent chooses an action $ a_t ∈ A $\n",
    "2. Environment reveals a reward \n",
    "\n",
    "The __reward__ is a function of both current context and action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Contextual bandits (CB) extend multi-armed bandits (MAB) towards full Reinforcement Learning (RL)</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/1_3NziBtrANN6UVltplxwaGA.png\" width=\"95%\"/></center>\n",
    "\n",
    "MAB are stateless (or always in the same state).\n",
    "\n",
    "Contextual bandits are stateful (imagine a slot machines that change colors when it is \"hot\").\n",
    "\n",
    "MAB learn a reward distribution for every action.\n",
    "\n",
    "Contextual bandits condition this distribution on a set of features, or a context.\n",
    "\n",
    "State is used in many cases already for A/B testing. For example in medical studies, you might to control for pre-existing conditions or demographic information. This is a \"warm-start\" to recommending options.\n",
    "\n",
    "Reinforcement Learning is the generalization where the agent's action will then influence the future state of the environment.\n",
    "\n",
    "In contextual bandits, an agents actions will only influence the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Image source](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Example: Watch News</h2></center>\n",
    "\n",
    "News articles for smart watches.\n",
    "\n",
    "You get recommend one article (item) at a time to users.\n",
    "\n",
    "Users:   \n",
    "$u_{1 \\ldots n}$  \n",
    "\n",
    "Items:   \n",
    "$i_{1 \\ldots m}$   \n",
    "\n",
    "How many variations of Context are there?\n",
    "\n",
    "The Cartesian product: $ n \\cdot m $   \n",
    "$x_{(u_1,i_1), (u_2, i_1), \\ldots (u_n, i_2) \\ldots (u_n, i_m)}$  \n",
    "\n",
    "\n",
    "A particular user is served a particular article\n",
    "\n",
    "Arms / articles:   \n",
    "$a_{1 \\ldots k}$   \n",
    "\n",
    "Right now item there is a one-to-one mapping of item to arm.\n",
    "\n",
    "There is a click or no click. Your app gets a little bit of money for clicks. Your job as a data scientist is to maximize click-through rate (CTR).\n",
    "\n",
    "Reward $r_{t}$ is a result of $a_t$. Update excepted reward based on context $x_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Naive Solution to CB</h2></center>\n",
    "\n",
    "Each arm has independent state. Learn separate a reward function for each combination of context and action.\n",
    "\n",
    "Tracking separate state is not scalable. It is very sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Building better contexts</h2></center>\n",
    "\n",
    "What about sharing state across context?\n",
    "\n",
    "What if instead we model our News Watch problem as the following:\n",
    "\n",
    "There are personas:  \n",
    "$p_{1 \\ldots i}$  \n",
    "\n",
    "There are topics:\n",
    "\n",
    "$t_{1 \\ldots j}$  \n",
    "\n",
    "Now we bucket (cluster) similar users into the same persona and similar articles into the same topics. Share information across users and items\n",
    "\n",
    "How many variations of Context are there?\n",
    "\n",
    "The Cartesian product: $ i \\cdot j $   \n",
    "$x_{(p_1, t_1), (p_2, t_1), \\ldots (p_n, t_2) \\ldots (p_i, t_j)}$  \n",
    "\n",
    "But there will be fewer personas and topics. That number can be controlled by us!\n",
    "\n",
    "We have also decoupled arms from context. Previously, there was direct mapping from the item context to arm. Now each many are arms with the same \"item\" context.\n",
    "\n",
    "Users like this click on stories like this. Or these stories are currently trending for all users. \n",
    "\n",
    "What does this sound like?\n",
    "\n",
    "__Recommender Systems__\n",
    "\n",
    "People like this tend to select arm #1 / article #1. Those people like this tend to select arm #4 / article #4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "Think, pair, share\n",
    "\n",
    "What is the simplest way to recommend items to users?\n",
    "\n",
    "What are more complex ways?\n",
    "\n",
    "What are all the ways?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend items:\n",
    "\n",
    "- Overall count\n",
    "- Count by group / segment\n",
    "\n",
    "- Market basket analysis or apriori algorithm\n",
    "- Item-by-item / collaborative filtering\n",
    "- Content Based\n",
    "- [Factorization Machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) - Compute factors (dense submatrices) that can model higher order interactions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Nice overview of recommendation systems](https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Recommender System should have a notion of time-base state</h2></center>\n",
    "\n",
    "People don't have stable preferences.\n",
    "\n",
    "Think about YouTube, you watch one video about unclogging a drain and the algorithm thinks that are you a plumber!\n",
    "\n",
    "Think about Spotify, I listen to different music in the morning, afternoon, and evening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Algorithms for contextual bandits</h2></center>\n",
    "\n",
    "Any MAB algorithm can be contextual bandit by relaxing the no-state assumption. Thus all our previous algorithms still work if we add context:\n",
    "\n",
    "- Pure Random\n",
    "- Epsilon-Greedy Algorithm  \n",
    "- Softmax  \n",
    "- Upper Confidence Bound (UCB)\n",
    "- Bayesian Bandit \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Contextual Adaptive Greedy Algorithm</h2></center>\n",
    "\n",
    "I found through research one of the most powerful algorithms is a variation of one the simplest - Epsilon-Greedy Algorithm.\n",
    "\n",
    "__Contextual Adaptive Greedy Algorithm__ takes regular Epsilon-Greedy Algorithm and adds:\n",
    "\n",
    "- Context\n",
    "- Adaptive threshold\n",
    "\n",
    "Takes the action with highest estimated reward, unless that estimation falls below a certain threshold, in which case it takes an action at random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Contextual Adaptive Greedy Algorithm Pseudocode__\n",
    "\n",
    "for each round $t$ with context $x_t$:      \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; if $max(r_{x_t}) > threshold$:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;         select action $a = argmax(r_{x_t})$   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;     else:   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;         selection action $a$ uniformly at random from 1 to k  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;     then:   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;         obtain reward   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;         add ${x_t, r_{a}}$ to arm a   \n",
    "    \n",
    "Note: This assumes each arm has its own context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the [implementation in code](https://github.com/david-cortes/contextualbandits/blob/ecd090a3ac03c1896b0c45c443fb79589fe0a450/contextualbandits/online.py#L1512)\n",
    "\n",
    "Highlights:\n",
    "\n",
    "- Mostly documentation & tests\n",
    "- Minimal amount of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "    \n",
    "- [Mortal Multi-Armed Bandits](http://papers.nips.cc/paper/3580-mortal-multi-armed-bandits.pdf)\n",
    "- [Adapting multi-armed bandits policies to contextual\n",
    "bandits scenarios](https://arxiv.org/pdf/1811.04383.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Case Study: Accelerated learning from recommender systems using multi-armed bandit</h2></center>\n",
    "\n",
    "by Hejazinia et al.\n",
    "\n",
    "[Expedia Group](expediagroup.com)\n",
    "\n",
    "Business Problem: Short-term vacation rentals\n",
    "\n",
    "\n",
    "Bring together: Recommender Systems, Design of Experiments, and Reinforcement Learning\n",
    "\n",
    "__Architecture__\n",
    "\n",
    "<center><img src=\"images/arch.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Metrics:</h3>\n",
    "\n",
    "1. Click- through-rate (CTR)   \n",
    "CTR is measured as the proportion of users who clicked on recommendation items viewed. Measures user’s engagement on the platform. Useful to assess multi-armed bandit performance.\n",
    "\n",
    "2. Conversion-rate (CVR)    \n",
    "CVR is measured as the proportion of users who purchased on recommended items. Measures the conversion impact of the model. Useful to assess business performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>MAB Arms:</h3>\n",
    "\n",
    "1. A variant of content based recommender system which is built on a set of top of key item features.\n",
    "\n",
    "2. Includes a variant of probabilistic recommender system. It is based on the conditional probability that a user clicks on item j given they clicked on item i within their shopping session.\n",
    "\n",
    "3. A variant of session based embedding recommender systems. It leverages user session activity data and estimates the similarity between the items in the embedding space.\n",
    "\n",
    "4. A variant of the Matrix Factorization model. It creates a low dimensional representation for the items by using session co-view data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Algorithm:</h3>\n",
    "\n",
    "Thompson sampling (aka, Bayesian Bandits)\n",
    "\n",
    "\"due to its optimality and robustness to noise in the production environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Results:</h3>\n",
    "\n",
    "<center><img src=\"images/table_1.png\" width=\"75%\"/></center>\n",
    "\n",
    "- Some arms are worse than control. You don't know until you run the experiment.\n",
    "- CTR winner != CTV winner\n",
    "- Which model would you put into production at the end as the \"winner\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Future Directions:</h3>\n",
    "\n",
    "- Scale our MAB framework to treat __each item__ in our platform as a MAB problem. \n",
    "\n",
    "- Extend our framework to enable multi-objective optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Contextual bandits takes multi-armed bandits and extend them to include state information.\n",
    "- Over time, contextual bandits learn which states and which actions have the highest excepted reward.\n",
    "- Most multi-armed bandits algorithms can be adapted to include state information.\n",
    "- State can be model simply as additional, independent information per action or a shared representations across actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "-  John Langford's [Contextual Reinforcement Learning](http://bigdataieee.org/BigData2017/files/Keynote_Langford.pdf) Talk\n",
    "- [A complete course on online/adpative machine learning](https://courses.cs.washington.edu/courses/cse599i/18wi/)\n",
    "- [A 12 minute intro to CB with connection to advanced RL concepts](https://www.youtube.com/watch?v=K2Hh-ayvsJU&feature=emb_title)\n",
    "- [Python package for contextual bandits](https://github.com/david-cortes/contextualbandits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus Material\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Hierarchy of personalization</h2></center>\n",
    "\n",
    "1. None - Everyone gets the same\n",
    "2. Suggest most popular - SQL order by\n",
    "3. Suggest by persona \n",
    "    - SQL group by then order by (independent univariate attributes)\n",
    "    - Clustering (across attribute relationships)\n",
    "4. Nearest neighbors (exact or approximate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Contextual MAB - your preferences keep winning, thus creating filter bubbles</h2></center>\n",
    "\n",
    "[ Source](http://jamesmc.com/blog/2018/10/1/explore-exploit-explain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Since contextual bandits track state, users don't have to see the same items over-and-over</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
