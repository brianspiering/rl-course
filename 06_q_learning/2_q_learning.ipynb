{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Q-Learning\" data-toc-modified-id=\"Q-Learning-1\">Q-Learning</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Choose-your-own-adventure:-The-Knight-&amp;-The-Castle\" data-toc-modified-id=\"Choose-your-own-adventure:-The-Knight-&amp;-The-Castle-3\">Choose your own adventure: The Knight &amp; The Castle</a></span></li><li><span><a href=\"#The-Knight-&amp;-The-Castle\" data-toc-modified-id=\"The-Knight-&amp;-The-Castle-4\">The Knight &amp; The Castle</a></span></li><li><span><a href=\"#What-would-help-you-make-decision?-\" data-toc-modified-id=\"What-would-help-you-make-decision?--5\">What would help you make decision? </a></span></li><li><span><a href=\"#-RL-Loop\" data-toc-modified-id=\"-RL-Loop-6\"> RL Loop</a></span></li><li><span><a href=\"#-Q-learning\" data-toc-modified-id=\"-Q-learning-7\"> Q-learning</a></span></li><li><span><a href=\"#Q-value-\" data-toc-modified-id=\"Q-value--8\">Q-value </a></span></li><li><span><a href=\"#How-to-choose-in-$a$-in-$Q(s,-a)$-----------?\" data-toc-modified-id=\"How-to-choose-in-$a$-in-$Q(s,-a)$-----------?-9\">How to choose in $a$ in $Q(s, a)$           ?</a></span></li><li><span><a href=\"#How-are-Q's-learned,-aka-Q-learning?\" data-toc-modified-id=\"How-are-Q's-learned,-aka-Q-learning?-10\">How are Q's learned, aka Q-learning?</a></span></li><li><span><a href=\"#How-are-Q's-learned,-aka-Q-learning?\" data-toc-modified-id=\"How-are-Q's-learned,-aka-Q-learning?-11\">How are Q's learned, aka Q-learning?</a></span></li><li><span><a href=\"#Q-learning-Flow\" data-toc-modified-id=\"Q-learning-Flow-12\">Q-learning Flow</a></span></li><li><span><a href=\"#-Q-learning-Pseudocode\" data-toc-modified-id=\"-Q-learning-Pseudocode-13\"> Q-learning Pseudocode</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-14\">Check for understanding</a></span></li><li><span><a href=\"#Q-learning-Formalism\" data-toc-modified-id=\"Q-learning-Formalism-15\">Q-learning Formalism</a></span></li><li><span><a href=\"#Q-Learning-Graphical-Interpretation\" data-toc-modified-id=\"Q-Learning-Graphical-Interpretation-16\">Q-Learning Graphical Interpretation</a></span></li><li><span><a href=\"#$Œ±$:-Learning-Rate\" data-toc-modified-id=\"$Œ±$:-Learning-Rate-17\">$Œ±$: Learning Rate</a></span></li><li><span><a href=\"#$Œ±$:-Learning-Rate\" data-toc-modified-id=\"$Œ±$:-Learning-Rate-18\">$Œ±$: Learning Rate</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-19\">Check for understanding</a></span></li><li><span><a href=\"#Why-is-Q-learning-useful?\" data-toc-modified-id=\"Why-is-Q-learning-useful?-20\">Why is Q-learning useful?</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-21\">Check for understanding</a></span></li><li><span><a href=\"#Q-learning\" data-toc-modified-id=\"Q-learning-22\">Q-learning</a></span></li><li><span><a href=\"#Model-based-vs-Model-free-Algorithms\" data-toc-modified-id=\"Model-based-vs-Model-free-Algorithms-23\">Model-based vs Model-free Algorithms</a></span></li><li><span><a href=\"#5-elements-of-MDP\" data-toc-modified-id=\"5-elements-of-MDP-24\">5 elements of MDP</a></span></li><li><span><a href=\"#Model-Free-Algorithms\" data-toc-modified-id=\"Model-Free-Algorithms-25\">Model-Free Algorithms</a></span></li><li><span><a href=\"#Model-Free-Algorithms\" data-toc-modified-id=\"Model-Free-Algorithms-26\">Model-Free Algorithms</a></span></li><li><span><a href=\"#Size-of-Q-Table-for-Game-of-Go\" data-toc-modified-id=\"Size-of-Q-Table-for-Game-of-Go-27\">Size of Q-Table for Game of Go</a></span></li><li><span><a href=\"#Challenge-Question\" data-toc-modified-id=\"Challenge-Question-28\">Challenge Question</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-29\">Check for understanding</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-30\">Takeaways</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-31\">Sources of Inspiration</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-32\">Bonus Material</a></span></li><li><span><a href=\"#On-policy-vs-Off-policy\" data-toc-modified-id=\"On-policy-vs-Off-policy-33\">On-policy vs Off-policy</a></span></li><li><span><a href=\"#Alternative-Q-learning-Formulas\" data-toc-modified-id=\"Alternative-Q-learning-Formulas-34\">Alternative Q-learning Formulas</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-Learning</h2></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Write the Q-learning formula.\n",
    "- Define the elements of Q-learning in your own words.\n",
    "- Explain why Q-learning is so powerful and popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Choose your own adventure: The Knight & The Castle</h2></center>\n",
    "\n",
    "<center><img src=\"http://blog-assets.bigfishgames.com/uploads/2012/08/Choose-your-own-adventure.jpg\" width=\"100%\"/></center> \n",
    "\n",
    "You are a young adventurer ‚öî You have your well-being and looking for treasure üí∞ You find a castle üè∞ You enter and find it an abandoned ‚Ä¶\n",
    "\n",
    "It is spooky üëª but intriguing üòØ The Entrance/Exit \"E\" room gives you sense of wonder. You feel okay (well-being is +5) and put you are poor and have no treaure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You see two doors. One label \"A\" and the other labeled \"K\".\n",
    "\n",
    "What do you do?\n",
    "\n",
    "- Go through door \"A\"\n",
    "- Go through door \"K\"\n",
    "- Stay in Entrance/Exit \"E\" room\n",
    "- Or leave the castle\n",
    "\n",
    "You might want to take notes of where (State) and what happens (Reward - Well being and treasure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><img src=\"images/adventure_world.png\" width=\"75%\"/></center>\n",
    "\n",
    "[Editable Figure](https://www.draw.io/#G17aMDvycTke0xjgYj5EV7SMpbCmNGYEoo)\n",
    "\n",
    "Rewards:\n",
    "\n",
    "- E - Entrance +5 well-being (awe)\n",
    "- A - Apartment -10 well-being (musty and moldy)\n",
    "- k - Keep  0 well-being (nothing happening)\n",
    "- B - [Buttery](https://en.wikipedia.org/wiki/Buttery_(room)) +10 well-being (butter is delicious)\n",
    "- F - Treasure room -20 well-being (damp) but [+100, +100, +100, 0, 0, 0, ‚Ä¶] treasure.\n",
    "- C - Dragon's Den -‚àû well-being  and -‚àû treasure (It will kill you and talk all of your treasure)\n",
    "\n",
    "Rules:\n",
    "\n",
    "- Since it is dark, you can get turned around don't always go where you intend (stochastic).\n",
    "- Can stay in any room as long you want.\n",
    "- The dragon gets angrier every time you enter its state and burns you more each time.\n",
    "- The dragoon will take all your treasure.\n",
    "- Your goal is to maximize treasure while keeping well-being above zero.\n",
    "\n",
    " [Source](http://people.revoledu.com/kardi/tutorial/ReinforcementLearning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>The Knight & The Castle</h2></center>\n",
    "\n",
    "Can this be solved with Reinforcement Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ Sequential? Be explicit on the time steps (e.g., discrete, continuous, finite, infinite)\n",
    "+ Environment\n",
    "+ Agent - Who/what takes an action and gets a reward?\n",
    "+ State - What are all possible states the agent can be in?\n",
    "+ Reward - What are the all the rewards in each state?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Is Markov decision process (MDP) a good model for this world?\n",
    "\n",
    "Technically - The Knight & The Castle is not Markov because rooms have state-based history.\n",
    "\n",
    "\"All models are wrong. Some are useful.\" You can model it as a MDP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What would help you make decision? </h2></center>\n",
    "<br>\n",
    "\n",
    "__Q-Table (aka, Quality of State Table)__:\n",
    "\n",
    "| State | Value  |  \n",
    "|:-------|:------|\n",
    "| E (Entrance) | +12.5 (a sense of awe) |\n",
    "| A  (Apartment)  | -13.6 (moldy and musty ) |  \n",
    "| B  (Buttery)  | +11.8 (Butter is delicious) |  \n",
    "| C  (Dragoon's den)  | -59.8 (üêâ üî•) |  \n",
    "| D  (Keep)  | 0 (Nothing) |  \n",
    "| F  (Treasure)  | +252.3 (üí∞üíéüëë)|  \n",
    "\n",
    "These values are very rough estimates!|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> RL Loop</h2></center>\n",
    " \n",
    "<center><img src=\"images/loop.png\" width=\"45%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2> Q-learning</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Run Policy - Take a single step in Environment. Either random or greedy (the best policy we have so far)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Evaluate Policy - After taking action following current policy, store results (s, a, s‚Ä≤, r). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Improve Policy - What is the next best action to maximize Utility in the current state?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-value </h2></center>\n",
    "\n",
    "<center><img src=\"images/q_learning.png\" width=\"95%\"/></center>\n",
    "\n",
    "The general tabular form is States x Actions with Utilities in the cells.\n",
    "\n",
    "$Q(s, a)$ is the Q-value function.\n",
    "\n",
    "It is the expected discounted reward if the agent performs action $a$ in state $s$, then follow optimal policy from then on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Q-value is similar to Value, except that Q-value takes the current action $a$ as additional parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$Q_œÄ(s, a)$ refers to the long-term return of the current state s, taking action $a$ under policy $œÄ$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>How to choose in $a$ in $Q(s, a)$           ?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Œµ-Greedy Algorithm\n",
    "\n",
    "$œÄ(a|s)$:\n",
    "\n",
    "- with probability 1-Œµ choose $argmax Q(s, a)$ (greedy action)\n",
    "- with probability Œµ choose random $a$ (random action)\n",
    "\n",
    "Always exploring new actions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>How are Q's learned, aka Q-learning?</h2></center>\n",
    "\n",
    "$Q$ learns to corresponds to taking the action $a$ and then continuing under a policy $œÄ$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This function is unknown.\n",
    "\n",
    "We'll learn about its Value through experience, based on $(s, a, s')$ triples. \n",
    "\n",
    "\"I was in state $s$, and I tried doing $a$ and $s'$ happened. What was the reward?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>How are Q's learned, aka Q-learning?</h2></center>\n",
    "\n",
    "Before we explore the environment, Q gives the same (arbitrary) fixed value. \n",
    "\n",
    "But then, as we explore the environment more, Q gives us a better and better approximation of the value of an action a at a state s. \n",
    "\n",
    "We update our function Q as we go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " We'll store the results in a Q-table which is state and reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-learning Flow</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/q_diagram.jpg\" width=\"45%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Q-learning Pseudocode</h2></center>\n",
    "\n",
    "Initialize the Values table $Q(s, a)$ \n",
    "\n",
    "Repeat until a terminal state is reached:\n",
    "\n",
    "1. Observe the current state $s$.\n",
    "1. Choose an action $a$ based on one of the action selection policies (i.e., epsilon greedy)\n",
    "1. Take the action, and observe the reward $r$ as well as the new state $s$.\n",
    "1. Update the Value for the state using the observed reward and the maximum reward possible for the next state. \n",
    "1. Set the current state to the new state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " [Source](https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "How could we initialize the Values table  ùëÑ(ùë†,ùëé)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Deterministic Neutral or Pessimistic  - Given a certain environment, a value of 0 would make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Deterministic Optimistic - Something good could happen. Encourage exploration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Random \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Something else (e.g., Priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-learning Formalism</h2></center>\n",
    "<br>  \n",
    "\n",
    "<center><img src=\"images/q_formula_annotated.svg\" width=\"100%\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The updated value is a combination of the previous value plus the weighted amount of reward and discounted future rewards if we get the best possible value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Q-Learning Graphical Interpretation</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/max.png\" width=\"25%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>$Œ±$: Learning Rate</h2></center>\n",
    "\n",
    "The learning rate determines to what extent the newly acquired information will override the old information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When Œ±=0, the agent not learn anything. Exclusively exploit prior knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When Œ±=1, the agent consider only the most recent information. Ignoring prior knowledge to explore possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>$Œ±$: Learning Rate</h2></center>\n",
    "\n",
    "In practice, the learning rate is often a low constant.\n",
    "\n",
    "$$Œ± = 0.1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "In fully deterministic environments, what should learning rate $Œ±$ be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$Œ± = 1$ is optimal in fully deterministic environments. \n",
    "\n",
    "The most recent information is all that is needed because the future will be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Why is Q-learning useful?</h2></center>\n",
    "\n",
    "Finds a policy that is optimal in the sense that it maximizes the expected value of the total reward over any and all successive steps, starting from the current state without a model of the Environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$œÄ^*(s) = argmax Q(s,a)$$\n",
    "\n",
    "Just chose the action that maximizes the Q value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What is the difference between temporal-difference and Q-learning?\n",
    "\n",
    "Temporal Difference (TD) is an approach to learning how to predict a quantity that depends on future values of a given state. \n",
    "\n",
    "TD can be used to learn both the V-function and the Q-function. \n",
    "\n",
    "Q-learning is a specific TD algorithm used to learn the Q-function $Q(s,a)$.\n",
    "\n",
    "[Source](https://stackoverflow.com/questions/34181056/q-learning-vs-temporal-difference-vs-model-based-reinforcement-learning#:~:text=1%20Answer&text=Temporal%20Difference%20is%20an%20approach,to%20learn%20the%20Q%2Dfunction.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-learning</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>A <b>model-free</b> technique to learn an optimal Q(s, a) for the Agent.</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2>Model-based vs Model-free Algorithms</h2></center>\n",
    " \n",
    "Model-based algorithms assume complete knowledge of 5 elements of MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Quick - write down the 5 elements of MDP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>5 elements of MDP</h2></center>\n",
    "\n",
    "<center>(S,A,P,R,Œ≥)</center>\n",
    "\n",
    "1. State\n",
    "2. Action\n",
    "3. Transition Probabilities  \n",
    "4. Reward\n",
    "5. Discounting rate of future rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Model-Free Algorithms</h2></center>\n",
    "\n",
    "MDP model is unknown but can be sampled.\n",
    "\n",
    "As we experience the MDP, we learn the States, Actions, Transition Probabilities, Reward, and Discounting rate of future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Model-Free Algorithms</h2></center>\n",
    "\n",
    "Model-Free Algorithms are also used when MDP model is known, but it is computationally infeasible to use directly (except through sampling).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is an example of computationally infeasible, but completely known MDP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Size of Q-Table for Game of Go</h2></center>\n",
    "\n",
    "<center><img src=\"images/go.jpeg\" width=\"50%\"/></center>\n",
    "\n",
    "Since each location on the board can be either empty, black, or white, there are a total of $3^{n^2}$ possible board positions on a square board with length n (However, only part of them are legal).\n",
    "\n",
    "[Let‚Äôs look at a table](https://en.wikipedia.org/wiki/Go_and_mathematics#Legal_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Learn more about the game complexity of Go](https://en.wikipedia.org/wiki/Go_and_mathematics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Challenge Question</h2></center>\n",
    "\n",
    "If your state space very large and you still want to apply Q-learning, what should you do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Compression through Value Function Approximation. \n",
    "\n",
    "One example is feature engineering, learn a useful compression representation of state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What technique is very good at feature engineering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/deep_learning.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Image Source](https://becominghuman.ai/deep-learning-made-easy-with-deep-cognition-403fbe445351)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Q-learning is the most useful RL algorithm we have seen thus far.\n",
    "- Because Q-learning is a __model-free__ technique.\n",
    "- Q-learning learns the best policy for given state by empirically taking actions and sampling rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- Spinning Up in Deep RL Workshop from OpenAI from 1:48 to 2:25\n",
    "    - [video](https://youtu.be/fdY7dt3ijgY?t=6482)\n",
    "    - [slides start at #48](https://github.com/openai/spinningup-workshop/blob/master/rl_intro/rl_intro.pdf)\n",
    "- [Q-learning for Tower of Hanoi](https://github.com/khpeek/Q-learning-Hanoi/blob/master/hanoi.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2>On-policy vs Off-policy</h2></center>\n",
    " \n",
    " __On-policy__: \n",
    " \n",
    "- Learns optimal policy only based on an agent following the current policy.\n",
    " \n",
    "- Updates Q-values using the Q-value of the next state ùë†‚Ä≤ and the current policy's action ùëé‚Ä≤. Estimates the return for state-action pairs assuming the current policy continues to be followed.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " __Off-policy__: \n",
    "\n",
    "- Learns optimal policy independently of agent's actions.\n",
    " \n",
    "- Updates Q-values using the Q-value of the next state ùë†‚Ä≤ and the greedy action ùëé‚Ä≤. Estimates the return for state-action pairs assuming a greedy policy were followed despite the fact that it's not following a greedy policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://stats.stackexchange.com/questions/184657/difference-between-off-policy-and-on-policy-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Alternative Q-learning Formulas</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"images/q.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/63b502aafbe6ea1585231222ea3783f40f0808a9\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><img src=\"images/another_version_q.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
