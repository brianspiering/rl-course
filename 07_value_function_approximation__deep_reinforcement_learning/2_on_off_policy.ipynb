{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Behavior-Policy-vs.-Target-Policy\" data-toc-modified-id=\"Behavior-Policy-vs.-Target-Policy-1\">Behavior Policy vs. Target Policy</a></span></li><li><span><a href=\"#On-policy-vs-Off-policy\" data-toc-modified-id=\"On-policy-vs-Off-policy-2\">On-policy vs Off-policy</a></span></li><li><span><a href=\"#Example-of-On-vs-Off-Policy-for-gridworld\" data-toc-modified-id=\"Example-of-On-vs-Off-Policy-for-gridworld-3\">Example of On vs Off Policy for gridworld</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Behavior Policy vs. Target Policy</h2></center>\n",
    "\n",
    "- Behavior Policy - The policy that yields the current action.\n",
    "\n",
    "- Target Policy - The policy that is being learned / updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2>On-policy vs Off-policy</h2></center>\n",
    " \n",
    "__On-policy training__: \n",
    " \n",
    "- Improves only the current policy the agent is following.\n",
    " \n",
    "- Updates Q-values using the Q-value of the next state $ùë†‚Ä≤$ and the current policy's action $ùëé‚Ä≤$. \n",
    "\n",
    "- Estimates the return for state-action pairs assuming the current policy continues to be followed.\n",
    " \n",
    "- On-policy training - Target Policy = Behavior Policy. Update only the Behavior Policy \n",
    "\n",
    "- Think of on-policy as sequential:\n",
    "    - Try something (action). \n",
    "    - How does it work (reward)?\n",
    "    - I'll do more or less of that thing (agent changes Q(s,a))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " __Off-policy training__: \n",
    "\n",
    "- Learns optimal policy independently of agent's actions. \n",
    "\n",
    "- Allows agent to learn policies not acted upon. Update all relevant Target Policies.\n",
    "\n",
    "- Think of off-policy as parallel:\n",
    "    - Try something (action). \n",
    "    - How does it work (reward)?\n",
    "    - I'll do more all related good actions.\n",
    "        - Or if I can invert, I'll less of distance bad actions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Example of On vs Off Policy for gridworld</h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_rewards.png\" width=\"75%\"/></center>\n",
    "\n",
    "\n",
    "\n",
    "Behavior Policy = \"Always go up\" \n",
    "\n",
    "Not great but will get the job down.\n",
    "\n",
    "For on-policy learning, Brownie can only evaluate that policy as good or bad.\n",
    "\n",
    "On-policy only allows Brownie to increase the Q(s,a) value entry for ((*, *), up).\n",
    "\n",
    "------\n",
    "\n",
    "Brownies how ever finds that going right later is better.\n",
    "\n",
    "Target Policy = \"I'll go up for a couple of steps. Then I'll go right\"\n",
    "\n",
    "Off-policy allows Brownie to increase the Q(s,a) value entry for ((2,0), right), ((2, 1), right), ((2,2), right). If it gets rewarded for it for any reason.\n",
    "\n",
    "\n",
    "Off-policy estimates the return for Q(s,a) current best guess, aka greedy. That is despite it's didn't follow the greedy policy to get the reward. \n",
    "\n",
    "Off-policy is very powerful!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://stats.stackexchange.com/questions/184657/difference-between-off-policy-and-on-policy-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
