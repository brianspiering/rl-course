{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Policy-Iteration\" data-toc-modified-id=\"Policy-Iteration-1\">Policy Iteration</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Calculating-Value-Function:-2-methods\" data-toc-modified-id=\"Calculating-Value-Function:-2-methods-3\">Calculating Value Function: 2 methods</a></span></li><li><span><a href=\"#-Policy-Iteration-Intuition\" data-toc-modified-id=\"-Policy-Iteration-Intuition-4\"> Policy Iteration Intuition</a></span></li><li><span><a href=\"#-Policy-Iteration-Intuition\" data-toc-modified-id=\"-Policy-Iteration-Intuition-5\"> Policy Iteration Intuition</a></span></li><li><span><a href=\"#-Policy-Iteration\" data-toc-modified-id=\"-Policy-Iteration-6\"> Policy Iteration</a></span></li><li><span><a href=\"#Policy-Iteration-Thought-Experiment\" data-toc-modified-id=\"Policy-Iteration-Thought-Experiment-7\">Policy Iteration Thought Experiment</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-8\">Check for understanding</a></span></li><li><span><a href=\"#-Policy-Iteration-Formalism\" data-toc-modified-id=\"-Policy-Iteration-Formalism-9\"> Policy Iteration Formalism</a></span></li><li><span><a href=\"#Value-Iteration-vs-Policy-Iteration\" data-toc-modified-id=\"Value-Iteration-vs-Policy-Iteration-10\">Value Iteration vs Policy Iteration</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-11\">Takeaways</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-12\">Sources of Inspiration</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Policy Iteration</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Explain the similarities and differences between Value Iteration and Policy Iteration\n",
    "- List the steps for Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/rl_types.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Calculating Value Function: 2 methods</h2></center>\n",
    "\n",
    "1. Value iteration\n",
    "2. Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Policy Iteration Intuition</h2></center>\n",
    "\n",
    "Initialize a starting policy. \n",
    "\n",
    "What would be a reasonable good choice for starting policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Random walk.\n",
    "\n",
    "(Brian's favorite baseline is random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Policy Iteration Intuition</h2></center>\n",
    "\n",
    "Initialize a starting policy. `\n",
    "\n",
    "Repeat until satisfied:\n",
    "\n",
    "1. __Run Policy__ - For given policy, pick an action based on the state to get a reward.\n",
    "1. __Evaluate Policy__ - For given policy, compute the estimated utility (that estimated utility is value function $v_œÄ$).\n",
    "1. __Improve Policy__ - Pick next, best (greedy) policy available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Policy Iteration</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/policy_loop.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Policy Iteration Thought Experiment</h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_with_neg_rewards.png\" width=\"50%\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's compare three different policies:\n",
    "\n",
    "1. Round #1 Given $œÄ_a$: \"Always Up\" - Intended trajectory: (U, U, U, U)\n",
    "\n",
    "2. Round #2 Given $œÄ_b$: \"Randomly choose between Right & Up\" - One possible trajectory (R, R, U, U, R)\n",
    "\n",
    "3. Round #3 Given $œÄ_c$: \"Follow Value Estimates\" - Intended trajectory: (U, U, R, R, R)\n",
    "\n",
    "<center>$œÄ_c$ is the winner, aka $œÄ_*$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What are examples of being \"satisfied\"? When would we stop looking for a better policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Guaranteed optimal\n",
    "1. Good enough\n",
    "1. Run out of resource budget (could be time or number of steps)\n",
    "1. Stops improving "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The book and video course use \"Stops improving\" because they are theoreticians.   \n",
    "In the applied world, \"good enough\" and \"out of budget\" are typically when projects stop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Policy Iteration Formalism</h2></center>\n",
    "\n",
    "1. Policy Evaluation:\n",
    "\n",
    "$$v_  œÄ(s) = ùîº[ùëà_{t+1}+Œ≥ùëà_{t+2}+‚Ä¶|S_t=s]$$\n",
    "\n",
    "1. Policy Improvement:\n",
    "\n",
    "$$œÄ' = greedy(v_œÄ)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Value Iteration vs Policy Iteration</h2></center>\n",
    "\n",
    "Value iteration is simpler but its computationally heavy.   \n",
    "Policy iteration is complicated but its computationally cheap.\n",
    "\n",
    "Value iteration includes: finding optimal value function + one policy extraction. There is no repeat of the two because once the value function is optimal, then the policy out of it should also be optimal (i.e. converged).\n",
    "\n",
    "Policy iteration includes: policy evaluation + policy improvement, and the two are repeated iteratively until policy converge.\n",
    "\n",
    "Policy Iteration often takes considerably fewer number of iterations to converge than Value Iteration. However, each policy iteration is more computationally expensive.\n",
    "\n",
    "In applied setting:\n",
    "\n",
    "- Value iteration tends to be more useful in a model-based environment.\n",
    "- Policy iteration tends to be more useful in a model-free environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Policy Iteration has 3 steps:\n",
    "    1. Run Policy - For given policy, take state-action pairs.\n",
    "    1. Evaluate Policy  - Compute the estimated utility.\n",
    "    1. Improve Policy - Pick a better policy.\n",
    "    \n",
    "- Generally, Value Iteration is simpler but more limited in application than Policy Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- https://www.quora.com/How-is-policy-iteration-different-from-value-iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
