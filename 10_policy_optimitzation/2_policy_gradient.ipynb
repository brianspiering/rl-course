{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Policy-Optimization\" data-toc-modified-id=\"Policy-Optimization-1\">Policy Optimization</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Overview-of-Algorithms\" data-toc-modified-id=\"Overview-of-Algorithms-3\">Overview of Algorithms</a></span></li><li><span><a href=\"#Defining-Policy-Gradient\" data-toc-modified-id=\"Defining-Policy-Gradient-4\">Defining Policy Gradient</a></span></li><li><span><a href=\"#Policy-Gradient-Objective-Function\" data-toc-modified-id=\"Policy-Gradient-Objective-Function-5\">Policy Gradient Objective Function</a></span></li><li><span><a href=\"#Updating-Policy-Gradient-Objective-Function\" data-toc-modified-id=\"Updating-Policy-Gradient-Objective-Function-6\">Updating Policy Gradient Objective Function</a></span></li><li><span><a href=\"#Gradient-Policy-Algorithm-Pseudocode:-General\" data-toc-modified-id=\"Gradient-Policy-Algorithm-Pseudocode:-General-7\">Gradient Policy Algorithm Pseudocode: General</a></span></li><li><span><a href=\"#Gradient-Policy-Algorithm-Pseudocode:-Specific\" data-toc-modified-id=\"Gradient-Policy-Algorithm-Pseudocode:-Specific-8\">Gradient Policy Algorithm Pseudocode: Specific</a></span></li><li><span><a href=\"#Simple-Policy-Spaces\" data-toc-modified-id=\"Simple-Policy-Spaces-9\">Simple Policy Spaces</a></span></li><li><span><a href=\"#Softmax-Policy-Search\" data-toc-modified-id=\"Softmax-Policy-Search-10\">Softmax Policy Search</a></span></li><li><span><a href=\"#Guassian-Policy-search\" data-toc-modified-id=\"Guassian-Policy-search-11\">Guassian Policy search</a></span></li><li><span><a href=\"#Pong:-Worked-example-for-Policy-Gradient\" data-toc-modified-id=\"Pong:-Worked-example-for-Policy-Gradient-12\">Pong: Worked example for Policy Gradient</a></span></li><li><span><a href=\"#Policy-Gradient-Strengths-(especially-compared-to-DQN)\" data-toc-modified-id=\"Policy-Gradient-Strengths-(especially-compared-to-DQN)-13\">Policy Gradient Strengths (especially compared to DQN)</a></span></li><li><span><a href=\"#Policy-Gradient-limitations\" data-toc-modified-id=\"Policy-Gradient-limitations-14\">Policy Gradient limitations</a></span></li><li><span><a href=\"#Policy-gradient-is-on-policy\" data-toc-modified-id=\"Policy-gradient-is-on-policy-15\">Policy gradient is on-policy</a></span></li><li><span><a href=\"#Possible-extension-to-off-policy-learning\" data-toc-modified-id=\"Possible-extension-to-off-policy-learning-16\">Possible extension to off-policy learning</a></span></li><li><span><a href=\"#off-policy-objective-function\" data-toc-modified-id=\"off-policy-objective-function-17\">off-policy objective function</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-18\">Takeaways</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-19\">Sources of Inspiration</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-20\">Bonus Material</a></span></li><li><span><a href=\"#Policy-Gradient-Algorithm-according-to-OpenAI\" data-toc-modified-id=\"Policy-Gradient-Algorithm-according-to-OpenAI-21\">Policy Gradient Algorithm according to OpenAI</a></span></li><li><span><a href=\"#Another-Overview-of-Algorithms\" data-toc-modified-id=\"Another-Overview-of-Algorithms-22\">Another Overview of Algorithms</a></span></li><li><span><a href=\"#Policy-Gradient-Variations\" data-toc-modified-id=\"Policy-Gradient-Variations-23\">Policy Gradient Variations</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Policy Optimization</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Policy Gradient (PG) in your owns.\n",
    "- Explain the elements in PG formalism.\n",
    "- List the pros and cons of PG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Overview of Algorithms</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"images/algorithms.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Defining Policy Gradient</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/overall.png\" width=\"75%\"/></center>\n",
    "\n",
    "The objective of a Reinforcement Learning agent is to maximize the \"expected\" reward when following a policy œÄ.\n",
    "\n",
    "Learn the policy directly as a parameterized function   \n",
    "\n",
    "$$œÄ(a|s;Œ∏)$$\n",
    "\n",
    "Œ∏ are the knobs to turn.\n",
    "\n",
    "Policy gradient reinforcement learning optimizes a policy directly instead of estimating an action-value function from which we infer the policy. \n",
    "\n",
    "The goal is to find the parameters of a policy that maximize rewards for a state-action pair.\n",
    "\n",
    "The selected actions are target values for supervised learning.\n",
    "\n",
    "\n",
    "----\n",
    "In contrast, Q-learning / DQN maximizes the value\n",
    "\n",
    "$Q(s, a)$\n",
    "\n",
    "Receives as input a state  s and an action a, and returns the utility  of that state-action pair.\n",
    "\n",
    "Then there is a separate step to choose the action (e.g., epsilon greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Policy Gradient Objective Function</h2></center>\n",
    "\n",
    "J is the objective functions which describe the policy‚Äôs quality which we want to maximize. \n",
    "\n",
    "$${J}(\\theta) = V_{\\pi_\\theta}(S) = \\mathbb{E}_{\\pi_\\theta}[V]$$\n",
    "<center>notion crosswalk</center>\n",
    "\n",
    "$$J(Œ∏) = ùîº[\\sum_{t=o}^{T-1} r_t+1]$$\n",
    "\n",
    "Learn set of policy parameters that maximizes the cumulative future reward to be received starting from any given time t until the terminal time T.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Updating Policy Gradient Objective Function</h2></center>\n",
    "\n",
    "$$Œ∏ \\leftarrow  Œ∏ +  \\frac{Œ¥}{Œ¥Œ∏} J(Œ∏)$$\n",
    "\n",
    "$$ Œî Œ∏ =  Œ±‚àá_Œ∏ J(Œ∏)$$\n",
    "\n",
    "$$‚àá_Œ∏ J(Œ∏) = ùîº_{œÄ_Œ∏}[‚àá_Œ∏ logœÄ_Œ∏(s,a)Q^{œÄ_Œ∏}(s,a) ]$$\n",
    "$$‚àá_Œ∏ J(Œ∏) = \\sum_{t=o}^{T-1}[‚àá_Œ∏ logœÄ_Œ∏(s,a)Q^{œÄ_Œ∏}(s,a) ]$$\n",
    "\n",
    "State-action pairs with higher estimated Q values will increase in\n",
    "probability on average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Gradient Policy Algorithm Pseudocode: General</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/template.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Gradient Policy Algorithm Pseudocode: Specific</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/gradient_policy_stanford.png\" width=\"75%\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i is current iteration   \n",
    "G is the return, sum of rewards thus far in the episode   \n",
    "b is the excepted sum of rewards   \n",
    "\n",
    "Advantage estimate - How much better empirically is the current policy?\n",
    "\n",
    "Update estimate baseline function - the excepted sum of rewards should be closer to the observed empirically rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:   \n",
    "Stanford CS234: Reinforcement Learning | Winter 2019 | Lecture 9 - Policy Gradient II  \n",
    "https://youtu.be/E-_ecpD5PkE?t=1658"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://miro.medium.com/max/1400/1*zkOBQ9Izq28yXCANTmdKtA.png\" width=\"75%\"/></center>\n",
    "\n",
    "Typically, RL is trying to maximize rewards, thus gradient __ascent__ of policy.\n",
    "\n",
    "Change the model parameters (policy) to increase the likelihood of trajectories that move higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Simple Policy Spaces</h2></center>\n",
    "\n",
    "\n",
    "- Discrete action problem: Softmax Policy\n",
    "- Continuous action problem: Gaussian Policy\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Softmax Policy Search</h2></center>\n",
    "\n",
    "<center><img src=\"images/softmax.png\" width=\"75%\"/></center>\n",
    "\n",
    "h(s, a) is a function approximator with Œ∏ parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Guassian Policy search</h2></center>\n",
    "\n",
    "Use neural network to approximate mean (often times the variance is fixed).\n",
    "\n",
    "Sample the action according to Gaussian distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Pong: Worked example for Policy Gradient</h2></center>\n",
    "\n",
    "<center><img src=\"http://karpathy.github.io/assets/rl/policy.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"http://karpathy.github.io/assets/rl/episodes.png\" width=\"75%\"/></center>\n",
    "\n",
    "4 rollouts: complete games. \n",
    "\n",
    "Each black circle is some game state (three example states are visualized on the bottom), and each arrow is a transition, annotated with the action that was sampled. \n",
    "\n",
    "The agent won 2 games and lost 2 games. \n",
    "\n",
    "Take the two games won and slightly encourage every single action we made in that episode. For example, given an observed state move towards the ball.\n",
    "\n",
    "Conversely, take the two games lost and slightly discourage every single action we made in that episode. For example, if ball is other side of court stay.\n",
    "\n",
    "Sources:\n",
    "\n",
    "- [Blogpost](http://karpathy.github.io/2016/05/31/rl/)\n",
    "- [Video](https://www.youtube.com/watch?v=tqrcjHuNdmQ&t=164s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Policy Gradient Strengths (especially compared to DQN)</h2></center>\n",
    "\n",
    "- Automated improvement - just run it more and it mostly get better.\n",
    "- Faster convergence - Gradient descent takes big steps quickly.\n",
    "- Messy World: If Q function is too complex to be learned, DQN may fail\n",
    "miserably, while PG will still learn a good policy.\n",
    "- Continuous actions: Much easier to model continuous action space\n",
    "    -  Q-Learning's max operator is only useful in approximate discrete action spaces \n",
    "- Can learn stochastic policies: Capable of learning stochastic policies \n",
    "    - DQN can only learn deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Policy Gradient limitations</h2></center>\n",
    "\n",
    "- Like all gradient based methods, there is no guarantee of finding a global optima.\n",
    "- Sample in-efficiency\n",
    "    - At most only one gradient step per environment sample\n",
    "- Primarily on-policy learning  (see next section)\n",
    "- Need to take small step sizes\n",
    "    - Large changes in policies can be catastrophic for learning and unsafe. \n",
    "- Exhibit high variance - 'cause Monte Carlo single complete trajectories\n",
    "- Difficult to debug when it does not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Policy gradient is on-policy</h2></center>\n",
    "\n",
    "1. Run a actual trajectory \n",
    "2. Find its actual rewards\n",
    "3. Improve the policy that generated that trajectory\n",
    "\n",
    "Extremely inefficient! (But automated!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Possible extension to off-policy learning</h2></center>\n",
    "Can make a guess at off-policy variant with importance sampling\n",
    "\n",
    "importance sampling - a general technique for estimating properties of a particular distribution, while only having samples generated from a different distribution than the distribution of interest.\n",
    "\n",
    "Expected value of f(x) where x has a data distribution p. However, Instead of sampling from p, we calculate the result from sampling q.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*iE54DcYFsXI6dJpic_PidQ.jpeg\" width=\"75%\"/></center>\n",
    "\n",
    "We use the sampling distribution q to estimate the expected value for p using:\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*PA2dSXYX-fbEV8TTm0eVvQ.jpeg\" width=\"75%\"/></center>\n",
    "Pick a simple distribution with a reasonable relationship to the distribution of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>off-policy objective function</h2></center>\n",
    "<center><img src=\"images/off-policy.png\" width=\"75%\"/></center>\n",
    "\n",
    "- Behavior policy ($Œº_\\phi$): the policy used to collect samples.\n",
    "- Target policy ($œÄ_Œ∏$): the learning policy, which we are interested in.\n",
    "\n",
    "We have define a surrogate function for the relationship between distributions / policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Policy Gradient (PG) seeks to optimize a set of parameters for a policy. \n",
    "- PG is best done with gradient ascent to maximize rewards.\n",
    "- PG works in a broad set of problems but can be costly with no guarantees finding of optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- https://medium.com/biffures/rl-course-by-david-silver-lectures-5-to-7-576188d3b033\n",
    "- http://www.scholarpedia.org/article/Policy_gradient_methods\n",
    "- https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d\n",
    "- https://www.slideshare.net/zhihua98/policy-gradient-98034864?from_action=save\n",
    "- https://medium.com/@jonathan_hui/rl-importance-sampling-ebfb28b4a8c6\n",
    "- [Policy Optimization via Importance Sampling](https://papers.nips.cc/paper/7789-policy-optimization-via-importance-sampling.pdf) Metelli et al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus Material\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Policy Gradient Algorithm according to OpenAI</h2></center>\n",
    "\n",
    "<center><img src=\"images/policy_gradient_algo_open_ai.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Another variation:    \n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*94EI9DpoXnWa6oLHvh14pw.jpeg\" width=\"75%\"/></center>\n",
    "\n",
    "Source: https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Another Overview of Algorithms</h2></center>\n",
    "\n",
    "<center><img src=\"images/rl_algo_diagram_week2.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Policy Gradient Variations</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/many_forms.png\" width=\"75%\"/></center>\n",
    "\n",
    "[Image Source](https://www.youtube.com/watch?v=KHZVXao4qXs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
