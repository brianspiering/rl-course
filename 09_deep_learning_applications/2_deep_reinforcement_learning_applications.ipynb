{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Deep-Reinforcement-Learning-Applications\" data-toc-modified-id=\"Deep-Reinforcement-Learning-Applications-1\">Deep Reinforcement Learning Applications</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Deep-Learning\" data-toc-modified-id=\"Deep-Learning-3\">Deep Learning</a></span></li><li><span><a href=\"#Deep-Reinforcement-Learning\" data-toc-modified-id=\"Deep-Reinforcement-Learning-4\">Deep Reinforcement Learning</a></span></li><li><span><a href=\"#General-schema-of-deep-RL-methods\" data-toc-modified-id=\"General-schema-of-deep-RL-methods-5\">General schema of deep RL methods</a></span></li><li><span><a href=\"#Common-properties-of-real-world-systems\" data-toc-modified-id=\"Common-properties-of-real-world-systems-6\">Common properties of real-world systems</a></span></li><li><span><a href=\"#Common-properties-of-simulated-environment-\" data-toc-modified-id=\"Common-properties-of-simulated-environment--7\">Common properties of simulated environment </a></span></li><li><span><a href=\"#Student-Activity:-What-are-the-Challenges-of-Real-World-Reinforcement-Learning?\" data-toc-modified-id=\"Student-Activity:-What-are-the-Challenges-of-Real-World-Reinforcement-Learning?-8\">Student Activity: What are the Challenges of Real-World Reinforcement Learning?</a></span></li><li><span><a href=\"#Challenges-of-Real-World-Reinforcement-Learning\" data-toc-modified-id=\"Challenges-of-Real-World-Reinforcement-Learning-9\">Challenges of Real-World Reinforcement Learning</a></span></li><li><span><a href=\"#Off-line-training\" data-toc-modified-id=\"Off-line-training-10\">Off-line training</a></span></li><li><span><a href=\"#-Learning-on-the-real-system-from-limited-samples.\" data-toc-modified-id=\"-Learning-on-the-real-system-from-limited-samples.-11\"> Learning on the real system from limited samples.</a></span></li><li><span><a href=\"#Data-center-cooling-using-Reinforcement-Learning\" data-toc-modified-id=\"Data-center-cooling-using-Reinforcement-Learning-12\">Data center cooling using Reinforcement Learning</a></span></li><li><span><a href=\"#How-much-is-spent-on-cooling?\" data-toc-modified-id=\"How-much-is-spent-on-cooling?-13\">How much is spent on cooling?</a></span></li><li><span><a href=\"#Data-Center-(DC)-HVAC-(Heating,-ventilation,-and-air-conditioning-(HVAC))-is-a-hard-problem\" data-toc-modified-id=\"Data-Center-(DC)-HVAC-(Heating,-ventilation,-and-air-conditioning-(HVAC))-is-a-hard-problem-14\">Data Center (DC) HVAC (Heating, ventilation, and air conditioning (HVAC)) is a hard problem</a></span></li><li><span><a href=\"#DC-HVAC-are-not-that-sophisticated\" data-toc-modified-id=\"DC-HVAC-are-not-that-sophisticated-15\">DC HVAC are not that sophisticated</a></span></li><li><span><a href=\"#Defining-DC-problem\" data-toc-modified-id=\"Defining-DC-problem-16\">Defining DC problem</a></span></li><li><span><a href=\"#DeepMind-Blogpost\" data-toc-modified-id=\"DeepMind-Blogpost-17\">DeepMind Blogpost</a></span></li><li><span><a href=\"#&quot;Data-center-cooling-using-model-predictive-control&quot;-paper\" data-toc-modified-id=\"&quot;Data-center-cooling-using-model-predictive-control&quot;-paper-18\">\"Data center cooling using model-predictive control\" paper</a></span></li><li><span><a href=\"#&quot;Transforming-Cooling-Optimization-for-Green-Data-Center-via-Deep-Reinforcement-Learning&quot;-paper\" data-toc-modified-id=\"&quot;Transforming-Cooling-Optimization-for-Green-Data-Center-via-Deep-Reinforcement-Learning&quot;-paper-19\">\"Transforming Cooling Optimization for Green Data Center via Deep Reinforcement Learning\" paper</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-20\">Takeaways</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-21\">Sources of Inspiration</a></span></li><li><span><a href=\"#Other-applications\" data-toc-modified-id=\"Other-applications-22\">Other applications</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Deep Reinforcement Learning Applications</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Explain how Deep Reinforcement Learning is variation of regular Deep Learning.\n",
    "- List the challenges of applying Deep Reinforcement Learning to real-world problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Deep Learning</h2></center>\n",
    "\n",
    "<center><img src=\"images/dl.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Deep Reinforcement Learning</h2></center>\n",
    "\n",
    "<center><img src=\"images/drl.png\" width=\"75%\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Source: [Introduction to Deep Reinforcement Learning](https://www.youtube.com/watch?v=zR11FLZ-O9M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Common properties of real-world systems</h2></center>\n",
    "\n",
    "- Rarely a good simulator, \n",
    "- Running them is expensive and/or slow\n",
    "- Systems are stochastic & non-stationary\n",
    "- Strong safety constraints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Common properties of simulated environment </h2></center>\n",
    "\n",
    "- Data is  unlimited (effectively)\n",
    "- System dynamics are clear and often deterministic\n",
    "- No consequences for poor actions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Student Activity: What are the Challenges of Real-World Reinforcement Learning?</h2></center>\n",
    "\n",
    "In small groups:\n",
    "\n",
    "- Brainstorm a list\n",
    "- Order them\n",
    "\n",
    "No need to provide examples or solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Challenges of Real-World Reinforcement Learning</h2></center>\n",
    "\n",
    "1. Training off-line from the fixed logs of an external behavior policy.\n",
    "1. Learning on the real system from limited samples.\n",
    "1. High-dimensional continuous state and action spaces. \n",
    "1. Tasks that may be partially observable, alternatively\n",
    "viewed as non-stationary or stochastic.\n",
    "1. Reward functions that are unspecified, multi-objective,\n",
    "or risk-sensitive.\n",
    "1. Safety constraints that should never or at least rarely\n",
    "be violated.\n",
    "1. System operators who desire explainable policies and\n",
    "actions.\n",
    "1. Inference that must happen in real-time at the control\n",
    "frequency of the system.\n",
    "1. Large and/or unknown delays in the system actuators,\n",
    "sensors, or rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Off-line training</h2></center>\n",
    "\n",
    "The acquisition of new observations may not be possible anymore (e.g., the batch setting). This scenario happens for instance in medical trials, in tasks with dependence on weather conditions or\n",
    "in trading markets (e.g., energy markets and stock markets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2> Learning on the real system from limited samples.</h2></center>\n",
    "\n",
    "The agent may not be able to interact with the true environment but only with an inaccurate simulation of it. This scenario occurs When first learning in a simulation, the difference with the real-world domain is known as the reality gap \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Data center cooling using Reinforcement Learning</h2></center>\n",
    "\n",
    "Data center electricity costs American businesses \\\\$13 billion annually, ~1.5\\% of the total electricity costs (for 2020). Could grow to \\\\$20 billion by 2024.\n",
    "\n",
    "\n",
    "Also use 626 billion liters of water mostly for cooling.\n",
    "\n",
    "<center><h2>How much is spent on cooling?</h2></center>\n",
    "\n",
    "<center><img src=\"images/DataCenterMonthlyCostPercentages.jpg\" width=\"75%\"/></center>\n",
    "\n",
    "Data centers are often located in cold places (including the bottom of the sea) with cheap electricity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source #1](https://info.siteselectiongroup.com/blog/power-in-the-data-center-and-its-costs-across-the-united-states#:~:text=The%20Natural%20Resources%20Defense%20Council,American%20businesses%20%2413%20billion%20annually)\n",
    "\n",
    "[Image Source](https://perspectives.mvdirona.com/2010/09/overall-data-center-costs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Data Center (DC) HVAC (Heating, ventilation, and air conditioning (HVAC)) is a hard problem</h2></center>\n",
    "    \n",
    "<center><img src=\"images/3-Figure1-1.png\" width=\"75%\"/></center>\n",
    "\n",
    "Designing control strategies is a challenging because:\n",
    "\n",
    "- Both mechanical and electrical systems\n",
    "- Complex interactions \n",
    "- Non-linear, dynamical systems\n",
    "- Adapt quickly to internal or external changes (like the weather).\n",
    "- Each data center has a unique architecture and environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>DC HVAC are not that sophisticated</h2></center>\n",
    "\n",
    "\"Most existing controllers tend to be fairly simple, somewhat conservative, and hand-tuned to specific equipment architectures, layouts, and configurations.\"\n",
    "\n",
    "Thus, it is often better not to mess something up than experiment towards improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Defining DC problem</h2></center>\n",
    "\n",
    "<center><img src=\"images/pue.png\" width=\"75%\"/></center>\n",
    "<center>Google ❤️ measuring and optimizing</center>\n",
    "\n",
    "North star metric: power usage efficiency (PUE)\n",
    "\n",
    "Inputs:\n",
    "\n",
    "- temperatures\n",
    "    - inside\n",
    "    - outside\n",
    "- power\n",
    "- pump speeds\n",
    "- setpoints\n",
    "- lots more …\n",
    "\n",
    "Possible actions: \n",
    "\n",
    "- Temperature of the water leaving the cooling tower and chilled water injection setpoints (original DeepMind work)\n",
    "- Fan speeds and water flow within air handling units (AHUs) (Google 2018 work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>DeepMind Blogpost</h2></center>\n",
    "\n",
    "<center><img src=\"https://lh3.googleusercontent.com/vQsFCdoQb-mUK7MC6jQjs1Wkw2vvkMpNmwA_y7DxQK-eughdE43k30pHVOmovHnwHBhyrF9UYFKHBlnMbhz9zx4Dsix4pwdItFcs=w1440-rw-v1\" width=\"75%\"/></center>\n",
    "\n",
    "Sequential A/B test\n",
    "\n",
    "\"40 percent reduction in the amount of energy used for cooling, which equates to a 15 percent reduction in overall PUE overhead after accounting for electrical losses and other non-cooling inefficiencies.\"\n",
    "\n",
    "Then moved to [autonomous control](https://deepmind.com/blog/article/safety-first-ai-autonomous-data-centre-cooling-and-industrial-control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>\"Data center cooling using model-predictive control\" paper</h2></center>\n",
    "\n",
    "\"Rather than executing entire trajectories, we re-optimize at each time step.\" \n",
    "\n",
    "<br>\n",
    "<details><summary>\n",
    "What type of learning is that?\n",
    "</summary>\n",
    "Temporal difference (TD) learning\n",
    "</details>\n",
    "\n",
    "\"The resulting system is simple to deploy, as it does not require historical data or a physics-based model.\"\n",
    "\n",
    "__Define model__:\n",
    "\n",
    "\n",
    "<center><img src=\"images/model_2018.png\" width=\"75%\"/></center>\n",
    "\n",
    "\"As safe operation during exploration is critical, we limit each control variable to a safe range informed by historical data. In the absence of such data, the safe range can be initialized conservatively and gradually expanded.\"\n",
    "\n",
    "Our exploration strategy is a simple, range-limited uniform random walk in each control variable:\n",
    "\n",
    "<center><img src=\"images/random_walk.png\" width=\"75%\"/></center>\n",
    "\n",
    "They did not define a Deep Learning system \n",
    "\n",
    "They define a linear system of equations with quadratic optimization objective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>\"Transforming Cooling Optimization for Green Data Center via Deep Reinforcement Learning\" paper</h2></center>\n",
    "\n",
    "Much more detailed...\n",
    "\n",
    "Off-policy and offline\n",
    "\n",
    "Used deep deterministic policy gradient (DDPG) (more on that next session)\n",
    "\n",
    "Ran on historical data simulation and trace (shadow data).\n",
    "\n",
    "\"while the policy can output the optimized control settings for any given state. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Deep Reinforcement Learning takes Deep Learning and replaces targets with actions.\n",
    "- Real-world problems can additional constraints (e.g., safety and more uncertainty) that make the application of Reinforcement Learning challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- An Introduction to Deep Reinforcement Learning by François-Lavet et al.\n",
    "- Challenges of Real-World Reinforcement Learning by Dulac-Arnold  et al.\n",
    "- Data center cooling using model-predictive control by Lazic et al.\n",
    "- https://deepmind.com/blog/article/deepmind-ai-reduces-google-data-centre-cooling-bill-40\n",
    "- Transforming Cooling Optimization for Green Data Center via Deep Reinforcement Learning by Li et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other applications\n",
    "-------\n",
    "\n",
    "- https://blog.griddynamics.com/deep-reinforcement-learning-for-supply-chain-and-price-optimization/\n",
    "\n",
    "- https://tryolabs.com/blog/price-optimization-machine-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
