{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#The-Deadly-Triad-of-Q-learning\" data-toc-modified-id=\"The-Deadly-Triad-of-Q-learning-1\">The Deadly Triad of Q-learning</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#-What-are-the-3-elements-for-the-Deadly-Triad?\" data-toc-modified-id=\"-What-are-the-3-elements-for-the-Deadly-Triad?-3\"> What are the 3 elements for the Deadly Triad?</a></span></li><li><span><a href=\"#-Why-can-the-Deadly-Triad-be-an-issues?\" data-toc-modified-id=\"-Why-can-the-Deadly-Triad-be-an-issues?-4\"> Why can the Deadly Triad be an issues?</a></span></li><li><span><a href=\"#Example-of-MDP-with-unbounded-estimates\" data-toc-modified-id=\"Example-of-MDP-with-unbounded-estimates-5\">Example of MDP with unbounded estimates</a></span></li><li><span><a href=\"#Where-can-Deadly-Triad-occur?\" data-toc-modified-id=\"Where-can-Deadly-Triad-occur?-6\">Where can Deadly Triad occur?</a></span></li><li><span><a href=\"#-Killing-The-Deadly-Triad-with-Deep-Q-Learning-Networks-(DQN)\" data-toc-modified-id=\"-Killing-The-Deadly-Triad-with-Deep-Q-Learning-Networks-(DQN)-7\"> Killing The Deadly Triad with Deep Q-Learning Networks (DQN)</a></span></li><li><span><a href=\"#-TD(λ)\" data-toc-modified-id=\"-TD(λ)-8\"> TD(λ)</a></span></li><li><span><a href=\"#Example-of-Q-values-overestimation\" data-toc-modified-id=\"Example-of-Q-values-overestimation-9\">Example of Q-values overestimation</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-10\">Takeaways</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-11\">Sources of Inspiration</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>The Deadly Triad of Q-learning</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- List the three elements in the Deadly Triad of Q-learning.\n",
    "- Explain how Deep Q-learning can mitigate the Deadly Triad issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> What are the 3 elements for the Deadly Triad?</h2></center>\n",
    "\n",
    "1. __Value function approximation__: Instead of precisely estimating the exact values for a state, estimate state values from nearby states that share similar features.\n",
    "\n",
    "1. __Bootstrapping__: Update future estimate rewards using previous estimates rather than relying exclusively on actual rewards.\n",
    "\n",
    "1. __Off-policy training__: Target (updated policy(ies) != Behavior Policy (yields the current action). Rewards can update all relevant policies.\n",
    "\n",
    "Each of these is a very powerful innovation that are be applied to solve novel problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Why can the Deadly Triad be an issues?</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Each approximation can compound and interact in non-linear ways.\n",
    "\n",
    "As, a value learning can diverge.\n",
    "\n",
    "Value estimates becoming unbounded:\n",
    "\n",
    "- Large updates\n",
    "- Becomes an expansion operator. Instead of getting closer to a solution with each step, we get father away!\n",
    "\n",
    "\n",
    "Other unpleasantness:\n",
    "\n",
    "- Over-generalization \n",
    "- Extrapolation error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Example of MDP with unbounded estimates</h2></center>\n",
    "\n",
    "<center><img src=\"images/diverge.png\" width=\"75%\"/></center>\n",
    "\n",
    "The system has no rewards, therefore $w_* = 0$. However when an agent samples each state equally often, the estimate of value diverges.\n",
    "\n",
    "\n",
    "Off-policy learning will diverge, aka move away from the true estimate.\n",
    "\n",
    "Source: Tsitsiklis and Van Roy (1997) J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674–690, 1997."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Where can Deadly Triad occur?</h2></center>\n",
    "\n",
    "Deadly Triad can happen in any type of Reinforcement Learning where all three elements are present.\n",
    "\n",
    "Traditional Q-learning is very susceptible to deadly triad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Killing The Deadly Triad with Deep Q-Learning Networks (DQN)</h2></center>\n",
    " \n",
    "Using DQN can stabilize the training:\n",
    "\n",
    "- DQN have increased learning capacity. Larger, more flexible networks diverge less easily, especially when feed with proportionally more data.\n",
    "- Longer multi-step returns, thus less likely to diverge:\n",
    "    - Experience Replay  \n",
    "    - TD(λ) (see below)\n",
    "- Use a variation of Deep Q-learning to correct for overestimation bias:\n",
    "    - Double Deep Q-learning\n",
    "    - Target Deep Q-learning\n",
    "    - Inverse Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Sources:\n",
    "\n",
    "- Sutton & Barton p 264\n",
    "- [Deep Reinforcement Learning and the Deadly Triad](https://arxiv.org/pdf/1812.02648.pdf) from DeepMind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2> TD(λ)</h2></center>\n",
    "    \n",
    "Traditional Q-learning uses the bootstrap - recursively uses its own value estimates:\n",
    "\n",
    "<center><img src=\"images/q.png\" width=\"75%\"/></center>\n",
    "\n",
    "The estimated as the sum of the immediate reward and a contribution of the following steps in the return. That contribution is estimated based on its own value estimate at the next time-step\n",
    "\n",
    "\n",
    "Monte Carlo (MC) is not a non-bootstrapping methods that learns directly from returns (complete episodes).\n",
    "\n",
    "There is intermediate idea: n-step (look at fixed number of steps)\n",
    "<center><img src=\"images/n_step.png\" width=\"75%\"/></center>\n",
    "\n",
    "Also called the n-gram trick. \n",
    "\n",
    "This could be useful in Breakout game to understand velocity.\n",
    "\n",
    "Then extend to add a combination of different multi-steps targets can also be used:\n",
    "\n",
    "<center><img src=\"images/multistep.png\" width=\"75%\"/></center>\n",
    "\n",
    "Temporal difference (TD) learning with a λ parameter to control number of steps to look at.\n",
    "\n",
    "<center><img src=\"https://lilianweng.github.io/lil-log/assets/images/TD_lambda.png\" width=\"75%\"/></center>\n",
    "\n",
    "When λ=1, it is TD (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: An Introduction to Deep Reinforcement Learning by François-Lavet et al.\n",
    "\n",
    "[Image Source](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#combining-td-and-mc-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Example of Q-values overestimation</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/over_estimate.png\" width=\"75%\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: van Hesselt et al, 2018: “Deep Reinforcement Learning and the Deadly Triad”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Reinforcement Learning can be unbounded because of the Deadly Triad:\n",
    "    1. Function approximation\n",
    "    1. Bootstrapping \n",
    "    1. Off-policy training\n",
    "- Deep Q-learning is a good approach because:\n",
    "    - Deep neural networks can do function approximation well.\n",
    "    - Bootstrapping is improved through Experience Replay.\n",
    "    - Double Deep Q-learning can better estimate off-policy values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- https://sureli.github.io/surelib/review/2019/07/09/Studies-of-the-Deadly-Triad/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
