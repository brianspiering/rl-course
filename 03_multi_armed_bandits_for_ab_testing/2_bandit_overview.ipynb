{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Multi-Arm-Bandits:-Reinforcement-Learning-Simplified\" data-toc-modified-id=\"Multi-Arm-Bandits:-Reinforcement-Learning-Simplified-1\">Multi-Arm Bandits: Reinforcement Learning Simplified</a></span></li><li><span><a href=\"#Reinforcement-Learning:-A-simple-definition\" data-toc-modified-id=\"Reinforcement-Learning:-A-simple-definition-2\">Reinforcement Learning: A simple definition</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-3\">Learning Outcomes</a></span></li><li><span><a href=\"#Traditional-A/B-Testing\" data-toc-modified-id=\"Traditional-A/B-Testing-4\">Traditional A/B Testing</a></span></li><li><span><a href=\"#Multi-Arm-Bandit\" data-toc-modified-id=\"Multi-Arm-Bandit-5\">Multi-Arm Bandit</a></span></li><li><span><a href=\"#Where-does-the-name-&quot;bandit&quot;-come-from?\" data-toc-modified-id=\"Where-does-the-name-&quot;bandit&quot;-come-from?-6\">Where does the name \"bandit\" come from?</a></span></li><li><span><a href=\"#Multi-Arm-Bandit\" data-toc-modified-id=\"Multi-Arm-Bandit-7\">Multi-Arm Bandit</a></span></li><li><span><a href=\"#Multi-Arm-Bandit-Approach-\" data-toc-modified-id=\"Multi-Arm-Bandit-Approach--8\">Multi-Arm Bandit Approach </a></span></li><li><span><a href=\"#Student-Activity\" data-toc-modified-id=\"Student-Activity-9\">Student Activity</a></span></li><li><span><a href=\"#Specific-situations-to-choose-A/B-testing-over-multi-arm-bandits-(MAB)\" data-toc-modified-id=\"Specific-situations-to-choose-A/B-testing-over-multi-arm-bandits-(MAB)-10\">Specific situations to choose A/B testing over multi-arm bandits (MAB)</a></span></li><li><span><a href=\"#Old-Favorite-or-Something-New?\" data-toc-modified-id=\"Old-Favorite-or-Something-New?-11\">Old Favorite or Something New?</a></span></li><li><span><a href=\"#Explore-vs.-Exploit--\" data-toc-modified-id=\"Explore-vs.-Exploit---12\">Explore vs. Exploit  </a></span></li><li><span><a href=\"#Language-Note\" data-toc-modified-id=\"Language-Note-13\">Language Note</a></span></li><li><span><a href=\"#Explore-vs.-Exploit--\" data-toc-modified-id=\"Explore-vs.-Exploit---14\">Explore vs. Exploit  </a></span></li><li><span><a href=\"#Explore\" data-toc-modified-id=\"Explore-15\">Explore</a></span></li><li><span><a href=\"#Exploit\" data-toc-modified-id=\"Exploit-16\">Exploit</a></span></li><li><span><a href=\"#Explore-vs-Exploit-in-Different-Environments\" data-toc-modified-id=\"Explore-vs-Exploit-in-Different-Environments-17\">Explore vs Exploit in Different Environments</a></span></li><li><span><a href=\"#Wikipedia-Fundraising-Banners\" data-toc-modified-id=\"Wikipedia-Fundraising-Banners-18\">Wikipedia Fundraising Banners</a></span></li><li><span><a href=\"#Traditional-A/B-Testing--\" data-toc-modified-id=\"Traditional-A/B-Testing---19\">Traditional A/B Testing  </a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-20\">Check for understanding</a></span></li><li><span><a href=\"#Potential-Inefficiencies--\" data-toc-modified-id=\"Potential-Inefficiencies---21\">Potential Inefficiencies  </a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-22\">Check for understanding</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-23\">Check for understanding</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-24\">Check for understanding</a></span></li><li><span><a href=\"#State-≠--Time\" data-toc-modified-id=\"State-≠--Time-25\">State ≠  Time</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-26\">Takeaways</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-27\">Sources of Inspiration</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-28\">Bonus Material</a></span></li><li><span><a href=\"#What-are-the-limitations-of-traditional-A/B-testing?\" data-toc-modified-id=\"What-are-the-limitations-of-traditional-A/B-testing?-29\">What are the limitations of traditional A/B testing?</a></span></li><li><span><a href=\"#MAB-vs-Supervised-Learning\" data-toc-modified-id=\"MAB-vs-Supervised-Learning-30\">MAB vs Supervised Learning</a></span></li><li><span><a href=\"#What-is-&quot;Regret&quot;-in-Explore-/-Exploit?-\" data-toc-modified-id=\"What-is-&quot;Regret&quot;-in-Explore-/-Exploit?--31\">What is \"Regret\" in Explore / Exploit? </a></span></li><li><span><a href=\"#Regret-Formalism\" data-toc-modified-id=\"Regret-Formalism-32\">Regret Formalism</a></span></li><li><span><a href=\"#Regret-assumes-non-real-world-assumptions\" data-toc-modified-id=\"Regret-assumes-non-real-world-assumptions-33\">Regret assumes non-real-world assumptions</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-34\">Check for understanding</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-35\">Check for understanding</a></span></li><li><span><a href=\"#multi-arm-bandits-(MAB)-and-MDPs:-Rewards-vs-Regret\" data-toc-modified-id=\"multi-arm-bandits-(MAB)-and-MDPs:-Rewards-vs-Regret-36\">multi-arm bandits (MAB) and MDPs: Rewards vs Regret</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Multi-Arm Bandits: Reinforcement Learning Simplified</h2></center>\n",
    "\n",
    "<center><img src=\"http://www.offtopia.net/ecai-2012-vago-poster/bandit.png\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Reinforcement Learning: A simple definition</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>\"Randomly try strategies.  </center>\n",
    "<center>If some strategies work, choose those more often.\"  </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Explain the \"why\" and the \"what\" of multi-arm bandits in your own words.\n",
    "- Explain the exploration / exploitation trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>Traditional A/B Testing</h2></center>\n",
    "\n",
    "<center><img src=\"images/ab.png\" height=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First, assign equal numbers of users to Group A and Group B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Then, stop the experiment and send all the users to the more successful version of your website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>Multi-Arm Bandit</h2></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/b.jpg\" width=\"70%\"/></center>\n",
    "\n",
    "<center>Systematically adjust the numbers of samples for each condition based on current results.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Where does the name \"bandit\" come from?\n",
    "-----\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><img src=\"http://images.fineartamerica.com/images-medium-large-5/one-arm-bandit-slot-machine-20130308-wingsdomain-art-and-photography.jpg\" width=\"35%\"/></center>\n",
    "\n",
    "A \"one armed bandit\" is a nickname for a slot machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>Multi-Arm Bandit</h2></center>\n",
    "\n",
    "\n",
    "<center><img src=\"http://www.offtopia.net/ecai-2012-vago-poster/bandit.png\" width=\"50%\"/></center>\n",
    "\n",
    "<center><img src=\"images/arms.png\" width=\"75%\"/></center>\n",
    "\n",
    "bandits (slot machines) = versions of the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Multi-Arm Bandit Approach \n",
    "-----\n",
    "\n",
    "1) Show a user the version of the site that you think is best most of the time. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "2) As the experiment runs, update the belief about the CTR (Click Through Rate) for each version.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "3) Run for however long until we are satisfied we believe we have found the best version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "Pretend you are in a job interview. One person answers each question in 30 seconds - 2 minutes. Switch the person answering after each question.\n",
    "\n",
    "- List the biggest 3 limitations of traditional A/B testing.\n",
    "- How does multi-armed bandit (MAB) address each one?\n",
    "- How does data analysis change between A/B testing and MAB?\n",
    "- Which one is more efficient? Why?\n",
    "- Which one is more ethical? Why?\n",
    "- Why would you choose multi-arm bandits (MAB) over A/B testing?\n",
    "\n",
    "Challenge Question: What are specific situations where you would choose __A/B testing over__ multi-arm bandits (MAB)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Specific situations to choose A/B testing over multi-arm bandits (MAB)</h2></center>\n",
    "\n",
    "1. Unable to implement multi-arm bandit due to technical debt or system complexity.\n",
    "\n",
    "1. You need to strictly control for false positive rates. MAB might have higher false positive rates because more people are assigned to the condition with more observed positive rates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: \"… false positive rate (Type I error rate) is increased due both to the uneven distribution of students across conditions and measurement errors from adaptively changing the design based on previous results.\"\n",
    "\n",
    "from _Statistical Consequences of using Multi- armed Bandits to Conduct Adaptive Educational Experiments_ by Rafferty et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Old Favorite or Something New?</h2></center>\n",
    "\n",
    "<center><img src=\"https://scstylecaster.files.wordpress.com/2014/05/takeout-food.jpg\" width=\"80%\"/></center>\n",
    "\n",
    "If you are getting takeout, should you try something new or go with an old favorite?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>Explore vs. Exploit  </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Language Note</h2></center>\n",
    "\n",
    "__Explore & Exploit__ is conventionally used by the Computer Science / Machine Learning (a group not known for their inclusive language).\n",
    "\n",
    "Alternative terms are:\n",
    "\n",
    "- __Explore & Execute__\n",
    "- __Learn & Earn__ (Brian's favorite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: [Algorithms to Live By: The Computer Science of Human Decisions ](https://www.amazon.com/Algorithms-Live-Computer-Science-Decisions/dp/1627790365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Explore vs. Exploit  </h2></center>\n",
    "\n",
    "<center><img src=\"images/eee.jpg\" width=\"70%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Explore\n",
    "------\n",
    "\n",
    "Trying out different options to find the reward associated each approach (aka, acquiring more knowledge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exploit\n",
    "-----\n",
    "\n",
    "Choosing the approach that you believe has the highest expected reward (aka, optimizing outcomes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/ee.jpg\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Explore vs Exploit in Different Environments</h2></center>\n",
    "\n",
    "1. Short vs Long Term Rewards\n",
    "\n",
    "    - Exploit - Guaranteed increased shorter term rewards\n",
    "\n",
    "    - Explore - Possibility of increased longer term rewards\n",
    "\n",
    "2. Lower vs  Higher Variance\n",
    "\n",
    "    - In lower reward variance environments, more exploitation makes more sense. \n",
    "\n",
    "    - In higher reward variance environments, more exploration makes more sense.\n",
    "3. Stationary vs non-stationary\n",
    "\n",
    "    - In more stationary environments, exploitation makes more sense. Once winner, always a winner.\n",
    "\n",
    "    - In more non-stationary environments, exploration makes more sense. Think about routing people make to normal route after traffic jam. You have \"explore\" the old arm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:  S&B p 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Wikipedia Fundraising Banners</h2></center>\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/FY1617_WMF_Banner_Impressions_Data_Big_English.png/1056px-FY1617_WMF_Banner_Impressions_Data_Big_English.png\" width=\"75%\"/></center>\n",
    "\n",
    "Non stationary data, the experiment itself changes the out of the experiment\n",
    "\n",
    "Source: https://foundation.wikimedia.org/wiki/2016-2017_Fundraising_Report#Online_Fundraising_Banners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Traditional A/B Testing  </h2></center>\n",
    "\n",
    "A short period of __pure exploration__, in which you assign equal numbers of users to Group A and Group B. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A long period of __pure exploitation__, in which you send all of your users to the more successful version of your site and never come back to the option that seemed to be inferior  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Check for understanding\n",
    "-------\n",
    "\n",
    "Why might this be a suboptimal strategy?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Potential Inefficiencies  \n",
    "------\n",
    "\n",
    "- Equal number of observations are routed to A and B for a preset amount of time or iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Only after that preset amount of time or observations we use the consistently better performer. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- This wastes time and __money__ showing users the suboptimal site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Need to wait for the experiment to conclude for certain statistical guarantees to be provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can not adjust for non-stationarity. There could be changes over time:\n",
    "\n",
    "- Revisit near-winners\n",
    "- Seasonality\n",
    "- Adversarial users (e.g., fraud)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Check for understanding\n",
    "----\n",
    "\n",
    "What are examples of where we can apply bandits to reduce inefficiencies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Dynamically A/B testing websites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Adaptive routing in attempts to minimize network delays (either packets 🖥 or packages 📦)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Clinical trial (I would agrue __NOT__ doing bandits is immoral)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* Budget allocation amongst competing projects (not a way to make friends)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "__What the relationship between multi-arm bandits (MAB) and MDPs?__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "MAB only has a single state, the agent starts and ends in the same state.\n",
    "\n",
    "You think of a physical location. The agent is in a always in the same location.\n",
    "\n",
    "\n",
    "Each arm is an action with a reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "__Why is MAB stateless?__\n",
    "\n",
    "Vs. a Markov decision process (MDP) which tracks states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "MAB does happen over time, just always the same set of actions.\n",
    "\n",
    "True RL problems change states, thus different actions are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>State ≠  Time</h2></center>\n",
    "\n",
    "MAB & MDP happen over time (often discrete time).\n",
    "\n",
    "MDP has state, aka different actions are available in different states. Remember Brownie the robot.\n",
    "\n",
    "MAB is stateless (technically has a single state). The actions available to a MAB is always the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- In your life, you need to balance exploration (finding what you like) and exploitation (doing what you like).\n",
    "- Multi-arm bandits is a way of optimizing A/B testing by minimizing regret.\n",
    "- Multi-arm bandits are a simple version of Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- _Are Multi-Armed Bandits Susceptible to Peeking?_ by Loecher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus Material\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "What are the limitations of traditional A/B testing?\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "1. Long time between idea for improvement and most people getting the benefit.\n",
    "\n",
    "1. You showing many people sub-optimal versions of your website (i.e., the losing variety during the experiment).\n",
    "\n",
    "1. Traditional A/B testing is a very manual process. MAB can more easily automated.\n",
    "\n",
    "1. Traditional A/B testing does not handle non-stationary data as easily as MAB.\n",
    "\n",
    "1. Gathering enough traffic to reach statistical significance can take unreasonably long, especially when the full shopping cycle can last days or weeks.\n",
    "\n",
    "1. High cost of exploration. New bad ideas can not be quickly discarded. They have to go through the complete process.\n",
    "\n",
    "1. Even a small set of features can quickly lead to many unique page layouts. Controlled A/B tests are well suited to simple binary decisions, but they do not scale well to many variations that are quickly evolving. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://blog.acolyer.org/2017/11/22/canopy-an-end-to-end-performance-tracing-and-analysis-system/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>MAB vs Supervised Learning</h2></center>\n",
    "\n",
    "Supervised Learning does not learn over time. It justs memorizes the past.\n",
    "\n",
    "MAB is a learning-over-time problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/bezos.jpeg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://medium.com/@savedali/jeff-bezos-guide-to-quitting-medicine-24e16325f159)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is \"Regret\" in Explore / Exploit? </h2></center>\n",
    "\n",
    "The difference of what we actually won vs. what we would expect to win with an optimal strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Regret is the cost function we are trying to minimize at a strategic level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Regret Formalism</h2></center>\n",
    "\n",
    "$$ \n",
    "\\begin{align*} \n",
    "\\text{regret} &= \\sum_{i = 1}^k (p_{opt} - p_i)   \\\\ \n",
    "&= k p_{opt} - \\sum_{i = 1}^k p_i\n",
    "\\end{align*} $$\n",
    "\n",
    "- k trials \n",
    "\n",
    "- $p_i$ is the probability of winning with the bandit chosen on the i-th run.   \n",
    "\n",
    "- $p_{opt}$ is the probability of winning with the best bandit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Regret assumes non-real-world assumptions</h2></center>\n",
    "\n",
    "Regret assume that optional can be measured.\n",
    "\n",
    "In the real-world, optional is only possible in hindsight.\n",
    "\n",
    "Optional is not clearly defined in non-stationary environments.\n",
    "\n",
    "For example, the optional sale price during summer will be different than the optional sale price in winter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What would a regret of zero mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A regret of 0 would mean you always made the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "In what situations can you can guarantee zero regret?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It is __never__ possible since you need to collect data to determine which variation is the best.\n",
    "\n",
    "<sub>Note that you need to know the true probabilities to calculate the regret. This is a theoretical idea to evaluate which algorithm is best.</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>multi-arm bandits (MAB) and MDPs: Rewards vs Regret</h2></center>\n",
    "\n",
    "Maximizing cumulative rewards is the goal of RL / MDP.  \n",
    "Minimizing regret is the goal of MAB.\n",
    "\n",
    "They are the very similar:\n",
    "Maximize cumulative reward = minimize total regret "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
