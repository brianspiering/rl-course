Multi-arm bandits
----

__Introductory__:

- Multi-Armed Bandits Intro (15:41) [Video](https://www.youtube.com/watch?v=qAvY2tkMHHA)

__Required__:

- Review course notes from MSDS's Design of Experiments Course.
- Be familiar with the following code. __You be using it for the live coding during class.__
    - `bandit.py` in this folder. It is the framework for the algorithms.
    - [`scipy.stats.beta`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html). Be familiar with __all__ methods.
    - We'll be implementing the following multi-armed bandit algorithms in class:
        + Random 
        + Epsilon-greedy
        + Soft-max
        + Bayesian bandits (aka, Thompson sampling)

__Optional__:

The Stanford instructor chooses to put multi-arm bandits at the end of her course and I choose to put it at the beginning. Given that you have already have seen the material in Design of Experiments, it should not be too difficult to follow along.

There is __no__ prework check since you have all taken Design of Experiments Course.

- Stanford CS234: Reinforcement Learning | Lecture 11 - Fast Reinforcement Learning
    + [Video](https://www.youtube.com/watch?v=RN8qpSs8ozY&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&index=11)
    + [Slides & Draft lecture notes](http://web.stanford.edu/class/cs234/schedule.html)
- Stanford CS234: Reinforcement Learning | Lecture 12 - Fast Reinforcement Learning II
    + [Video](https://www.youtube.com/watch?v=jJ7JbQBTChM&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&index=12)
    + [Slides & Draft lecture notes](http://web.stanford.edu/class/cs234/schedule.html)
- Stanford CS234: Reinforcement Learning | Lecture 13 - Fast Reinforcement Learning III
    + [Video](https://www.youtube.com/watch?v=Hg_uyWezMM0&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&index=13)
    + [Slides & Draft lecture notes](http://web.stanford.edu/class/cs234/schedule.html)

- SB (Sutton and Barton) Chp 2 (theory) & Chp 16.7 (application)
- [20 lines of code that will beat A/B testing every time](http://stevehanov.ca/blog/index.php?id=132)
- [Efficient experimentation and the multi-armed bandit](http://iosband.github.io/2015/07/19/Efficient-experimentation-and-multi-armed-bandits.html)
- [Multi-Armed Bandits](http://blog.thedataincubator.com/2016/07/multi-armed-bandits-2/)
- [Beer bandit](http://blog.yhat.com/posts/the-beer-bandit.html)
- [Multi-Armed Bandits](https://dataorigami.net/blogs/napkin-folding/79031811-multi-armed-bandits) 

__Challenge__:

- Reinforcement Learning and Multi-Armed Bandits (40 minutes)
    + [Video](https://www.youtube.com/watch?v=aAdD2XRC044)
    + [Slides](https://github.com/brianfarris/RLtalk/blob/master/RLtalk.ipynb)
- [Using multi-arm bandits for high-growth products](http://www.unofficialgoogledatascience.com/2019/04/misadventures-in-experiments-for-growth.html)
- [A collection of Bandit algorithms](http://banditalgs.com/)
- ["Taking the Human Out of the Loop: A Review of Bayesian Optimization"](https://ieeexplore.ieee.org/document/7352306)
- [Bandit Algorithms for Website Optimization](http://shop.oreilly.com/product/0636920027393.do) (book)
- [Sigopt's blog](https://blog.sigopt.com/)