{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Multi-arm-Bandits-Algorithms\" data-toc-modified-id=\"Multi-arm-Bandits-Algorithms-1\">Multi-arm Bandits Algorithms</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Random-Algorithm\" data-toc-modified-id=\"Random-Algorithm-3\">Random Algorithm</a></span></li><li><span><a href=\"#Let's-Code-a-Bandit\" data-toc-modified-id=\"Let's-Code-a-Bandit-4\">Let's Code a Bandit</a></span></li><li><span><a href=\"#Student-Activity\" data-toc-modified-id=\"Student-Activity-5\">Student Activity</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-6\">Check for understanding</a></span></li><li><span><a href=\"#Epsilon-Greedy-Algorithm\" data-toc-modified-id=\"Epsilon-Greedy-Algorithm-7\">Epsilon-Greedy Algorithm</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-8\">Check for understanding</a></span></li><li><span><a href=\"#Epsilon-Default-Value\" data-toc-modified-id=\"Epsilon-Default-Value-9\">Epsilon Default Value</a></span></li><li><span><a href=\"#Student-Activity\" data-toc-modified-id=\"Student-Activity-10\">Student Activity</a></span></li><li><span><a href=\"#A-Common-Real-world-Design-Pattern-for-Epsilon-Greedy-\" data-toc-modified-id=\"A-Common-Real-world-Design-Pattern-for-Epsilon-Greedy--11\">A Common Real-world Design Pattern for Epsilon-Greedy </a></span></li><li><span><a href=\"#What-are-limitations-of-Epsilon-Greedy-algorithm?\" data-toc-modified-id=\"What-are-limitations-of-Epsilon-Greedy-algorithm?-12\">What are limitations of Epsilon-Greedy algorithm?</a></span></li><li><span><a href=\"#-Softmax-Algorithm\" data-toc-modified-id=\"-Softmax-Algorithm-13\"> Softmax Algorithm</a></span></li><li><span><a href=\"#Student-Activity\" data-toc-modified-id=\"Student-Activity-14\">Student Activity</a></span></li><li><span><a href=\"#Softmax-Limitation\" data-toc-modified-id=\"Softmax-Limitation-15\">Softmax Limitation</a></span></li><li><span><a href=\"#Bayesianian-Updates\" data-toc-modified-id=\"Bayesianian-Updates-16\">Bayesianian Updates</a></span></li><li><span><a href=\"#Bayesian-Bandit-(or-Thompson-Sampling)-Algorithm\" data-toc-modified-id=\"Bayesian-Bandit-(or-Thompson-Sampling)-Algorithm-17\">Bayesian Bandit (or Thompson Sampling) Algorithm</a></span></li><li><span><a href=\"#Bayesian-Bandit-(or-Thompson-Sampling)\" data-toc-modified-id=\"Bayesian-Bandit-(or-Thompson-Sampling)-18\">Bayesian Bandit (or Thompson Sampling)</a></span></li><li><span><a href=\"#Thompson-Sampling-for-Bernoulli-Bandits\" data-toc-modified-id=\"Thompson-Sampling-for-Bernoulli-Bandits-19\">Thompson Sampling for Bernoulli Bandits</a></span></li><li><span><a href=\"#Student-Activity\" data-toc-modified-id=\"Student-Activity-20\">Student Activity</a></span></li><li><span><a href=\"#Simulate-Results-With-The-Implementations\" data-toc-modified-id=\"Simulate-Results-With-The-Implementations-21\">Simulate Results With The Implementations</a></span></li><li><span><a href=\"#Which-MAB-strategy-should-I-use?\" data-toc-modified-id=\"Which-MAB-strategy-should-I-use?-22\">Which MAB strategy should I use?</a></span></li><li><span><a href=\"#Challenge-Question\" data-toc-modified-id=\"Challenge-Question-23\">Challenge Question</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-24\">Takeaways</a></span></li><li><span><a href=\"#Bonus-Materials\" data-toc-modified-id=\"Bonus-Materials-25\">Bonus Materials</a></span></li><li><span><a href=\"#Full-Bayesian-updating\" data-toc-modified-id=\"Full-Bayesian-updating-26\">Full Bayesian updating</a></span></li><li><span><a href=\"#Which-strategy-should-I-use?\" data-toc-modified-id=\"Which-strategy-should-I-use?-27\">Which strategy should I use?</a></span></li><li><span><a href=\"#Upper-Confidence-Bound-(UCB)\" data-toc-modified-id=\"Upper-Confidence-Bound-(UCB)-28\">Upper Confidence Bound (UCB)</a></span></li><li><span><a href=\"#Upper-Confidence-Bound-(UCB)\" data-toc-modified-id=\"Upper-Confidence-Bound-(UCB)-29\">Upper Confidence Bound (UCB)</a></span></li><li><span><a href=\"#Upper-Confidence-Bound-(UCB)-Features\" data-toc-modified-id=\"Upper-Confidence-Bound-(UCB)-Features-30\">Upper Confidence Bound (UCB) Features</a></span></li><li><span><a href=\"#Upper-Confidence-Bound-(UCB)-Limitations\" data-toc-modified-id=\"Upper-Confidence-Bound-(UCB)-Limitations-31\">Upper Confidence Bound (UCB) Limitations</a></span></li><li><span><a href=\"#Further-Study\" data-toc-modified-id=\"Further-Study-32\">Further Study</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Multi-arm Bandits Algorithms</h2></center>\n",
    "\n",
    "\n",
    "<center><img src=\"http://i1.wp.com/banditalgs.com/wp-content/uploads/2016/09/cropped-bandit-algorithm-full.png?fit=512%2C512\" width=\"35%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "Explain and code the following multi-arm bandits algorithms:\n",
    "\n",
    "1. Random\n",
    "1. Epsilon-Greedy Algorithm  \n",
    "2. Softmax  \n",
    "4. Bayesian Bandit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Random Algorithm</h2></center>\n",
    "\n",
    "1) Pure exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Good for checking the entire system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) Reasonable baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Let's Code a Bandit</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "palette = \"Dark2\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import common packages\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Seed the random number generators so you get the same results every time.\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Bandit is the class that setups and runs our multi-arm bandit\n",
    "from bandit import Bandit\n",
    "\n",
    "# Define function to always pick the same arm\n",
    "def fixed(self):\n",
    "    \"Always pull the same bandit arm everytime.\"\n",
    "    return 0 # Index zero / 1st arm\n",
    "\n",
    "# Let's instantiate the class and simulate outcomes\n",
    "b = Bandit(p_array=[0.6, 0.6, 0.6, 0.6],\n",
    "           choice_function=fixed)\n",
    "b.sample_bandits(n=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.60   0.60   0.60   0.60\n",
      "Number of wins:      613      0      0      0\n",
      "Number of trials:   1000      0      0      0\n",
      "Conversion rates:   0.61    nan    nan    nan\n",
      "\n",
      "A total of 613 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "from bandit import print_results\n",
    "\n",
    "print_results(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.60   0.60   0.60   0.60\n",
      "Number of wins:      570      0      0      0\n",
      "Number of trials:   1000      0      0      0\n",
      "Conversion rates:   0.57    nan    nan    nan\n",
      "\n",
      "A total of 570 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "# Another way to do it\n",
    "# Let's instantiate the class and simulate outcomes\n",
    "b = Bandit(p_array=[0.6, 0.6, 0.6, 0.6],\n",
    "           choice_function=(lambda *_: 0) # A nameless function that only returns zeros everytime it is called\n",
    "          )\n",
    "b.sample_bandits(n=1_000)\n",
    "print_results(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "Implement the following \n",
    "\n",
    "```python\n",
    "def random_choice(self): \n",
    "    \"Pick a bandit uniformly at random.\"\n",
    "    pass\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "Why use `np.random.randint` instead of `random.choice`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's instantiate the class and simulate outcomes\n",
    "b = Bandit(p_array=[0.6, 0.6, 0.6, 0.6],\n",
    "           choice_function=random_choice)\n",
    "b.sample_bandits(n=1_000)\n",
    "# Let's see which arm is the winner\n",
    "b.max_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.60   0.60   0.60   0.60\n",
      "Number of wins:      153    138    144    142\n",
      "Number of trials:    248    247    255    250\n",
      "Conversion rates:   0.62   0.56   0.56   0.57\n",
      "\n",
      "A total of 577 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "from bandit import print_results\n",
    "\n",
    "print_results(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/eg.jpg\" width=\"55%\"/></center>\n",
    "\n",
    "Some percent of the time we explore - randomly choose one of the options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The rest of the time we exploit - choose the option that has so far had the highest conversion rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Epsilon-Greedy Algorithm</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/epislon_greedy.png\" width=\"75%\"/></center>\n",
    "\n",
    "$a$ is action.  \n",
    "$t$ is time.  \n",
    "$a^*$ is optimal action.  \n",
    "$ε$ epsilon - a hyper-parameter to choose the probability that we explore.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What is the range for epsilon?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Epsilon Default Value</h2></center>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "Implement the following \n",
    "\n",
    "```python\n",
    "def epsilon_greedy(self, epsilon=0.1):\n",
    "    \"\"\"Pick a bandit uniformly at random epsilon percent of the time.\n",
    "    Otherwise pick the bandit with the best observed proportion of winning.\n",
    "    Return the index of the winning bandit.\"\"\"\n",
    "    pass\n",
    "```               \n",
    "\n",
    "It is okay to look at course notes and library documentation.\n",
    "\n",
    "Write the code without looking at other people's implementation.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.90   0.60   0.60   0.60\n",
      "Number of wins:      850     15     13     13\n",
      "Number of trials:    934     21     21     24\n",
      "Conversion rates:   0.91   0.71   0.62   0.54\n",
      "\n",
      "A total of 891 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate the class and simulate outcomes\n",
    "b = Bandit(p_array=[0.9, 0.6, 0.6, 0.6],\n",
    "                    choice_function=epsilon_greedy)\n",
    "b.sample_bandits(n=1_000)\n",
    "# Let's see which arm is the winner\n",
    "print_results(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>A Common Real-world Design Pattern for Epsilon-Greedy </h2></center>\n",
    "    \n",
    "1. At the beginning, set epsilon high (.15-.25).\n",
    "1. After fix number of trials, lower (.05-.1).\n",
    "1. Keep low and fixed for remaining trails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is similar to a fixed schedule for learning rate decay for stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are limitations of Epsilon-Greedy algorithm?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) At the beginning, we might select exploitation but do __not__ yet know which version is right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) If epsilon is .10, the epsilon-greedy algorithm will eventually show the best option 90% of the time. \n",
    "\n",
    "So this means that 10% of the time the algorithm will continue to randomly show different versions of the site, no matter how confident we are that a certain version is the best. There is a point where we should stop the exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Softmax Algorithm</h2></center>\n",
    "\n",
    "Choose an arm in proportion to its estimated value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Say $p_A$, $p_B$ and $p_C$ are the conversion rates for three versions of the site. We would choose site A with the following probability  \n",
    "\n",
    "$$ \\frac{exp(p_A/ \\tau)}{exp(p_A/ \\tau) + exp(p_B/ \\tau) + exp(p_C/ \\tau)} $$\n",
    "\n",
    "$\\tau$ is a chosen parameter that controls the ‘randomness’ of the choice, usually around 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "Implement the following \n",
    "\n",
    "```python\n",
    "def softmax(self, tau=0.01):\n",
    "    \"\"\"Pick an bandit according to the Boltzman Distribution.\n",
    "    Return the index of the winning bandit.\"\"\"\n",
    "    pass\n",
    "```                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint:\n",
    "\n",
    "# math.exp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.90   0.60   0.60   0.60\n",
      "Number of wins:      916      0      0      1\n",
      "Number of trials:    996      1      1      2\n",
      "Conversion rates:   0.92   0.00   0.00   0.50\n",
      "\n",
      "A total of 917 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate the class and simulate outcomes\n",
    "b = Bandit(p_array=[0.9, 0.6, 0.6, 0.6],\n",
    "                    choice_function=softmax)\n",
    "b.sample_bandits(n=1_000)\n",
    "# Let's see which arm is the winner\n",
    "print_results(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Softmax Limitation</h2></center>\n",
    "\n",
    "It does __not__ take into account how many times an arm has been tried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Bayesianian Updates</h2></center>\n",
    "\n",
    "<center><img src=\"images/bayesian updates.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayesian Bandit (or Thompson Sampling) Algorithm</h2></center>\n",
    "\n",
    "\n",
    "\n",
    "1. Calculate Posterior for each \"arm\" / action\n",
    "2. Pick a random sample from each Posterior\n",
    "3. Choose the arm with highest reward\n",
    "4. Update Posteriors for next round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bayesian Bandit (or Thompson Sampling)</h2></center>\n",
    "\n",
    "<br>\n",
    "<center><img src=\"http://fastml.com/images/ab-testing/bandits_small.png\" width=\"80%\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<center><a href=\"https://learnforeverlearn.com/bandits/\">Demo!</a></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Thompson Sampling for Bernoulli Bandits</h2></center>\n",
    "\n",
    "Given k bandits, each k pays off:\n",
    "\n",
    "- Reward of 1 with probability $θ_k$ \n",
    "- Reward of 0 with probability 1 − $θ_k$\n",
    "\n",
    "Parameterization of the Beta family of distribution $θ ∼ Beta(α, β)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$\\alpha = 1 + $ number of times bandit has won   \n",
    "$\\beta = 1 + $ number of times bandit has lost  \n",
    "\n",
    "Thus a  simple (and effective) Posterior is:\n",
    "\n",
    "$$Beta_k(1 + n_{k0}, 1 + n_{k1})$$\n",
    "\n",
    "where $n_{k0}$ is the number of losses for arm k and $n_{k1}$ is the number of wins for arm k\n",
    "\n",
    "[Source](https://davidrosenberg.github.io/mlcourse/in-prep/thompson-sampling-bernoulli.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "Implement the following \n",
    "\n",
    "```python\n",
    "def bayesian_bandit(self):\n",
    "    \"\"\"Randomly sample from a beta distribution for each bandit and pick the one\n",
    "    with the largest value.\n",
    "    Return the index of the winning bandit.\"\"\"    \n",
    "    pass\n",
    "```     \n",
    "\n",
    "Hints: \n",
    "\n",
    "- Use `scipy.stats.beta`\n",
    "- Seriously - use the all the methods on `scipy.stats.beta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.90   0.60   0.60   0.60\n",
      "Number of wins:      883      8      4      3\n",
      "Number of trials:    973     13      8      6\n",
      "Conversion rates:   0.91   0.62   0.50   0.50\n",
      "\n",
      "A total of 898 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "# Let's instantiate the class and simulate outcomes\n",
    "b = Bandit(p_array=[0.9, 0.6, 0.6, 0.6],\n",
    "                    choice_function=bayesian_bandit)\n",
    "b.sample_bandits(n=1_000)\n",
    "# Let's see which arm is the winner\n",
    "print_results(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Simulate Results With The Implementations</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################\n",
      "\n",
      "Epsilon_Greedy\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.00   0.00   0.00   0.00\n",
      "Number of wins:        0      0      0      0\n",
      "Number of trials:    919     20     34     27\n",
      "Conversion rates:   0.00   0.00   0.00   0.00\n",
      "\n",
      "A total of 0 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Softmax\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.00   0.00   0.00   0.00\n",
      "Number of wins:        0      0      0      0\n",
      "Number of trials:    240    247    269    244\n",
      "Conversion rates:   0.00   0.00   0.00   0.00\n",
      "\n",
      "A total of 0 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Bayesian_Bandit\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.00   0.00   0.00   0.00\n",
      "Number of wins:        0      0      0      0\n",
      "Number of trials:    248    249    249    254\n",
      "Conversion rates:   0.00   0.00   0.00   0.00\n",
      "\n",
      "A total of 0 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "p_array = [0]*4 # No winners\n",
    "# p_array = [1]*4 # All winners\n",
    "# p_array = [.5]*4 # All the same\n",
    "# p_array = [.01, .4, .8, .999] # All different\n",
    "\n",
    "strategies = [epsilon_greedy, softmax, bayesian_bandit]\n",
    "\n",
    "for strategy in strategies:\n",
    "    b = Bandit(p_array=p_array,\n",
    "                choice_function=strategy)\n",
    "    b.sample_bandits(n=1_000) # 1000 is a good value\n",
    "    print()\n",
    "    print('#'*50)\n",
    "    print()\n",
    "    print(strategy.__name__.title())\n",
    "    print_results(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################################################\n",
      "\n",
      "Epsilon_Greedy\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.90\n",
      "Number of wins:        6      0      6    819\n",
      "Number of trials:     36     22     27    915\n",
      "Conversion rates:   0.17   0.00   0.22   0.90\n",
      "\n",
      "A total of 831 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Softmax\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.90\n",
      "Number of wins:        0      1      0    888\n",
      "Number of trials:      4      1      2    993\n",
      "Conversion rates:   0.00   1.00   0.00   0.89\n",
      "\n",
      "A total of 889 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Bayesian_Bandit\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.90\n",
      "Number of wins:        0      0      0    896\n",
      "Number of trials:      3      3      3    991\n",
      "Conversion rates:   0.00   0.00   0.00   0.90\n",
      "\n",
      "A total of 896 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Epsilon_Greedy\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.12\n",
      "Number of wins:       87      2      2      5\n",
      "Number of trials:    893     28     25     54\n",
      "Conversion rates:   0.10   0.07   0.08   0.09\n",
      "\n",
      "A total of 96 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Softmax\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.12\n",
      "Number of wins:        0    106      0      0\n",
      "Number of trials:      4    992      1      3\n",
      "Conversion rates:   0.00   0.11   0.00   0.00\n",
      "\n",
      "A total of 106 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Bayesian_Bandit\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.10   0.10   0.12\n",
      "Number of wins:       36     19     51      2\n",
      "Number of trials:    311    176    463     50\n",
      "Conversion rates:   0.12   0.11   0.11   0.04\n",
      "\n",
      "A total of 108 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Epsilon_Greedy\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.20   0.40   0.50\n",
      "Number of wins:        2      3     16    470\n",
      "Number of trials:     31     21     40    908\n",
      "Conversion rates:   0.06   0.14   0.40   0.52\n",
      "\n",
      "A total of 491 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Softmax\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.20   0.40   0.50\n",
      "Number of wins:        0      1      1    500\n",
      "Number of trials:      1      3      2    994\n",
      "Conversion rates:   0.00   0.33   0.50   0.50\n",
      "\n",
      "A total of 502 wins out of 1,000 trials.\n",
      "\n",
      "##################################################\n",
      "\n",
      "Bayesian_Bandit\n",
      "Options: \t       A      B      C      D\n",
      "Probablities: \t    0.10   0.20   0.40   0.50\n",
      "Number of wins:        1      2     63    412\n",
      "Number of trials:     12     16    153    819\n",
      "Conversion rates:   0.08   0.12   0.41   0.50\n",
      "\n",
      "A total of 478 wins out of 1,000 trials.\n"
     ]
    }
   ],
   "source": [
    "probs ={'atlantic_city': [0.1, 0.1, 0.1, 0.90],\n",
    "        'las_vegas':     [0.1, 0.1, 0.1, 0.12],\n",
    "        'reno':          [0.1, 0.2, 0.4, 0.50]}\n",
    "\n",
    "for prob in  probs.items():\n",
    "    p_array = prob[1]\n",
    "    for strategy in strategies:\n",
    "        b = Bandit(p_array=p_array,\n",
    "                    choice_function=strategy)\n",
    "        b.sample_bandits(n=1_000) # 1000 is a good value\n",
    "        print()\n",
    "        print('#'*50)\n",
    "        print()\n",
    "        print(strategy.__name__.title())\n",
    "        print_results(b)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
