{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#What-are-the-general-limitations-of-Deep-Q-Learning-Networks-(DQN)?\" data-toc-modified-id=\"What-are-the-general-limitations-of-Deep-Q-Learning-Networks-(DQN)?-1\">What are the general limitations of Deep Q-Learning Networks (DQN)?</a></span></li><li><span><a href=\"#Common-bugs-in-DQN\" data-toc-modified-id=\"Common-bugs-in-DQN-2\">Common bugs in DQN</a></span></li><li><span><a href=\"#Instability-in-DQN\" data-toc-modified-id=\"Instability-in-DQN-3\">Instability in DQN</a></span></li><li><span><a href=\"#Overweighted-Outliers\" data-toc-modified-id=\"Overweighted-Outliers-4\">Overweighted Outliers</a></span></li><li><span><a href=\"#-Goldilocks-Neural-Network-Size\" data-toc-modified-id=\"-Goldilocks-Neural-Network-Size-5\"> Goldilocks Neural Network Size</a></span></li><li><span><a href=\"#Maximization-bias\" data-toc-modified-id=\"Maximization-bias-6\">Maximization bias</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-7\">Takeaways</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are the general limitations of Deep Q-Learning Networks (DQN)?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Takes a LOOOOONG time. Training Atari Breakout took \"almost a week on a single GPU\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Requires an end-to-end continuous function (because the networks needs to take the derivative during backpropagation). Not all systems are end-to-end continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Inherently, not very good at exploration. Lack of exploration is apparent in hierarchical games, like Montezuma's Revenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Common bugs in DQN</h2></center>\n",
    "\n",
    "- Instability\n",
    "- Overweighted outliers\n",
    "- Incorrect neural network size\n",
    "- Maximization bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2>Instability in DQN</h2></center>\n",
    " \n",
    "Problem - Each update can be large, thus sarameters values change wildly.\n",
    "\n",
    "What are possible solutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Increase batch size.\n",
    "- Increase the amount of older data in replay buffer.\n",
    "- Decrease either Q-learning rate or SGD learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Overweighted Outliers</h2></center>\n",
    "\n",
    "Q-learning typically learns a regression / MSE loss function:\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\Sigma_{i=1}^{n}{({y_i -\\hat{y_i}}^2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We could switch loss functions to clip or attenuate extreme updates.\n",
    "\n",
    "What are other possible loss functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://i.stack.imgur.com/vXMgz.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " [Source](https://learning.oreilly.com/library/view/machine-learning-algorithms/9781789347999/7159b3a9-c481-4227-9641-0b0b59cac4ff.xhtml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Goldilocks Neural Network Size</h2></center>\n",
    "\n",
    "If the deep learning network is too small, it may fail to approximate the Q function properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If the deep learning network is too big, it may overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Maximization bias</h2></center>\n",
    "\n",
    "$$Q(s, a) = r_0 + \\gamma \\max_a Q^*(s', a)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The $max$ function is trouble:\n",
    "\n",
    "1. Non-linear \n",
    "1. The model is bias towards overestimation of the Q functionâ€™s value, thus leads to poor performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Double Q-learning can help to mitigate maximization bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- There is great potential power with Deep Q-Learning Networks (DQN).\n",
    "- Those strengths are also weakness. DQN can over learn.\n",
    "- Use fundamental machine learning techniques to minimize the potential weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
