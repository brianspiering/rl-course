{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Deep-Reinforcement-Learning\" data-toc-modified-id=\"Deep-Reinforcement-Learning-1\">Deep Reinforcement Learning</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#&quot;Playing-Atari-with-Deep-Reinforcement-Learning&quot;-from-DeepMind\" data-toc-modified-id=\"&quot;Playing-Atari-with-Deep-Reinforcement-Learning&quot;-from-DeepMind-3\">\"Playing Atari with Deep Reinforcement Learning\" from DeepMind</a></span></li><li><span><a href=\"#Deep-Reinforcement-Learning-for-video-games\" data-toc-modified-id=\"Deep-Reinforcement-Learning-for-video-games-4\">Deep Reinforcement Learning for video games</a></span></li><li><span><a href=\"#Reinforcement-Learning-can-handle-sparse-and-time-delayed-labels.\" data-toc-modified-id=\"Reinforcement-Learning-can-handle-sparse-and-time-delayed-labels.-5\">Reinforcement Learning can handle <b>sparse</b> and <b>time-delayed</b> labels.</a></span></li><li><span><a href=\"#Deep-Reinforcement-Learning\" data-toc-modified-id=\"Deep-Reinforcement-Learning-6\">Deep Reinforcement Learning</a></span></li><li><span><a href=\"#Why-deep-learning-for-Reinforcement-Learning?\" data-toc-modified-id=\"Why-deep-learning-for-Reinforcement-Learning?-7\">Why deep learning for Reinforcement Learning?</a></span></li><li><span><a href=\"#What-is-the-key-data-structure-for-Deep-Q-learning-(DQN)?\" data-toc-modified-id=\"What-is-the-key-data-structure-for-Deep-Q-learning-(DQN)?-8\">What is the key data structure for Deep Q-learning (DQN)?</a></span></li><li><span><a href=\"#Replay-Buffer\" data-toc-modified-id=\"Replay-Buffer-9\">Replay Buffer</a></span></li><li><span><a href=\"#Different-Data-Structures\" data-toc-modified-id=\"Different-Data-Structures-10\">Different Data Structures</a></span></li><li><span><a href=\"#Why-“Experience-Replay”-batching-is-useful\" data-toc-modified-id=\"Why-“Experience-Replay”-batching-is-useful-11\">Why “Experience Replay” batching is useful</a></span></li><li><span><a href=\"#Frames-over-time-with-Reward-values\" data-toc-modified-id=\"Frames-over-time-with-Reward-values-12\">Frames over time with Reward values</a></span></li><li><span><a href=\"#Experience-Replay-as-Regularization-for-traditional-Q-learning\" data-toc-modified-id=\"Experience-Replay-as-Regularization-for-traditional-Q-learning-13\">Experience Replay as Regularization for traditional Q-learning</a></span></li><li><span><a href=\"#Stationary-vs.-Nonstationary-data\" data-toc-modified-id=\"Stationary-vs.-Nonstationary-data-14\">Stationary vs. Nonstationary data</a></span></li><li><span><a href=\"#-Nonstationary-Data-FTW-(literally-winning)\" data-toc-modified-id=\"-Nonstationary-Data-FTW-(literally-winning)-15\"> Nonstationary Data FTW (literally winning)</a></span></li><li><span><a href=\"#Actual-DQN-Results\" data-toc-modified-id=\"Actual-DQN-Results-16\">Actual DQN Results</a></span></li><li><span><a href=\"#-Experience-replay\" data-toc-modified-id=\"-Experience-replay-17\"> Experience replay</a></span></li><li><span><a href=\"#-Experience-Replay-Advantages-over-Traditional-Q-learning\" data-toc-modified-id=\"-Experience-Replay-Advantages-over-Traditional-Q-learning-18\"> Experience Replay Advantages over Traditional Q-learning</a></span></li><li><span><a href=\"#Grab-bag-of-helpful-techniques-for-DQN\" data-toc-modified-id=\"Grab-bag-of-helpful-techniques-for-DQN-19\">Grab-bag of helpful techniques for DQN</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-20\">Takeaways</a></span></li><li><span><a href=\"#-Bonus-Material\" data-toc-modified-id=\"-Bonus-Material-21\"> Bonus Material</a></span></li><li><span><a href=\"#Self-Driving-Car-Example\" data-toc-modified-id=\"Self-Driving-Car-Example-22\">Self-Driving Car Example</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Deep Reinforcement Learning</h2></center>\n",
    "\n",
    "<center><img src=\"https://ih1.redbubble.net/image.311665146.8095/ra%2Crelaxed_fit%2Cx1100%2Cathletic_heather%2Cfront-c%2C260%2C195%2C225%2C375-pad%2C220x294%2Cffffff.lite-1u1.jpg\" width=\"40%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Explain the connection between Q-learning and Deep Q-learning Network (DQN).\n",
    "- Describe how \"Experience Replay\" enables DQN.\n",
    "- List the advantages and disadvantages of Experience Replay.\n",
    "- List the common limitations of DQN and how to overcome each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice at first the Q-values Q(s, a) are random. \n",
    "\n",
    "Then it learns to do mostly nothing. Sometimes move left or right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>\"Playing Atari with Deep Reinforcement Learning\" from DeepMind</h2></center>\n",
    "\n",
    "<center><img src=\"images/games.png\" width=\"100%\"/></center>\n",
    "Games: Pong, Breakout, Space Invaders, Seaquest,  Beam Rider\n",
    "\n",
    "DeepMind demonstrated that a computer can learn to play Atari video games by observing just the screen pixels and receiving a reward when the game score increased. \n",
    "\n",
    "The result was remarkable because the game play and the goals were very different in each game. \n",
    "\n",
    "The games were designed to be challenging for humans. \n",
    "\n",
    "The same model architecture, without any change, was used to learn 7 different games.\n",
    "\n",
    "In 3 of them, the algorithm out performed a human!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Read the DeepMind Atari paper](http://arxiv.org/pdf/1312.5602v1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Deep Reinforcement Learning for video games</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Pixel learning is difficult (but very flexible).\n",
    "\n",
    "In a future lab, we'll be using a lower-state representation of Catch, a simplified version of Pong.\n",
    "\n",
    "OpenAI frequently uses a API output as representation of a game state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Reinforcement Learning can handle <b>sparse</b> and <b>time-delayed</b> labels.</h2></center>\n",
    "\n",
    "<center><img src=\"images/rewards.jpg\" width=\"75%\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Deep Reinforcement Learning</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/image-2.png\" width=\"75%\"/></center>\n",
    "\n",
    "Leverage the power of Deep Neural Networks (DNN) in a scalable fashion with the flexibility of Reinforcement Learning (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/going_deep.png\" width=\"75%\"/></center> \n",
    "\n",
    "<center>DQN replace table with a neural network</center>\n",
    "\n",
    "<center><img src=\"images/dqn.png\" width=\"100%\"/></center>\n",
    "\n",
    "[Image Source](http://people.csail.mit.edu/hongzi/content/publications/DeepRM-HotNets16.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/DQN_overall.png\" width=\"100%\"/></center> \n",
    "\n",
    "For Breakout, actions:\n",
    "\n",
    "- Nothing\n",
    "- Left\n",
    "- Right\n",
    "- Fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Image source](http://karpathy.github.io/2016/05/31/rl/)\n",
    "\n",
    "[Read the classic Deep Q-Learning paper](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Why deep learning for Reinforcement Learning?</h2></center>\n",
    "\n",
    "- Learn state representations\n",
    "- Learn value function\n",
    "- Learn policy\n",
    "- In the future, learn a more complete simulated environment of the world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is the key data structure for Deep Q-learning (DQN)?</h2></center>\n",
    "\n",
    " <center><img src=\"images/experience_replay.png\" width=\"75%\"/></center>\n",
    " \n",
    "<center>“Experience Replay”</center>\n",
    "\n",
    "During learning, a DQN is trained on a pool of stored episodes called the replay buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Replay Buffer</h2></center>\n",
    "\n",
    "<center><img src=\"images/replay_buffer.png\" width=\"75%\"/></center>\n",
    "\n",
    "- Store a large collection of $(s, a, r, s′)$ transitions.\n",
    "- Randomly sample batches from buffer. \n",
    "- Train neural networks with fixed length batches ( Deep Learning systems prefer fixed length inputs).\n",
    "- Limited cache size (new data kicks out old data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Different Data Structures</h2></center>\n",
    "    \n",
    "1. Observations -  individual tuples (s, a, r, s′)\n",
    "1. Buffer - collection of ordered tuples\n",
    "1. Cache - variable length most important tuples\n",
    "1. Batch - fixed length collection of tuples sent to Deep Learning network for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Limited size cache has 2 advantages:\n",
    "    \n",
    "1. Computer memory will not be completely filled up.\n",
    "1. Hopefully the model as the learns - older, worse gameplay will be \"forgotten\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://www.intel.ai/demystifying-deep-reinforcement-learning/#gs.i7eg8u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>Why “Experience Replay” batching is useful</h2></center>\n",
    "\n",
    "- Batch state to reduce state space (overall size). \n",
    "- Batch reward to increase density of reward and learn transitions (reduce sparsity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Frames over time with Reward values</h2></center>\n",
    "<br>\n",
    "<center><img src=\"https://adeshpande3.github.io/assets/IRL14.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2MBERISGBUYLxoaL2NCOEJjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY//AABEIAWgB4AMBIgACEQEDEQH/xAAbAAEAAgMBAQAAAAAAAAAAAAAABQYCAwQBB//EAEYQAAEDAgIFBwkGBQIGAwAAAAABAgMEEQUhBhIWMdETIkFRVJGSFBU0U1Vhc4GxMlJxcpOhIzajssEzQiQ1YqLh8CVk8f/EABoBAQEBAQEBAQAAAAAAAAAAAAABAgMEBQb/xAAnEQEAAQQCAQUAAwADAAAAAAAAAQIDEVETFBIEITEyUiJBcVNhof/aAAwDAQACEQMRAD8A+fgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA68Uw+XC8Qlop3MdJEqI5WKqpmiL0/ichadK8KravSHEqmCme+GNzbuRP8AoTd1kWujmKpM6JaN+s1muvUiXtv+QEUDrrMMrKGGKWpgdGyZLsV3Tki/5OQAAAAAAAAAAAAB61Fc5Eaiqq5IiAeAldnMW10Z5HJrKzX/AAS9szFmAYpIyJ7aOS0yKrMs1tn8twEYDZUQyU074ZWq2SNytci9CmsAAAAAAAAAAAAAAHfQYTPXxOkifE1Gu1eeqp/g4Dod/wAvi+K/6NLCSkNnKv1tP4l4DZur9dT+JeBDguadJidpjZur9dT+JeA2bq/XU/iXgQ4GadGJ2mNm6v11P4l4DZur9dT+JeBDgZp0YnaY2bq/XU/iXgNm6v11P4l4Hdgmi8eKYayqfVPjVyqmqjUXctjCLBcJmr/IY8SmWfXVmryPSm/P5G/GNHvtybN1frqfxLwGzdX66n8S8Dp0h0djwekjmZUPlV79Sytt0Kv+DPAdGo8WoFqX1L4111bZG33WJiM4wYnbj2bq/W0/iXgNm6v11P4l4Ei3RmifUcglTWo5Xat1pXI3v3WNOP6NR4RQNqWVL5FWRGWVtt6LwL4xoxO3Js3V+up/EvAbN1frqfxLwNNDT4VJT61ZXSwy3XmNiVyW/E64cOwSeaOGPFJ1fI5GtTkF3rkT20YnbVs3V+up/EvAbN1frqfxLwO7HNF4sLw19UyqfIrXImqrbb1KyScR8wYnaY2bq/XU/iXgNm6v11P4l4EQCZp0YnaX2bq/XU/iXgNm6v11P4l4GOE4bTV0D3zyVTVa6ycjAsifNUJBmj1C97WpUV6K5bJejcid5qIif6MTtw7N1frqfxLwGzdX66n8S8DvxrRaLDMNkq2VT5FYqJqq1E3rYrAnEfMGJ2mNm6v11P4l4DZur9dT+JeBEHhnNOjE7TGzdX66n8S8Bs3V+up/EvAhy04PopFiWGQ1bqp7FkvzUai2sqp/g1GJ+IMTtHbN1frqfxLwGzdX66n8S8CKmZycz2XvquVLmJnMaMTtL7N1frqfxLwC6OVaIq8rT5Jf7S8CHOnDvT4fzFzToxO1y0ir8RjxSvhpqJktPFMx73qxFvzWrZe5DgkxHHpH1UbsPbyssaK5eTTmtS7b/NVU79IcbqqXFK+mgo+UiimY+WRFXNLNWy9W7ecMmkeKvfVM83qk0sabr3YiXaq26c1Uw2iseqq+rZSyVlMkESNVIka1ERU3/SyfIhyZx/FKrEmUqz06wQsaqRpnZ3v7rf8AqkMEetarnI1qKqrkiJ0ncuDYi172rSSI5jdZydSZ8FOehqPJK2GoRiP5J6O1V6bFlm0hxR6VbPN7mySMRVVUXmNRLLl8nd4EC3B8Qc6NqUkt5E1mpbelkX/KHE9qserXJZzVsqFuTSbEnVDXsw1dZ8WrEma2/wB192e9pU5Vc6Vyvvrqqq6/WBgAAAAAGcD1jnjejUcrXIuqqXRc9xgbIJOSnjk1dbUcjrL02UC1PxLHn1EqphzUfPEiInJpzW5pfvdfMxixXGtelmZhrNVI1ZGnJpna117m2MnaSYo6olc3DnI6aJEYmfNTPPdnznIp5FpNiGtSysw5VjbGrWIirZypa6pl1NUKrFXy3lUvlCWm1110tbO+ZpN1ZJLLVyvnRUlc9Vei9C3zQ0hAAAAAAAAAAAAAAJSiwusxLD08jh5Xk5Xa3ORLXRtt6+4iy76Beg1XxE+hqiMzgQGy2M9i/qM4jZbGexf1GcT6SDtxQmXzbZbGexf1GcRstjPYv6jOJ9JA4oMvm2y2M9i/qM4jZbGexf1GcT6SBxQZV7A0xLC8MZSyYXK9zVVbtljtmt+s1w4fyOIeXMwWq5fXV9/KI7XXflf3llBrwMqxpHBieMUccMWGSRqyTXu6WPqVOv3mej8eJYTh600uFySO11ddssds7e/3FkA8PfJlAsZVsqUnSgxJV1tbVWtarfwtrWsaNIY8SxegbTxYXJGqSI+7pY+hF9/vLKB4/wBD5tstjPYv6jOJvotHMYpq2Cd1CqpFI16okrM7LfrPoQM8UGVex1MSxTDH0seFysc5yLd0sdsl/Eq+y2M9j/qM4n0kFm3E/Jl822Wxnsf9RnEbLYz2P+ozifSQTigyq+jlPieD0ssUuGSSK9+sitlZ1fidNRHVzzOldQYkxXf7Y65rWp+CI4nwa8PbBlUsSw2uqqJ8MNHiGu5Uty1a17d/VrEHstjPYv6jOJ9JBJtxJl822Wxnsf8AUZxGy2M9j/qM4n0kE4oMvm2y2M9j/qM4lrwZ2I4dhcNJJhUr3R3u5ssdluqr1+8ngWKIj4MvnM2jOMyTSPSiVEc5VtyrOJhstjPYv6jOJ9JBOKDL5tstjPYv6jOJtpdHMVpqhk01LqxxrrOXlGrZO8+iGmt9Cn+G76DjgygdIMc8jxOupGUsj0SZj5Xtc5E1bNW2S5LkcMmlM73VVqCVJZY0RtnP5qIioq2v1qp36QY7TUOJ11G6me97pmLK9ESyts397J+5wyaWwOdVPbRPSSSNGx5Jlkt7/Nf2PO0idIMXfibKZqQPhhiaqN1lVdb35+6xCk3pDjMeKMpo6eB0UULVTnIl1Xd0e5EIQI6KCdtLXQTvZrtjejlbe17KWabSed7azVoZklmaiJdz+aiJZen8xWcPmjp66CaZiyRsejnNTpS5Z5tLIXtq3Non8rK1GsuiWRLWW/zVy9wVntXItSkjMPm/0dSJNZ3Xe+/PLVKhK5z5XvffWcqqt+st+11KlQkjKGSzItSJNVu+/BGoVCV6ySvkdvcqqoGAACAAAGyB7Y543ubrNa5FVL2vmazZTuayoje9FcxrkVyJ0pcC2v0qnWome2gmRZIkbGms7JM89/3lQxi0qc19K9tBMsUceq1Nd3OXJFXfnki/MyfpdT8vM9lE/OJGRIqNy3rn89X5IeR6V0jH0tqKTkoY1S2q37S2RbfK/wA1CqrWTSVFXLLMipI96q5FvkvVmaTdWTuqquWd6arpHq5U6rmkIAAAAAAAAAAAAABd9AvQar4ifQpBd9AvQar4ifQ6W/skrUUfbis7JB3qXg+PG7kzHwQtO3FZ2SDvUbcVnZIO9SrA5ecrhaduKzskHeo24rOyQd6lWA85MLTtxWdkg71G3FZ2SDvUqwHnJhaduKzskHeo24rOyQd6lWA85MLTtxWdkg71G3FZ2SDvUqwHnJhaduKzskHeo24rOyQd6lWA85MLTtxWdkg71G3FZ2SDvUqwHnJhaduKzskHeo24rOyQd6lWA85Fp24rOyQd6jbis7JB3qVYDzkWnbis7JB3qNuKzskHepVgPOTC07cVnZIO9RtxWdkg71KsB5yYWnbis7JB3qNuKzskHepVgPOTC07cVnZIO9RtxWdkg71KsB5yPqWB178TwyOqkY1jnqqWbuyWx01voU/w3fQitD/5dg/M/wDuUla30Kf4bvoeiPqzKCx/FsOpMRr6WWNyzSzM5VdRq82zetOq+XvOOTSXDFkq5mU6pI6JGRcxvUqr/t+8v7Hbj9dg9NiNfBUsR1RNMxJFVirZtm53/C/eccmN4CslVMyBuvySMiRYlzvdV/dU7jytojSLFKSujpIaKPVjgaqKqtRLru6EToRCDJzSKuw6pjpIcNjRGxNXWfqq265J0/hf5kGEdGHvhjroH1KK6Fr0V6Il8rlpn0lw5yVsjIXLJIxGRpqN3Wz/ANvW5V+RVqBadK6Bau/Ia6cplfItU2N4F/xkjIGq9zEZG1Ilzumf4ZuXuCsk0kwhlS2RkDtWGHVjTk2pd1/y5ZNRPmU6Z/KTPkXe5yr3lyTGdHmVLXNharIYbM/hLznbvo1N/WU2Z6STPeiI1HOVbJ0AlgAAgAABsp1Yk8ay31EcmtbqvmazZT8n5RHyv+nrJrfhfMC4P0mwtKmV7IHWbCjIeY33r93LNGoYxaQ4RG+kZyLuRhjW/wDDb9pbIv8At97l+Zm7G8AbUSvbA1WxwoyL+EvOXNfqjUzMIsW0ejdSR8k3koo11l5J32lslv3coVVK2fyqsmn1dXlHq62WV19xoN9dM2orZpmMRjZHq5Gp0IqmgIAAAAAAAAAAAAABd9AvQar4ifQpBd9AvQar4ifQ6W/skrUfHj7CVvzRS39Ci8CC/VFOMvRYsTdzicKEC++aKTsUXgQeaKTsUXgQ83LD09Gf1ChAvvmik7FF4EHmik7FF4EHLB0Z/UKEC++aKTsUXgQeaKTsUXgQcsHQq/UKEC++aKTsUXgQeaKTsUXgQcsHQq/UKEC++aKTsUXgQeaKTsUXgQcsHRq/UKEC++aKXsUXgQeaKXsUXgQcsHQq/UKEC++aKTsUXgQeaKTsUXgQcsHQq/UKEC++aKTsUXgQeaKTsUXgQcsHQq/UKEC++aKXsUXgQeaKXsUXgQcsHQn9QoQL75opOxReBB5opOxReBBywdCr9QoQL75opOxReBB5opOxReBBywdCr9QoQL75opOxReBB5opexReBBywdCr9QoQL3JhlFFG6SSkhaxqXVVYmSHHfA/wD6nchqK8/DM+j8fmuExof/AC9B+Z/9ykpW+hT/AA3fQ0YMsHm6PyXU5K621N2831voU/w3fQ9tP1eGunxqmEHj64OzEa9tWsS1U0zGrrNVVY2zc9/Vf9jjlqdHEkq5mRwc2JGxs1FzVbruvvvZDsx+mwZcSxB1ZLH5VNMxiIslljSzUvuyyut/wOKSHRlstVKx8StjiRGM5Teq3ddMs13IeVUTpHJhnJ0kOGtjVWNXlHsRUuuSda9V/mQZOaRR4VDHSRYa5r3o1eVe12tdckzy60Vf/wBIMI6MPbA+vgbVO1YFeiPVeq5ap6nR5PLZWxwLzEbExI1zumapzut37FVoI4Zq6COok5OFz0R7r2sn49BaZotGm+WStdFZrEbGxH3uqpe6JbPNyJ8vcFbEl0ajqWqiU6shhX/Yq67t33s8m/uU2ZyPme5qI1quVUROguKUujDKliLLE5kMN3fxb67t3Vmtmqvz95Tplasz1jSzFcuqnUgJYAAIAAAbKdGOqI2yLqsVyI5V6EvmazZTta+ojZI7VY5yI53Ul94FxfUaNtqZVRlOrIoURnMXnOzd157kT5mML9HGupIncgrGRqr3K1c1WyWVdbrVV+R66DRhlRJz4lZDClv4l0e7NerNbNt87GMVJo0jqSJ80S2jV0j1l3qtkRFy61XuCqpXSRy1s0kLEZG56q1qJayXNBvrnQurZnUzdWFXqrE918jQEAAAAAAAAAAAAAAu+gXoNV8RPoUgu+gXoNV8RPodLf2SVqKNtjJ2Nv6n/gvJ8eF+mKsZdLdyqj6y+gYHi7sWjlc6FIuTVEyW97kofMIameBFSGaSNF36jlS5s841vbKj9RTx1WMz7PXT6vEYmMvpYIxmk+EIxqLUre2f8N3Ah9JMdp6qCFMOqpEe1yq7VRzMrHitTXXX4zRMOk+rp0tYPmvnGt7ZP+op9IpZGrSQqq3XUbfuOl+mbURMRlju0x8+zIGeuzr/AGGuzr/Y8vNX+JO9a3/6wI/GsRdhdGlQkfKXejdVVt0LwEmkeExSOjfUKjmqqKnJu39xDaTYzh9fhjYaWZXyJKjraiplZetDtZqrqriKqJiFr9VHj/FhtlJ2Jv6n/gs9LN5RSwzKmqsjGut1XS58wL3QaR4VDQU0UlQqPZE1rk5N2Son4Hf1MTREeFOf8c7Xqaon+fumQRu0+D9pX9N3AbT4P2lf03cDycl3/jl27VOkkCN2nwftK/pu4DafB+0r+m7gOS7/AMcnap0kiu4npK+gxCWlSla9I7c7XtfJF6iQ2nwftK/pu4FNx2phrMXqJ6d2tE9U1VsqdCIej001V1YromP9c7vqcx/FZsH0hfidclOtO2Pmq7WR19xPFA0braegxRJqp+pHqKl7KufyLbtPg/aV/TdwHqJqoqxRRMlv1MxH8kkCi47i7qjE3yUNXMkCtS2q5zUvbPIz0brqmTHKZktTM9i610c9VReap04p8PL/AK+Gp9ZEf0u5XsV0kfh2IS0qUzZEZbnK+17oi9XvLJyjOv8AYjKzF8Hpql8VU5nKttrXiVejrseWi9Vn3tzLFXq4rjFEq5WaUvq6SWnWka1JGq2+ve37FfLlimM4LPhtRFA5nKuYqNtCqZ/jYph9CxX5U58Zj/XmuV1VTmqcvo+h/wDLsH5n/wBykpW+hT/Dd9CL0P8A5dg/M/8AuUlK30Kf4bvofRp+rjKDx/C8OnxGvnqZE8olmZGxOUtq5NTdb3r3HHJhGAskqpEkRYoYks3lVzVbuRd3VZDrx/AqWrxLEKuedWyyzMjiRLZLZqded7/scUmjWFMlqn+VuWGCJFtrN+0t3It79Vk+Z5W0VpHQ4fQRUkdG7WmVqrKqP1vd1daKQZN6Q4VSYXFSMhmWSd7VWRFtluToXLO//qEIEdGHwMqa+CCR6MZI9Gq69rXUtU+EYFGlbKj28nExEanKr9pUvfd72oVWgp/K62Gn10Zyr0brL0XLTNo3hUXlknlTkigYlrub9pUunT72oFbUwPAmVLWPlRWRQ68l5VzXd1e5ylMm1OWfyf2NZdX8C4t0Vw1tQyN9W7Vjh5SbNqe7ryzRylOmRqTPSNbsRy6qr1AlgAAgAABsp40lqI41WyPcjbr0XU1myCPlp44kW2u5G3/FQLi/BsBZUytWRNWCFFd/FXnOzXq+639zGLA8F1qSF8qK50avkdyu+9kTo63fsHaMYUypkY6rdqwQo6TnN+1mvXlzWqvzMYdFsPVaSJ9U7lZGK+Rbt3ZWtn1uRPkFVWu5FK2ZKb/R111M75XNBvrWQx1kzKd2tC16oxV6UuaAgAAAAAAAAAAAAAF30C9BqviJ9CkF30C9BqviJ9Dpb+yStR8ePsJ8eNXSAAHFQAACdj0qrY42sSKnVGoiJdq8SCBJiJ+WaqYq+U/tZXeqp/C7iNrK71VP4V4kACeFOmeKjTOaRZpnyutrPcrlt7zAA06AAA7MMpY6up5OTW1dVV5qkt5kpeuXxJwK+x741uxytXrRbGzyqo9fJ4lPTauW6acVU5SYl7WwtgqpImX1WrZLmg9c5XKquVVVd6qeHnn3n2UABAAAA6KGrfQ1bKmJGq9l7I7dmlv8nOATGU/tZXeqp/CvEiK+rfXVb6mVGo99ro3dklv8HOCRTEfDNNFNPxAACtPo+h/8uwfmf/cpKVvoU/w3fQi9D/5dg/M/+5SUrfQp/hu+h6qfqzKA0g0fjrcTxCskncx7pWsials11Wp1+/8AY4pNE6SOWpvWPWKGNHKvNuqrdU6epP3O3SDBKqsxPEKttW6JqSsSGNNayu1WpfL3qm73nFJorOyWpY7En8kyJHPWzucudkt1aqHlbRWP4PDhMNLqzukllbd7VtlknV77kKTWP4RLhsVLJUVSzSzNVVa7Wu3cvT71UhQjfQ0zquthp2u1Vlejb9Vyzy6K0jFq3eWScnAxFvzc3KiqnT+VCsUMMtTWwwwLqyveiNXPJevIs82i0yLVtdiL3RRsRzrtdzltdEX5I0KzbohTJUMjfWPskWvKvNy3J19esVGZrWzPaxbtRyoi9aFuborVuqGMfiUl3xa0q2fkm63vzVxUZmJHM9iLrI1yoi9YGAACAAAGcEazTsjTe9yNT5mBnAx0k8bI1s9zkRtuu4FuXRKkbUPjdWPtFEj3/Z33/HqRTCLRKBy0rHVb+UlYr3/ZsiWy6etUQzdopUeUSMfiT8okdKtnZ9FvflrGEWi9Y9aXWxF6TSMVVtr8xqJdLLbr1QqsV0UcFbNFC9XxserWuXpS5oN9bAlNWTQNfrtjerUda17KaAgAAAAAAAAAAAAAF30C9BqviJ9CkF30C9BqviJ9Dpb+yStR8ePsJ8eNXSAAHFQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAH0fQ/+XYPzP/uUlK30Kf4bvoReh/8ALsH5n/3KSlb6FP8ADd9D1U/VmVf0hwrEKrFMQqYqtYoo5mLHHnm5Gtz92aocT9G8SbLUxOxH+GkSOlXPnWuiJ4UOzSKmxabFMQlpqlsdNDMxzGOeiazka3d81Tf1nDJhWPJLUwvro9VYkdK5ZEsqJdETr3IqnlbRmPYZVUUVLNWVXLSTNyRb3am/p97lIYmMdpcRiipZ8RqGyrK3mIj0VWpv6PxIcI30Mc0tZDHTqqTOemoqdC3yUs02jmI/8XG/EVdG1iOkujrPW10TuahWaLl/LIfJVtPrpya3ROdfLeWSbC8cXyyF9fErEYj5f4ic7LJO5qKFbE0dxZ9Q1j8SXWlivIvOWzV6Pfm5SpzM5OZ7NZHarlS6dJa0w3SF9Qxrq6NHzxc5eUbzW9S/NypkVSVqslexVRVaqpdFuigYAAIAAAZwI907GxqqPVyI23X0GBnCr0mYsa2ejk1V9/QBbHaN4m6oka/ElVXRI6Veduzbb35axjFo/iz1pXecVbLIxbJd3MalnIl/CHYVj7qiRrq+PWkiR0q8omSZtsve7uMYqDSB7qSRK2NskjFRico27WpZ3+G94VWqyDyWsmp9dH8m9W6ydNjSbqyF9PVywyOR72PVquRbovvNIQAAAAAAAAAAAAAC76Beg1XxE+hSC76Beg1XxE+h0t/ZJWo+PH2E+PGrpAADioAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD6Pof8Ay7B+Z/8AcpKVvoU/w3fQi9D/AOXYPzP/ALlJSt9Cn+G76Hqp+rMoDSBcbfiletHJq0sEzHoira7tVq29+djhfT6TrJVQPnTnRosrtfciXaie5bIqnRpPiOLQYvVx00DlpWTMdrpGqoq2atlX8bEZLi2kCpVMfBKiuYnKfwluxqJa/uvn+55W3Nj8eLLHTVGKP1ke1eTTW3Iue75/sQpKY3V4lVvhXEonxarVSNqsVqWv0X+XchFhG+hfPHWwvpb8uj05O3XfIs01PpMqVcL6hLKxHyqj+hEyRF/Bv7lZoppaeshlp0vKx6KxLXuvVYsOIV2O0za2OdzG81iyua1ebezURF/96Qro5PSmSoROWRJJ4l/3fZbv+WbrfIqUzXMmex63c1youfSWPzxpE6VXpSya0kSo20Lsm3vdO9P2K09VVyq7ffO4HgBthpaidj3wwySNjS71a1VRqe/uCNQOtcLreV5JtNK+TUR6tYxVVEXrSxsTBcQdJDGyme9Z2JJGqJkqKiLv/BQOA2QOeyeN0X+o1yK38b5GD2OY9WParXNWyoqZopnA9Y545GqiOa5FRV3JmBbHQaUPqJUdOiPmiRX8/wCy3Nvy3r3GMbdKHupZmyojnxq2NNbc1LLmnyTvOeoxbHUraqNiJM9Y011hYrka1WrZU6vtX/E3srdIl8jnjia5z4ncnGjFvqpa7l/HVtv+oVWKyKWCrminXWla9Uet73XruaTOd0jpnumvyiuVX62+/SYBAAAAAAAAAAAAAALvoF6DVfET6FILvoF6DVfET6HS39klaj48fYT48aukAAOKgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPo+h/wDLsH5n/wBykpW+hT/Dd9CL0P8A5dg/M/8AuUlK30Kf4bvoeqn6syrOkmJ1dJjlbJDDG6KnejdZzul6McmV8/8AT6CPpsWq6pkrqWgjcyFnNbyq3ZZH55rd2TnZHVpW2KTF8QpXTas000UjE1VtkxUzX5lfw+uio4qhHQOfLK3UbK2TVViLvtku88kTlqKsu3EamXSCoVtHTarIdaVdZ6Iq31UVVutk3JkhwMwqsfHFI2JupKtmu5Rtr2Vc88skXebcPxGKhqo54oZm6rEa5rJ9XlFRb55fZW32TcmL0urTNdh/MikWWSNJbNkct81TV6MulUslukquN+F1kV1kh1LSpCt3Iln9W/3b9xLyR1//AMm+bDWvWpbrvVJU5qfaS1lz3X/Ajp8SingqY3wzOfPO2bXdMi2tdLfZ/wCperoOuPSN7Uq/+HT+Mq6lnZMTU1ETdnZLdW4I6oKnFKaumqHUUb+Wga63KNVEbGiZot96W3FakeskjnrvcqqpMrjkKup0dSyOZBDJE28ya1nIqb9XoRVtkQq2utronRcDoShqFolrEjvAi2VyOTLO2ab95vpmVa4PVLHGi0zpGK96vRLK1FyRL5/aPErom4QtEyBzZHP1nypJ9u25FS25PxNsGKx00VXHDDK2OdHI2NZrsbdLXVLZqnQoE7TVU9JVq6owtYmxQNejWSI5bR3tmrk3Z3TNfcckU+K6lCvkTFYmq6NXSIiORI0bZbrldMzRVaRR1VTyktG7U5KRmq2VEW78lW+r1bsg3SKNYqWOahSRIEauclrua3Vau752W4VwSYRWq9qpS6iSyOYxmul7oqoqb75WXP3GL8Iro2TPdBlCl32e1bJZFva+aWVM0OyLHnRRajWzuVZeU1pJkdq5qvNRW2RVvmvT1CqxyOpjqU8kWOSZrY0kZIiarGoiI22ruyutrBHtPpE+nkke2mj1pGMRy3X7TEsjk6sjWstXQJQ1E1M1YmRuiZZ6Kj0W6qiqi5LZ5EdJJzYhSyx0sXkb2wwIutGk2T1VM3fZve/49QGpmF1lSxk1PTa0cr9ViNei2veyLndNy5r1GPmurVHqkSORkiRu1XtWzl3JvO3D8ajoIKdkNK5HxvV8jkltyiqipnzcrIuWdukSY1HI6rctNKjqh0ao5s1lbqWsv2c1XfcDl8zV/KSxpBd0KXejXtW2/qXNclyTqPI8IrZJWRMhu98aSt57c2qtr3v15EizSV0c6yshe5UiRjeVkR6q5NaznKrbqvOXdY002NR00tK9lNJ/Ah5JyLI1UfztZN7cs/8AAEbPRz08Ucs0asbIrmtuqXu210t0b03mqNiySNY1Luctkztmd1biDKylRj4XJNy0kyvR/NVX2umrb3J0nFErGysdIzXYiorm3tdOq/QBsq6SejlSKoZqOVEcmaKiovSiodK4LXpUpTrC3llbrIzlG3+u/wBwrq+GsqZJfJnNasaMiZyl0isuVrImVsre+9ztZpA1lTSzLDO91Mi6rnzo5zrqmSqrd2Vre/eBwtwbEFbE7yZyJLdG3cibkVc7rlkirn1Hq4JiKJdaVyc9I81T7SqiJ07rqiX3Eimk66sCeSI1Y1uqtfb/AGOYlssrI7pvuFRpOtRE9i0qxq96O1mS2VER6P3233Tf79wVEyYbVRNnc6NLU9uUVHtXVva25c96bj12GVTJeTdGiP5JZra7c2Il1VM89y5bzrqMYiqPKdemkVZoGQo5Zk1uaqLrOXV5y3ROo885075I3cjLHyVG+nTnI/WVWqiLuS32l6wiJLvoF6DVfET6FILvoF6DVfET6HS39klaj48fYT48aukAAOKgAAAAAAAAB24ZVQ0kznzwpM1W2RFRFsvXmGqYiZxM4cQADIAAAAAAAAAAAAAAAAAAPo+h/wDLsH5n/wBykpW+hT/Dd9CL0P8A5dg/M/8AuUlK30Kf4bvoeqn6syr+P4BidXpO+sgpXPgy5yKnUV7ZLHfZ0ve3iWzEaupZiFQ1lRK1qSLZEeqImZz+XVfap/1FJT6ScZy8nZppmYwreyWPezZe9vEbJY97Nl728Sy+XVnap/1FPfLqvtU/6imurVs7lOlZ2Sx72bL3t4jZLHvZsve3iWby6r7VP+op75dV29Kn/UUdSrZ3adKxsljvs2XvbxGyOPezZe9vEs/l1X2qb9RT3y6r7VP+oo6lW07tOlX2Sx32dL3t4jZHHvZsve3iWlK2q7TN+op75bV9qm/UUdSrZ3adKrslj3s2XvbxGyWPezZe9vEtfltV2mb9RR5bVdpm/UUvUq2d6nSqbJY97Nl728Rslj3s2XvbxLZ5bVdpm/UUeWVXaZvGo6lWzvU6VPZLHvZsve3iNksd9nS97eJbUraq/pM3jU98sqe0TeNR1Ktp3qdKjsjj3s2XvbxGyWO+zpe9vEt/llV2mbxqeeWVPaJvGo6lWzvU6VHZHHvZsve3iNkse9my97eJb/LKrtM3jU98sqe0TeNR1Ktnfp0p+yWPezZe9vEbJY97Nl728S4+WVPaJvGo8rqe0S+NR1Ktp36dKdslj3s2XvbxGyOPezZe9vEuXldT2iXxqPK6m3pEvjUdSrZ36dKbsljvs6XvbxGyOPezZe9vEuXldR2iXxqe+V1PaJfGo6lWzv06UzZLHfZ0ve3iNkce9my97eJc/K6ntEvjU98rqe0S+NR1Ktnfp/Kl7JY97Nl728S0aJYZWYZTVEddA6F73o5qOtmljs8rqe0S+NTfT1EjtZZHOeuX2luI9PNHvlqn1tNU4xh2Hx4+t8v/ANJ8lU5Xoxh6rdymv6vAAcHQAAAAAAAAATeWVmjlK5jXLVOzS/QSZw6W7VVz6q0CzbN0van/ALDZul7U/wDYnlDt1LulZBnMxI5nsRbo1ypcwNPKAAAAAAAAAAAAAAAA+j6H/wAuwfmf/cpK1TVfSTNal3OYqInWtiv6L1qw4HCzURbK7O/vUlXYg5zVajLXS10XcdqbtOPF26t3x84j2R2Kf8yqfiL9TmOnFP8AmdV8Rfqcx9Gn6w/P3PtIengNObJD08PQAAKjJD0xPQj09PD1Co9AAHqGSGJ7cI9AAQPTwAZIp6YhFAyB4ehHqA8PQPQeIp6ECH0hxKrw9tP5JLyeurtbmot7W6095MEVjuEV2Ktg8hp3Tcmrteyolr2tv/BThfmYtzh6fSRE3YiUFtJi3a/6beBFE3sjjvs6TxN4jZHHfZ0nibxPmTMz8vtRTTT8QhATeyOO+zpPE3iNkcd9nSeJvEjSEBN7I477Ok8TeI2Rx32dJ4m8QIQE3sjjvs6TxN4jZHHfZ0nibxAhATeyOO+zpPE3iNkcd9nSeJvECEBN7I477Ok8TeI2Rx32dJ4m8QIQE3sjjvs6TxN4jZHHfZ0nibxAhATeyOO+zpPE3iNkcd9nSeJvECEBN7I477Ok8TeI2Rx32dJ4m8QIQE3sjjvs6TxN4jZHHfZ0nibxAhATeyOO+zpPE3iNkcd9nSeJvECEBN7I477Ok8beI2Rx32dJ4m8QIQE3sjjvs5/jbxGyOO+zpPG3iBCAm9kcd9nSeJvEbI477Ok8TeIEdBiVZTxJHDO5jE3Ih3YbildLiEEclQ9zHPRFTLMz2Rx32dJ4m8TfQ6MYzS1sM89C9kcbkc52s3JO8tMRmFqrq8ZjLlqNJKyonkmfHAjnuVyojVt9TX5+qvVw9y8SLBvlrj+3GbNuffCU8/VXq4e5eJ75/qvVw9y8SKBeavacFvSV2gq/Vw+FeI2gq/Vw+FeJFAc1ezgt6S20FX6uHwrxG0FX6uHwrxIkDmubOC3pLbQ1fq4PCvEbQ1fq4PCvEiQOa5s4LekvtFV+rg8K8RtFV+rg8K8SIA5rmzr2tJfaOs9XB4V4nu0dZ6uDwrxIcDmubOva/KY2jrPVweFeI2krPVweFeJDgc1zZ17X5TO0tZ6qDwrxG0tZ6qDwrxIYDmubOva/KZ2lrPVQeFeI2lrPVQeFeJDAc1zZ17X5TO0tZ6qDwrxG0tZ6qDwrxIYDmubTr2vymdpq31UHhXie7TVvqqfwrxIUDmubOva/Ka2nrfVU/hXiNp631VP4XcSFA5rmzr2vymtp631VP4V4jaet9VT+F3EhQOa5s61r8pvaet9VT+F3EJpTXpujgT8EdxIQDmr2sentR/Sd2rxD7kPc7iNq8Q+5D3O4kECcte14bek7tZiH3IO53EbV4h9yHudxIIDlr2cNvSd2sxD7kHc7iNrMQ+5B3O4kEBy17OG3pO7WYh9yDudxG1mIfcg7ncSCA5a9nDb0ndrMQ+5B3O4jazEPuQdzuJBActezht6Tu1mIfcg7ncRtXiH3Ie53EggOWvZw29J3avEPuQ9zuI2rxD7kPc7iQQHLXs4bek7tXiH3Ie53E82rxD7kPc7iQYHLXs4bek5tXiH3Ye53EbV4h92HudxIMDlr2cNvSc2rxD7sPc7iNq8Q+7D3O4kGBy17OGjSc2qxD7sPc7iNqsQ+7D3O4kGBy17OGjSb2pxD7sPc7iNqcQ+7D3O4kIBy17OGjSb2pxD7sXc7iNqcQ+7F3O4kIBy17OGjSb2pxDqi/wC7iNqMQ6ov+7iQgHLXs4aNJrajEOqL/u4hdJ69UsqRdzuJCgctezho0AA5uoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA//9k=\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/UXurvvDY93o\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1032833d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo(\"UXurvvDY93o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Experience Replay as Regularization for traditional Q-learning</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "__What is regularization?__\n",
    "\n",
    "Regularization is adding an additional constraint to a model to encourage generalization.\n",
    "\n",
    "__How does Experience Replay regularize Q-values?__\n",
    "\n",
    "Traditional Q-learning updates on __the single most recent action__ / __every single__  (s, a) pair with a reward which could lead to over-fitting the recent specific sequences of observed data.\n",
    "\n",
    "Experience Replay regularizes Q-values by  __a random sample of prior actions batches__ the value is not always updated with every reward signal and values at t-1 can have greater impact.\n",
    "\n",
    "Experience Replay is similar to the sampling that is done for online stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Stationary vs. Nonstationary data</h2></center>\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e1/Stationarycomparison.png/390px-Stationarycomparison.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Nonstationary Data FTW (literally winning)</h2></center>\n",
    "\n",
    "<center><img src=\"https://user-images.githubusercontent.com/46422351/53296086-14701500-3811-11e9-8281-6a9f513c7764.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Actual DQN Results</h2></center>\n",
    "\n",
    "<center><img src=\"https://jaromiru.com/media/dqn/basic_average_reward.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2> Experience replay</h2></center>\n",
    " \n",
    "Data distribution changes over time. As the Q-function gets better, you'll visit different $(s , a , s′, r)$ transitions than earlier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Need to stabilize learning by mixing old and new transitions in the replay buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Otherwise \"catastrophic forgetting\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Measuring Catastrophic Forgetting in Neural Networks](https://arxiv.org/abs/1708.02072) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Experience Replay Advantages over Traditional Q-learning</h2></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) Improves data efficiency - Can sample with replacement from replay buffer. Traditional Q-learning throws away data after single use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Given sampling, experience replay can minimize correlations in the observation sequences. Experience replay learns more generalized properties of the state space, less likely to memorize specific sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) Smooths over changes in the data distribution - another variation of regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " [Source](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#deep-q-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Grab-bag of helpful techniques for DQN</h2></center>\n",
    "\n",
    "- Prioritized experience replay - Remember all your clever ways to resample simulations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Prioritize ground known truth rewards \n",
    "    - If there is ground-truth $ Q(s,a)=r$, prioritize those in the network.\n",
    "    - For example, in video games there is clear very positive or negative outcome for certain states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://jaromiru.com/2016/10/12/lets-make-a-dqn-debugging/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Q-learning can easily be extended to Deep Q-Network (DQN) \n",
    "- DQN is another type of Value Function Approximation (VFA)\n",
    "- Can learn from raw pixels to master complex video games\n",
    "- \"Experience Replay\" is training a selected sample of $(s , a , s′, r)$\n",
    "- Experience Replay can stabilize and generalize Q-learning by using more and better examples for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Bonus Material</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Self-Driving Car Example</h2></center>\n",
    "\n",
    "End-to-end learning can be difficult. In particular, there can be discontinuous parts of the functions thus preventing the derivative from being taking.\n",
    "\n",
    "Here is a concrete, real-world example from \n",
    "\n",
    "They to had to reframe the problem in order to support end-to-end learning:\n",
    "> “In order to make our system independent of the car geometry, we represent the steering command as 1/r where r is the turning radius in meters. We use 1/r instead of r to prevent a singularity when driving straight (the turning radius for driving straight is infinity). 1/r smoothly transitions through zero from left turns (negative values) to right turns (positive values).“”\n",
    "The entire paper is relevant!\n",
    "\n",
    "Note a variation of prioritize replay:\n",
    "> “Training data contains single images sampled from the video, paired with the corresponding steering command (1/r). Training with data from only the human driver is not sufficient. The network must learn how to recover from mistakes. Otherwise the car will slowly drift off the road. The training data is therefore augmented with additional images that show the car in different shifts from the center of the lane and rotations from the direction of the road.”\n",
    "Note ground-truth rewards:\n",
    "> “Since human drivers might not be driving in the center of the lane all the time, we manually calibrate the lane center associated with each frame in the video used by the simulator. We call this position the “ground truth”.”\n",
    "\n",
    "\n",
    "“End to End Learning for Self-Driving Cars”    \n",
    "https://arxiv.org/abs/1604.07316"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://jaromiru.com/media/dqn/dqn.png\" width=\"45%\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Image source](https://jaromiru.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
