{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Fill-in-the-blank-Prompts\" data-toc-modified-id=\"Fill-in-the-blank-Prompts-1\">Fill-in-the-blank Prompts</a></span></li><li><span><a href=\"#Fill-in-the-blank-Answers\" data-toc-modified-id=\"Fill-in-the-blank-Answers-2\">Fill-in-the-blank Answers</a></span></li><li><span><a href=\"#Q-learning-Steps-Prompts\" data-toc-modified-id=\"Q-learning-Steps-Prompts-3\">Q-learning Steps Prompts</a></span></li><li><span><a href=\"#Q-learning-Steps-Answers\" data-toc-modified-id=\"Q-learning-Steps-Answers-4\">Q-learning Steps Answers</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Fill-in-the-blank Prompts</h2></center>\n",
    "\n",
    "Q-learning updates a table of \\_\\_\\_\\_\\_\\_\\_\\_ which are the \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ if the agent performs action $a$ in state $s$.\n",
    "\n",
    "The process of discovering the states and their rewards during learning is called \\_\\_\\_\\_\\_\\_\\_\\_.\n",
    "\n",
    "If the agent always choose the action with the current predicted maximum reward (aka, greedy policy), the agent might not find the excepted reward from other state action pairs. Thus, the agent should \\_\\_\\_\\_\\_\\_\\_\\_. \n",
    "\n",
    "The process of updating the value table with a better reward estimate even though the action might not be associated with the strategy the agent is currently following is called \\_\\_\\_\\_\\_\\_\\_\\_.\n",
    "\n",
    "Q-learning non-mathematically:   \n",
    "The updated value is a combination of the \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ plus the weighted amount of \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ and the estimated \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ if the agent acts according to the best estimated optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Fill-in-the-blank Answers</h2></center>\n",
    "\n",
    "Q-learning updates a table of __Q-values__ which are the __expected discounted reward__ if the agent performs action $a$ in state $s$.\n",
    "\n",
    "The process of discovering the states and their rewards during learning is called __model-free__.\n",
    "\n",
    "If the agent always choose the action with the current predicted maximum reward (aka, greedy policy), the agent might not find the excepted reward from other state action pairs. Thus, the agent should __explore__.\n",
    "\n",
    "The process of updating the value table with a better reward estimate even though the action might not be associated with the strategy the agent is currently following is called __off-policy__.\n",
    "\n",
    "Q-learning in non-mathematical notion:   \n",
    "The updated value is a combination of the previous value plus the weighted amount of reward and the estimated optimal discounted future values if we act in our best estimated optimal policy.\n",
    "\n",
    "Q-learning non-mathematically:   \n",
    "The updated value is a combination of the __the previous value__ plus the weighted amount of __reward__ and the estimated __optimal discounted future values__ if the agent acts according to the best estimated optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-learning Steps Prompts</h2></center>\n",
    "\n",
    "1. Measure Reward\n",
    "1. Initialize Q\n",
    "1. Perform action\n",
    "1. Update Q\n",
    "1. Choose action from Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Somehow these steps got mixed up ü§¶‚Äç‚ôÇÔ∏è    \n",
    "Please put the steps for Q-learning in order üîÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-learning Steps Answers</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/q_learning_steps.jpg\" width=\"45%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
