{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Double-Q-learning\" data-toc-modified-id=\"Double-Q-learning-1\">Double Q-learning</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Q-learning-Limitation\" data-toc-modified-id=\"Q-learning-Limitation-3\">Q-learning Limitation</a></span></li><li><span><a href=\"#Double-Q-learning-as-a-solution\" data-toc-modified-id=\"Double-Q-learning-as-a-solution-4\">Double Q-learning as a solution</a></span></li><li><span><a href=\"#Double-Q-learning-as-a-solution\" data-toc-modified-id=\"Double-Q-learning-as-a-solution-5\">Double Q-learning as a solution</a></span></li><li><span><a href=\"#Double-Q-learning-Pseudocode\" data-toc-modified-id=\"Double-Q-learning-Pseudocode-6\">Double Q-learning Pseudocode</a></span></li><li><span><a href=\"#Double-Q-learning-Results\" data-toc-modified-id=\"Double-Q-learning-Results-7\">Double Q-learning Results</a></span></li><li><span><a href=\"#Double-Deep-Q-learning-Network(DDQN)\" data-toc-modified-id=\"Double-Deep-Q-learning-Network(DDQN)-8\">Double Deep Q-learning Network(DDQN)</a></span></li><li><span><a href=\"#Double-Q-learning-is-not-always-significantly-better.\" data-toc-modified-id=\"Double-Q-learning-is-not-always-significantly-better.-9\">Double Q-learning is not always significantly better.</a></span></li><li><span><a href=\"#Double-Q-learning-is-almost-always-more-stable\" data-toc-modified-id=\"Double-Q-learning-is-almost-always-more-stable-10\">Double Q-learning is almost always more stable</a></span></li><li><span><a href=\"#Wizard-of-Wor\" data-toc-modified-id=\"Wizard-of-Wor-11\">Wizard of Wor</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-12\">Takeaways</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-13\">Sources of Inspiration</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Double Q-learning</h2></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Explain the Maximization Bias in Q-learning.\n",
    "- Explain how Double Q-learning can reduce that bias.\n",
    "- Explain how Double Q-learning can be easily extend to Deep Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Q-learning Limitation</h2></center>\n",
    "\n",
    "<br>\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/800/1*KsQ46R8zyTQlKGv91xi6ww.png\" width=\"75%\"/></center>\n",
    "\n",
    "The agent tends to overestimate the Q function value because of  the max:\n",
    "\n",
    "$$ Q(s, a) \\xrightarrow{} r + \\gamma max_a Q(s', a) $$\n",
    "\n",
    "This is formally called Maximization Bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The same samples are being used to decide which action is the best (highest expected reward) and are also being used to estimate that action-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Double Q-learning as a solution</h2></center>\n",
    "\n",
    "Have 2 related Q functions: \n",
    "\n",
    "1. Action selection function - Determine the maximizing action.\n",
    "2. Action evaluation function - Estimate the value of that action. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Double Q-learning as a solution</h2></center>\n",
    "\n",
    "The 2 functions are:\n",
    "\n",
    "- Independently learned\n",
    "- Randomly chosen (50/50 to be updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>Double Q-learning Pseudocode</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/double_q_learning_pseudocode.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://miro.medium.com/max/1276/1*Vd1kcpLoQDnM5vrKnvzxbw.png\" width=\"75%\"/></center>\n",
    "\n",
    " Temporal Difference Target (TD-Target) update applies to a __different network__ than the network that made the predicted action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Double Q-learning Results</h2></center>\n",
    "\n",
    "<center><img src=\"images/sutton_result.png\" width=\"70%\"/></center>\n",
    "\n",
    "Double Q-learning can signicantly speed up training time by eliminating suboptimal actions more quickly than normal Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: Figure 6.7 in Sutton and Barto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Double Deep Q-learning Network(DDQN)</h2></center>\n",
    "\n",
    "<a href=\"images/ddqn_results.png\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "<center><img src=\"images/ddqn_results.png\" width=\"75%\"/></center>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: \"Deep Reinforcement Learning with Double Q-learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Double Q-learning is not always significantly better.</h2></center>\n",
    "\n",
    "<center><img src=\"images/breakout.png\" width=\"100%\"/></center>\n",
    "\n",
    "Q-learning and Double Q-learning performance is about same for several Atari games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>Double Q-learning is almost always more stable</h2></center>\n",
    "<center><img src=\"images/results.png\" width=\"75%\"/></center>\n",
    "\n",
    "The confidence bars on the value estimates for Double Q-learning are:\n",
    "\n",
    "- Smaller\n",
    "- Less likely to overestimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Wizard of Wor</h2></center>\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/e/e5/Wizard_of_wor_gameplay.png/220px-Wizard_of_wor_gameplay.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Regular Q-learning can over estimate the value of states because of using states to both update actions and their value.\n",
    "- Double Q-learning separates those into two different functions (action selection and action evaluation).\n",
    "- Double Deep Q-learning Networks (DDQN) are more effective and efficient than DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- Human-level control through deep reinforcement learning, Mnih, Kavukcuoglu, Silver et al., 2015\n",
    "- [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/pdf/1509.06461.pdf)\n",
    "- https://towardsdatascience.com/deep-double-q-learning-7fca410b193a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "---- "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
