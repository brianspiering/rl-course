{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Policy-&amp;-Bellman-Equation\" data-toc-modified-id=\"Policy-&amp;-Bellman-Equation-1\">Policy &amp; Bellman Equation</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Review-of-Markov-decision-process-(MDP)\" data-toc-modified-id=\"Review-of-Markov-decision-process-(MDP)-3\">Review of Markov decision process (MDP)</a></span></li><li><span><a href=\"#Defining-Policy\" data-toc-modified-id=\"Defining-Policy-4\">Defining Policy</a></span></li><li><span><a href=\"#Symbolic-Representation-of-Policy-in-Reinforcement-Learning-\" data-toc-modified-id=\"Symbolic-Representation-of-Policy-in-Reinforcement-Learning--5\">Symbolic Representation of Policy in Reinforcement Learning </a></span></li><li><span><a href=\"#Common-Policy-Types:-Deterministic,-Random,-&amp;-Learned\" data-toc-modified-id=\"Common-Policy-Types:-Deterministic,-Random,-&amp;-Learned-6\">Common Policy Types: Deterministic, Random, &amp; Learned</a></span></li><li><span><a href=\"#Finite-vs-Infinite-Horizons\" data-toc-modified-id=\"Finite-vs-Infinite-Horizons-7\">Finite vs Infinite Horizons</a></span></li><li><span><a href=\"#Rewards-vs-Utility\" data-toc-modified-id=\"Rewards-vs-Utility-8\">Rewards vs Utility</a></span></li><li><span><a href=\"#Each-State's-Reward\" data-toc-modified-id=\"Each-State's-Reward-9\">Each State's Reward</a></span></li><li><span><a href=\"#Student-Activity\" data-toc-modified-id=\"Student-Activity-10\">Student Activity</a></span></li><li><span><a href=\"#Each-State's-Utility\" data-toc-modified-id=\"Each-State's-Utility-11\">Each State's Utility</a></span></li><li><span><a href=\"#How-does-the-agent-actually-make-a-decision?\" data-toc-modified-id=\"How-does-the-agent-actually-make-a-decision?-12\">How does the agent actually make a decision?</a></span></li><li><span><a href=\"#π*-is-the-Optimal-Policy\" data-toc-modified-id=\"π*-is-the-Optimal-Policy-13\">π<sup>*</sup> is the Optimal Policy</a></span></li><li><span><a href=\"#π*s-as-the-optimal-policy-for-a-give-state-\" data-toc-modified-id=\"π*s-as-the-optimal-policy-for-a-give-state--14\">π<sup>*</sup><sub>s</sub> as the optimal policy for a give state </a></span></li><li><span><a href=\"#Expected-Utility-of-executing-a-policy-in-a-state\" data-toc-modified-id=\"Expected-Utility-of-executing-a-policy-in-a-state-15\">Expected Utility of executing a policy in a state</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-16\">Check for understanding</a></span></li><li><span><a href=\"#Optimal-Policy-Equation\" data-toc-modified-id=\"Optimal-Policy-Equation-17\">Optimal Policy Equation</a></span></li><li><span><a href=\"#Bellman-equation-to-calculate-the-Utility-of-a-state\" data-toc-modified-id=\"Bellman-equation-to-calculate-the-Utility-of-a-state-18\">Bellman equation to calculate the Utility of a state</a></span></li><li><span><a href=\"#What-is-the-difference-between-max-and-argmax?\" data-toc-modified-id=\"What-is-the-difference-between-max-and-argmax?-19\">What is the difference between max and argmax?</a></span></li><li><span><a href=\"#From-Utility-Values-to-Optimal-Policy\" data-toc-modified-id=\"From-Utility-Values-to-Optimal-Policy-20\">From Utility Values to Optimal Policy</a></span></li><li><span><a href=\"#Optimal-Paths-vs.-Optimal-Policy\" data-toc-modified-id=\"Optimal-Paths-vs.-Optimal-Policy-21\">Optimal Paths vs. Optimal Policy</a></span></li><li><span><a href=\"#Optimal-Policy-For-Each-State\" data-toc-modified-id=\"Optimal-Policy-For-Each-State-22\">Optimal Policy For Each State</a></span></li><li><span><a href=\"#Recall,--Brownie-Moves-Stochastically\" data-toc-modified-id=\"Recall,--Brownie-Moves-Stochastically-23\">Recall,  Brownie Moves Stochastically</a></span></li><li><span><a href=\"#Student-Activity:-How-do-policies-change-as-rewards-change?\" data-toc-modified-id=\"Student-Activity:-How-do-policies-change-as-rewards-change?-24\">Student Activity: How do policies change as rewards change?</a></span></li><li><span><a href=\"#Let's-explore-a-range-of-the-rewards\" data-toc-modified-id=\"Let's-explore-a-range-of-the-rewards-25\">Let's explore a range of the rewards</a></span></li><li><span><a href=\"#What-happens-if-each-state-has-a-positive-reward?\" data-toc-modified-id=\"What-happens-if-each-state-has-a-positive-reward?-26\">What happens if each state has a positive reward?</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-27\">Takeaways</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-28\">References</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-29\">Check for understanding</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Policy & Bellman Equation</h2></center>\n",
    "\n",
    "<center><img src=\"https://www.comprose.com/wp-content/uploads/2012/11/dilbert.gif\" width=\"95%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Define a Reinforcement Learning policy in your own words.\n",
    "- Explain how the Bellman equation calculates the utility of each state.\n",
    "- List the steps to finding an optimal policy for a model-based MDP.\n",
    "- Understand how changing rewards changes policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Review of Markov decision process (MDP)</h2></center>\n",
    "\n",
    "- MDP's has 5 Elements (S, A, P, R, γ):\n",
    "    1. S - State space\n",
    "    1. A - Action space\n",
    "    1. P - Transition probability function\n",
    "    1. R - Reward function\n",
    "    1. γ - Discount factor \n",
    "- γ - Discount factor\n",
    "    - 0 - Maximizes current rewards. Future rewards have no value.\n",
    "    - 1 - Future rewards are just as valuable as current rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Defining Policy</h2></center>\n",
    "\n",
    "<center><img src=\"images/mapping.png\" width=\"30%\"/></center>\n",
    "\n",
    "Informally, __policy__ is a way of acting.  \n",
    "\n",
    "More formally, __policy__ is a mapping between state and actions.\n",
    "\n",
    "A policy has to specify any action for all states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Symbolic Representation of Policy in Reinforcement Learning </h2></center>\n",
    "\n",
    "π is the symbol to represent policy in Reinforcement Learning.\n",
    "\n",
    "$π(s)$ or $π_s$  is the action recommended by the policy $π$ for state $s$. \n",
    "\n",
    "Given the current state, what action should the agent should try to do.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Common Policy Types: Deterministic, Random, & Learned</h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_blank.png\" width=\"55%\"/></center>\n",
    "\n",
    "__Deterministic Policy__: \"Always move Right.\" aka, [The UPS strategy](http://www.bromfordlab.com/lab-diary/2019/4/9/why-do-ups-trucks-only-turn-right). Turns out will be successfully. Brownie will bump into walls until it reaches terminal state.\n",
    "\n",
    "__Random Policy__: \"Select randomly from {Left, Up, Right, Down}\" Also could be successful. \n",
    "\n",
    "__Learned Policy__: What could be better than chance? Possibly optimal? We'll spend the rest of the term on learned policies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Finite vs Infinite Horizons</h2></center>\n",
    "\n",
    "A finite horizon means a fixed number of time steps (N).\n",
    "\n",
    "Infinite horizon means an agent can take as many steps as needed to accomplish the task.  \n",
    "The agent still are penalized for each step, but there is no fixed deadline.\n",
    "\n",
    "You can think of infinite horizon as approximately \"to the limit\".\n",
    "\n",
    "Let's assume infinite horizons for now (it makes the calculations easier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Rewards vs Utility</h2></center>\n",
    "\n",
    "R(s) is the \"short-term\" reward for being in a state.\n",
    "\n",
    "U(s) is the \"long-term\" total reward excepted from a state in the future.    \n",
    "\n",
    "$$U_{s} = R(s_t) + \\gamma R(s_{t+1}) + \\gamma^{2} R(s_{t+2}) + ... + \\gamma^{n} R(s_{t+n})$$\n",
    "\n",
    "$$ U(s) = E\\bigg[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t) \\bigg] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"Utility provides in which the likelihood of success can be weighted against the importance of the goals.\"\n",
    "\n",
    "Quote Source: Artificial Intelligence: A Modern Approach, 4th edition, p 55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Each State's Reward</h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_rewards.png\" width=\"55%\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "Calculate the utility of Brownie reaching the +1 state after 10 steps.\n",
    "\n",
    "For simplicity, assume $γ = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps_with_neg_rewards = 9\n",
    "neg_reward_value = -.04\n",
    "n_steps_with_terminal_reward = 1\n",
    "pos_reward_value = 1\n",
    "(n_steps_with_neg_rewards*neg_reward_value) + (n_steps_with_terminal_reward*pos_reward_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Each State's Utility</h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_utilities.png\" width=\"55%\"/></center>\n",
    "<center>Assuming $γ = 1$ and $R(s) = -0.04$ for nonterminal states.</center>\n",
    "\n",
    "Utilities are higher for states closer to the +1 exit, because fewer steps are required to reach the successful exit.\n",
    "\n",
    "Utility is lowest in the bottom right corner because it requires the most steps to successful exit.\n",
    "\n",
    "\n",
    "\n",
    "We are now going to learn how to calculate those utility values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>How does the agent actually make a decision?</h2></center>\n",
    "\n",
    "Given that we have the Utility of each state, we need to chain state-action together (i.e., a policy).\n",
    "\n",
    "The optimal policy follows the trajectory of with highest next state Utility?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>π<sup>*</sup> is the Optimal Policy</h2></center>\n",
    "\n",
    "An optimal policy:\n",
    "\n",
    "- Yields the highest excepted utility.\n",
    "- Always chooses the action that maximizes the expected return for any state.\n",
    "\n",
    "$π^∗$ is the overall optimal policy.\n",
    "\n",
    "To guarantee that we have found an optimal policy, we need to look at every state and have a complete model of the environment (model-based).\n",
    "\n",
    "With discounted future rewards and infinite horizons, an optimal policy is independent of the starting state. In other words - if we play the game until it ends, we can find an optimal policy from any specific state.\n",
    "\n",
    "Remember the Markov assumption (i.e., history doesn't matter), once Brownie is in a state, it does not matter how it got there.\n",
    "\n",
    "To maximize cumulative rewards, we only need to know current state and optimal State-Action pairs (i.e., policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>π<sup>*</sup><sub>s</sub> as the optimal policy for a give state </h2></center>\n",
    "\n",
    "$π^*_s$ is an optimal policy that recommends an action for a specific state.\n",
    "\n",
    "This is what the agent needs in order to act optimally.\n",
    "\n",
    "The agent observes where it is then $π^∗_s$ is which is the optimal policy from state $s$.   \n",
    "Keeps observing $s$ and attempting  $π^∗_s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Expected Utility of executing a policy in a state</h2></center>\n",
    "\n",
    "$$ U^π(s)  = E \\left[ \\sum_{t=0}^{\\infty}γR(S_t, π(S_t), S_{t+1} )\\right]  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "Given these Utilities, what is the optimal policy?\n",
    "\n",
    "In other words, what is the best action for each state?\n",
    "\n",
    "<center><img src=\"images/wharehouse_utilities.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/wharehouse_policy.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Optimal Policy Equation</h2></center>\n",
    "\n",
    "$$π^*(s) = argmax \\sum_{s′}P(s′ | s,a)[R(s, a, s′ ) +  γU(s′)]$$\n",
    "\n",
    "The optimal policy for a given state is the policy that is mostly likely to be successful, weighted by utility.\n",
    "\n",
    "Choose the action that maximizes the reward for the next step plus the discounted utility of the subsequent state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bellman equation to calculate the Utility of a state</h2></center>\n",
    "\n",
    "$$U(s) = max \\sum_{s′} P(s′ | s,a)[R(s, a, s′ ) +  γU(s′)]$$\n",
    "\n",
    "The utility of a state is the immediate reward for that state plus the expected discounted utility of the next state, assuming that the agent chooses the optimal action. \n",
    "\n",
    "The solution to the Bellman equation is the $U(s′)$ for the optimal policy. \n",
    "\n",
    "The Bellman equation is the most important equation in Reinforcement Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[An indepth discussion here](https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>What is the difference between max and argmax?</h2></center>\n",
    "\n",
    "$max(𝑥,𝑦)$ is the maximum of 𝑥 and 𝑦\n",
    "\n",
    "whereas $argmax_{x, y}𝑓(𝑥,𝑦)$ are the values of 𝑥,𝑦 that maximize the function 𝑓.\n",
    "\n",
    "In this case, max will give the agent the value and argmax will give the agent the action.\n",
    "\n",
    "[Source](https://math.stackexchange.com/questions/673842/maxx-y-and-argmaxx-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>From Utility Values to Optimal Policy</h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_utilities.png\" width=\"55%\"/></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_policy.png\" width=\"57%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Optimal Paths vs. Optimal Policy</h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_rewards.png\" width=\"55%\"/></center>\n",
    "\n",
    "Brownie's optimal paths could be:\n",
    "\n",
    "1. Up, Up, Right, Right, Right\n",
    "1. Right, Right, Up, Up, Right\n",
    "\n",
    "For Brownie, what is $π^*$ for S<sub>0</sub>? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Optimal Policy For Each State</h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_policy.png\" width=\"57%\"/></center>\n",
    "\n",
    "Given starting in (0,0), we'll recommend Brownie to follow the optimal policy of taking optimal path #1: Up, Up, Right, Right, Right\n",
    "\n",
    "There is a huge penalty for ending up in the -1 terminal state by accident so go the long way!\n",
    "\n",
    "The Bellman equation allows an agent to account for stochasticity (random stuff happens) and future discounting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Recall,  Brownie Moves Stochastically</h2></center>\n",
    "\n",
    "<center><img src=\"images/oops.jpg\" width=\"30%\"/></center>\n",
    "\n",
    "What is the probability that Brownie will perfectly follow the optimal policy (Up, Up, Right, Right, Right) from S<sub>0</sub> and reach (3,2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.77%\n"
     ]
    }
   ],
   "source": [
    "n_moves = 5\n",
    "probability_of_optimal_execution = .8**n_moves\n",
    "print(f\"{probability_of_optimal_execution:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " We'll need to monitor Brownie during the entire path to make sure!\n",
    " \n",
    " The optimal policy includes every state (that is why we don't just define a path and not monitor our agent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"https://www.azquotes.com/picture-quotes/quote-show-me-the-incentive-and-i-will-show-you-the-outcome-charlie-munger-138-85-72.jpg\" width=\"80%\"/></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity: How do policies change as rewards change?</h2></center>\n",
    "\n",
    "<center><img src=\"images/optimal_policies_blank.png\" width=\"75%\"/></center>\n",
    "\n",
    "What the different policies for different reward (R) levels?   \n",
    "For each square (state), what is the optimal action (i.e., policy)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/optimal_policies_answers.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If reward is just a little negative R(s) = -0.01 & -0.03, okay to relative stay okay to take long way. Just make sure to avoid -1 terminal state.\n",
    "- If reward is more negative R(s) = -0.4, plan to leave quickly.\n",
    "- If reward is more negative R(s) = -2.0, plan leave immediately (even if there is huge negative reward for leaving!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Let's explore a range of the rewards</h2></center>\n",
    "\n",
    "What is the optimal policy, if:\n",
    "- $R$ is very negative.\n",
    "- $R$ is gradually less negative.\n",
    "- $R$ is positive.\n",
    "- What range of values would our current optimal policy remain optimal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/r_very_neg.png\" width=\"50%\"/></center>\n",
    "\n",
    "When R(s) ≤ −1.6284, work is so painful that the Brownie heads straight for the nearest finish.\n",
    "\n",
    "Even if the finish is worth a large negative reward (–1)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/r_kinda_neg.png\" width=\"55%\"/></center>\n",
    "\n",
    "When −0.4278 ≤ R(s) ≤ −0.0850, work is quite unpleasant.\n",
    "\n",
    "Brownie always takes the shortest route to the +1 state.\n",
    "\n",
    "In particular, the agent will the take the dangerous shortcut and is willing to risk falling into the –1 state by accident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/r_little_negative.png\" width=\"55%\"/></center>\n",
    "\n",
    "When work is only slightly dreary (−0.0221 < R(s) < 0), the optimal policy is taking no risks at all. \n",
    "\n",
    "The agent will always heads directly away from the –1 state so that it cannot fall in by accident, even though this means banging its head against the column or wall quite a few times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What happens if each state has a positive reward?</h2></center>\n",
    "\n",
    "<center><img src=\"images/r_positive.png\" width=\"55%\"/></center>\n",
    "\n",
    "If R(s) > 0, Brownie finds just cruising the warehouse is positively enjoyable. \n",
    "\n",
    "The agent actively avoids both exits!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "<center><img src=\"images/policy.jpg\" width=\"75%\"/></center>\n",
    "\n",
    "- $π$ is a policy, a mapping from state to action.\n",
    "- $π^*$ is the optimal policy. For each state there is an action that will maximize discounted cumulative rewards.\n",
    "- Steps to find the optimal policy for a model-based MDP:\n",
    "    1. Define all 5 Elements of a MDP: (S, A, P, R, γ)\n",
    "    2. Calculate the Utility for each state.\n",
    "    3. Define an optimal policy, aka the best action for each state.\n",
    "- During an episode:\n",
    "    1. Agent starts starting state.\n",
    "    2. Repeat until terminal state:\n",
    "        1. Given the current state, agent initiates an action based on the optimal policy.\n",
    "        2. Agent observes the environment to find new current state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "------\n",
    "\n",
    "- Chapter 17 of [Artificial Intelligence: A Modern Approach, 4th edition](http://aima.cs.berkeley.edu/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "For a given state, what is $\\sum_{a} π(s, a)$ equal?\n",
    "\n",
    "What is that equation in your own words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\sum_{a} π(s, a)$ = 1\n",
    "\n",
    "In a given state, there is a set of actions. Policies must select among them."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
