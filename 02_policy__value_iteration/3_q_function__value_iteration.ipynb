{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Q-Value-Function-&amp;-Value-Iteration\" data-toc-modified-id=\"Q-Value-Function-&amp;-Value-Iteration-1\">Q-Value Function &amp; Value Iteration</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Q-Value-Function\" data-toc-modified-id=\"Q-Value-Function-3\">Q-Value Function</a></span></li><li><span><a href=\"#Value-Iteration-well-works-for-Model-Based-MDPs\" data-toc-modified-id=\"Value-Iteration-well-works-for-Model-Based-MDPs-4\">Value Iteration well works for Model-Based MDPs</a></span></li><li><span><a href=\"#Solving-the-Bellman-Equation\" data-toc-modified-id=\"Solving-the-Bellman-Equation-5\">Solving the Bellman Equation</a></span></li><li><span><a href=\"#Putting-the-Iterative-in-Value-Iteration\" data-toc-modified-id=\"Putting-the-Iterative-in-Value-Iteration-6\">Putting the Iterative in Value Iteration</a></span></li><li><span><a href=\"#Fundamental-Value-Iteration-Idea\" data-toc-modified-id=\"Fundamental-Value-Iteration-Idea-7\">Fundamental Value Iteration Idea</a></span></li><li><span><a href=\"#Bellman-Update\" data-toc-modified-id=\"Bellman-Update-8\">Bellman Update</a></span></li><li><span><a href=\"#Value-Iteration-Algorithm\" data-toc-modified-id=\"Value-Iteration-Algorithm-9\">Value Iteration Algorithm</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-10\">Takeaways</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-11\">References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Q-Value Function & Value Iteration</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Define the q-value function in words and mathmatically.\n",
    "- Explain the Value Iteration algorithm in your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Q-Value Function</h2></center>\n",
    "\n",
    "I like to think of the Q is standing for \"Quality\". \n",
    "\n",
    "The Utility of the state is the \"Quality\" of the best action taken from that state\n",
    "\n",
    "Q-Value Function is also called:\n",
    "\n",
    "- Q-function\n",
    "- Action-utility function\n",
    "\n",
    "$Q(s, a)$ is the excepted utility of taking a given action in a state.\n",
    "\n",
    "$$U(s) = max_a Q(s, a)$$  \n",
    "\n",
    "The optimal policy is thus,\n",
    "\n",
    "$$π^*(s) = argmax_a Q(s, a)$$\n",
    "\n",
    "Use the Bellman equation to define the Q-function:\n",
    "\n",
    "$$Q(s, a) = \\sum_{s′}P(s′ | s,a)[R(s, a, s′ ) +  γU(s′)]$$\n",
    "$$Q(s, a) = \\sum_{s′}P(s′ | s,a)[R(s, a, s′ ) +  γmax_{a′} Q(s′, a′)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions?\n",
    "\n",
    "You'll be coding the Q-function for the next lab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Value Iteration well works for Model-Based MDPs</h2></center>\n",
    "\n",
    "<center><img src=\"images/model_based__model_free.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Solving the Bellman Equation</h2></center>\n",
    "\n",
    "If there are $n$ possible states, then there are $n$ Bellman equations (one for each state).\n",
    "\n",
    "The $n$ equations contain $n$ unknowns (the utilities of the states). \n",
    "\n",
    "We want to solve these simultaneous equations to calculate the utilities. \n",
    "\n",
    "\n",
    "How do you solve n simultaneous equations n unknowns?\n",
    "\n",
    "Linear algebra!\n",
    "\n",
    "But wait, there is a problem - the equations are __nonlinear__, because the $max$ operator is __not__ a linear operator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Putting the Iterative in Value Iteration</h2></center>\n",
    "\n",
    "1. Start with arbitrary initial values for the utilities.\n",
    "1. Calculate the right-hand side of the equation.\n",
    "1. Plug it into the left-hand side - thereby updating the utility of each state from the utilities of its neighbors.\n",
    "1. Repeat step 2 & 3 until we reach an equilibrium (no change in utilities or only minor updates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Fundamental Value Iteration Idea</h2></center>\n",
    "\n",
    "To find the optimal policy, we need the best actions.\n",
    "\n",
    "Best actions are defined as the maximum estimated utility of *each and every state* (need a complete model). \n",
    "\n",
    "We share with zero Utility value. We are given the complete MDP which includes the rewards.\n",
    "\n",
    "Then step through every state, find the best action. Using that to update our estimate of the Utility for the current state.\n",
    "\n",
    "Keep going until there no update or only minor updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Bellman Update</h2></center>\n",
    "\n",
    "$$U_{t+1}(s) \\leftarrow  max_{a∈A(s)} \\sum_{s′} P(s′ | s,a)[R(s, a, s′ ) +  γU(s′)]$$\n",
    "\n",
    "We propagate information through the state space through local updates.\n",
    "\n",
    "Over, over, over, and over (iterative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Value Iteration Algorithm</h2></center>\n",
    "\n",
    "Givens:\n",
    "\n",
    "- MDP - (S, A, P, R, γ)\n",
    "- U - Utilities for each state, initially set to 0\n",
    "- ε - the maximum error allowed\n",
    "\n",
    "\n",
    "__repeat__:   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; U ← U′ Keep updating the Utilities   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; δ ← 0 The delta changes should be getting smaller and smaller   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; __for each state__:   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; $U′(s) \\leftarrow  max_{a∈A(s)} Q(mdp, s, a, U)$   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; if $|U′(s) - U(s)  | > δ $:   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;δ  ← |U′(s) - U(s)|    \n",
    "__until__ δ ≤  ε(1- γ) / γ   \n",
    "__return__ U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions?\n",
    "\n",
    "You'll be coding the Value Iteration for the next lab!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- The Bellman equation can calculate the expected utility, thus picking optimal policy.\n",
    "- Value iteration solves the Bellman equation by repetitively solving the equation until equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "------\n",
    "\n",
    "- Chapter 17 of [Artificial Intelligence: A Modern Approach, 4th edition](http://aima.cs.berkeley.edu/)\n",
    "- [Video of Overall MDP, policy, and VI](https://www.youtube.com/watch?v=KovN7WKI9Y0)\n",
    "- [Video walk through of VI for gridworld](https://www.youtube.com/watch?v=4KGC_3GWuPY)\n",
    "- [Detailed video Bellman equation](https://www.youtube.com/watch?v=lfHX2hHRMVQ&list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&index=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
