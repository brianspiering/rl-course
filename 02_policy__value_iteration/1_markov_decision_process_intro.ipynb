{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Markov-Decision-Process-(MDP)\" data-toc-modified-id=\"Markov-Decision-Process-(MDP)-1\">Markov Decision Process (MDP)</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Brownie---The-Warehouse-Robot\" data-toc-modified-id=\"Brownie---The-Warehouse-Robot-3\">Brownie - The Warehouse Robot</a></span></li><li><span><a href=\"#Markov-Decision-Process-(MDP)\" data-toc-modified-id=\"Markov-Decision-Process-(MDP)-4\">Markov Decision Process (MDP)</a></span></li><li><span><a href=\"#Markov-Decision-Process-(MDP)-as-a-model\" data-toc-modified-id=\"Markov-Decision-Process-(MDP)-as-a-model-5\">Markov Decision Process (MDP) as a model</a></span></li><li><span><a href=\"#Brownie's-Warehouse\" data-toc-modified-id=\"Brownie's-Warehouse-6\">Brownie's Warehouse</a></span></li><li><span><a href=\"#Markov-Decision-Process-(MDP)\" data-toc-modified-id=\"Markov-Decision-Process-(MDP)-7\">Markov Decision Process (MDP)</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-8\">Check for understanding</a></span></li><li><span><a href=\"#As-Brownie's-Managers,-we-are-incentivizing-Brownie-to-be-efficient\" data-toc-modified-id=\"As-Brownie's-Managers,-we-are-incentivizing-Brownie-to-be-efficient-9\">As Brownie's Managers, we are incentivizing Brownie to be efficient</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-10\">Check for understanding</a></span></li><li><span><a href=\"#Trajectory-is-represented-as-τ-\" data-toc-modified-id=\"Trajectory-is-represented-as-τ--11\">Trajectory is represented as τ </a></span></li><li><span><a href=\"#If-the-MDP-is-deterministic-we-are-done!\" data-toc-modified-id=\"If-the-MDP-is-deterministic-we-are-done!-12\">If the MDP is deterministic we are done!</a></span></li><li><span><a href=\"#Reinforcement-Learning-deals-with-stochastic-MDPs\" data-toc-modified-id=\"Reinforcement-Learning-deals-with-stochastic-MDPs-13\">Reinforcement Learning deals with stochastic MDPs</a></span></li><li><span><a href=\"#Student-Activity\" data-toc-modified-id=\"Student-Activity-14\">Student Activity</a></span></li><li><span><a href=\"#MDP's-5-Elements\" data-toc-modified-id=\"MDP's-5-Elements-15\">MDP's 5 Elements</a></span></li><li><span><a href=\"#S---the-state-space\" data-toc-modified-id=\"S---the-state-space-16\">S - the state space</a></span></li><li><span><a href=\"#A---the-action-space\" data-toc-modified-id=\"A---the-action-space-17\">A - the action space</a></span></li><li><span><a href=\"#P---The-Transition-Probability-Function-\" data-toc-modified-id=\"P---The-Transition-Probability-Function--18\">P - The Transition Probability Function </a></span></li><li><span><a href=\"#R---the-reward-function\" data-toc-modified-id=\"R---the-reward-function-19\">R - the reward function</a></span></li><li><span><a href=\"#γ---The-discount-factor\" data-toc-modified-id=\"γ---The-discount-factor-20\">γ - The discount factor</a></span></li><li><span><a href=\"#&quot;A-Bird-in-the-Hand-is-Worth-Two-in-the-Bush&quot;\" data-toc-modified-id=\"&quot;A-Bird-in-the-Hand-is-Worth-Two-in-the-Bush&quot;-21\">\"A Bird in the Hand is Worth Two in the Bush\"</a></span></li><li><span><a href=\"#RL-goal:-The-agent-maximizes-its-expected-discounted-return\" data-toc-modified-id=\"RL-goal:-The-agent-maximizes-its-expected-discounted-return-22\">RL goal: The agent maximizes its expected discounted return</a></span></li><li><span><a href=\"#What-happens-if-there-are-broken-discounted-rewards?\" data-toc-modified-id=\"What-happens-if-there-are-broken-discounted-rewards?-23\">What happens if there are broken discounted rewards?</a></span></li><li><span><a href=\"#Student-Activity\" data-toc-modified-id=\"Student-Activity-24\">Student Activity</a></span></li><li><span><a href=\"#Putting-the-Markov-in-Markov-decision-process-(MDP)\" data-toc-modified-id=\"Putting-the-Markov-in-Markov-decision-process-(MDP)-25\">Putting the Markov in Markov decision process (MDP)</a></span></li><li><span><a href=\"#Markov-Assumption\" data-toc-modified-id=\"Markov-Assumption-26\">Markov Assumption</a></span></li><li><span><a href=\"#Markov-decision-process-(MDP)-vs-Markov-Chain\" data-toc-modified-id=\"Markov-decision-process-(MDP)-vs-Markov-Chain-27\">Markov decision process (MDP) vs Markov Chain</a></span></li><li><span><a href=\"#Markov-Assumption-helps-overcome-the-Sunk-Cost-fallacy\" data-toc-modified-id=\"Markov-Assumption-helps-overcome-the-Sunk-Cost-fallacy-28\">Markov Assumption helps overcome the Sunk Cost fallacy</a></span></li><li><span><a href=\"#Model-based-vs-model-free\" data-toc-modified-id=\"Model-based-vs-model-free-29\">Model-based vs model-free</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-30\">Check for understanding</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-31\">Check for understanding</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-32\">Takeaways</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-33\">Bonus Material</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-34\">Sources of Inspiration</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Markov Decision Process (MDP)</h2></center>\n",
    "\n",
    "<center><img src=\"images/mdp_overview.png\" width=\"95%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Image Source](https://images.deepai.org/django-summernote/2019-03-19/c8c9f96b-cc21-4d33-8b37-cb810f599e6e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Frame Reinforcement Learning problems as a Markov decision process (MDP).\n",
    "- Define the 5 elements of MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Brownie - The Warehouse Robot</h2></center>\n",
    "<center><img src=\"images/robot-control-warehouse.jpg\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Markov Decision Process (MDP)</h2></center>\n",
    "\n",
    "A mathematical framework for modeling decision-making in under uncertainty.\n",
    "\n",
    "Well, partially uncertainty - where outcomes are partly random and partly under the control of an agent.\n",
    "\n",
    "MDP are a \"goldilocks\" model.\n",
    "\n",
    "MDPs are __not__ appropriate if:\n",
    "\n",
    "- State-Action pairs are completely deterministic.\n",
    "- Rewards are completely stochastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>Markov Decision Process (MDP) as a model</h2></center>\n",
    "\n",
    "The goal of this (like all computational fields) to create a simplified model of the world that computational efficient.\n",
    "\n",
    "The real-world is complex\n",
    "\n",
    "Reinforcement Learning models in a simpler, very specific form.\n",
    "\n",
    "Markov {Chain, Reward, Decision} Process is a simpler, very specific form.\n",
    "\n",
    "Then we can apply the set of RL algorithms.\n",
    "\n",
    "This is the same process as modeling the real-world with Probability Theory:\n",
    "\n",
    "1. Observe the real-world.\n",
    "2. Map real-world entities and actions onto mathematical constructs and symbols.\n",
    "3. Use existing mathematical formulas to solve for unknowns.\n",
    "4. Use those estimated values for unknowns to make decisions about the real-world.\n",
    "\n",
    "Think about solving biased coin-flip problem with Bayes Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Brownie's Warehouse</h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_blank.png\" width=\"60%\"/></center>\n",
    "\n",
    "Model-based learning - We are going to compute a model of the world (i.e., the transitions and rewards).\n",
    "\n",
    "- Brownie always starts in (0,0) \n",
    "- Brownie makes a finite sequence of discrete steps.\n",
    "- Possible actions are: Up, Down, Left, and Right (aka Pac-man actions).\n",
    "- The end (aka, terminal) states are (3, 2) with +1 and (3, 1) with -1.\n",
    "- Assume the environment is fully observable.\n",
    "- Brownie wants to do a good job and maximize cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Markov Decision Process (MDP)</h2></center>\n",
    "\n",
    "<center><img src=\"https://images.deepai.org/django-summernote/2019-03-19/c8c9f96b-cc21-4d33-8b37-cb810f599e6e.png\" width=\"75%\"/></center>\n",
    "\n",
    "At each time step, the agent is in some state $s$ in the environment.\n",
    "\n",
    "The agent may choose any action $a$ that is available in state $s$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "<center><img src=\"images/wharehouse_blank.png\" width=\"55%\"/></center>\n",
    "\n",
    "<center>What are the states? How many states are there?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The states are grid-world cells within the warehouse.\n",
    "\n",
    "There are 11 valid states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(4*3)-1 # 1 state is unreachable because of a support column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>As Brownie's Managers, we are incentivizing Brownie to be efficient</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/wharehouse_with_neg_rewards.png\" width=\"55%\"/></center>\n",
    "\n",
    "<center>A small negative reward for each move.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What are the optimal trajectory (i.e., sequential actions) to maximize cumulative rewards?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Brownie's optimal trajectory could be:\n",
    "\n",
    "1. Up, Up, Right, Right, Right\n",
    "1. Right, Right, Up, Up, Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Trajectory is represented as τ </h2></center>\n",
    "\n",
    "A  τ is a sequence of states and actions in an environment:\n",
    "\n",
    "$$τ = s₀ → a₀ →  s₁ →  a₁ → …)$$\n",
    "\n",
    "$$τ = ( s₀, a₀ , s₁ , a₁ , …)$$\n",
    "\n",
    "A trajectory is sometimes also called an episode or rollout.\n",
    "\n",
    "A trajectory be deterministic or stochastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>If the MDP is deterministic we are done!</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Reinforcement Learning deals with stochastic MDPs</h2></center>\n",
    "\n",
    "\n",
    "<center><img src=\"images/tranisition_model.png\" width=\"55%\"/></center>\n",
    "\n",
    "<center>Our Agent is Stochastic!</center>\n",
    "\n",
    "However, Brownie is in training and moves non-deterministically.\n",
    "\n",
    "Each action achieves intended outcome with probability 0.8. \n",
    "\n",
    "But there is probability 0.2 that Brownie moves at right angles to the intended direction (.01 to the left and .01 to the right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/wharehouse_with_neg_rewards.png\" width=\"55%\"/></center>\n",
    "\n",
    "If in (0,0) and the intended action is Up, the result could be:\n",
    "\n",
    "- With probability 0.8, actually moves Up to (0,1) \n",
    "- With probability 0.1, actually moves Right to (1,0) \n",
    "- With probability 0.1, actually moves Left, bumps into the wall, and stays in (0,0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "What is the probability that Brownie perfectly carries out an intended best trajectory of [Up, Up, Right, Right, Right]?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32768\n"
     ]
    }
   ],
   "source": [
    "randomness_level = .8\n",
    "n_steps = 5\n",
    "\n",
    "print(f\"{randomness_level**n_steps:.7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>MDP's 5 Elements</h2></center>\n",
    "\n",
    "<center>(S, A, P, R, γ)</center>\n",
    "\n",
    "1. $S$ - State space\n",
    "2. $A$ - Action space\n",
    "3. $P$ - Transition probability function\n",
    "4. $R$ - Reward function\n",
    "5. $γ$ - The discount factor for future rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>S - the state space</h2></center>\n",
    "\n",
    "S - the state space - the set of all unique situations the agent could be in. \n",
    "\n",
    "We are going to assume finite, discrete states.\n",
    "\n",
    "S<sub>0</sub> is the initial state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>A - the action space</h2></center>\n",
    "\n",
    "A - the action space, the set of all actions available to the agent.\n",
    "\n",
    "We are going to assume finite, discrete actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>P - The Transition Probability Function </h2></center>\n",
    "\n",
    "P - the transition probability function or \"model\"\n",
    "\n",
    "$$P(s′ |s, a)$$\n",
    "\n",
    "Describes the outcome of each action in each state.\n",
    "\n",
    "The probability of transition to another state s′ when given action a is taken in current s. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/tree_of_state.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$P(s′ |s, a)$ is a table containing probabilities for outcome for each state-action pair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>R - the reward function</h2></center>\n",
    "\n",
    "R - the reward function\n",
    "\n",
    "$$R(s, a, s′)$$\n",
    "\n",
    "For every transition from s to s′ via action a, the agent receives a reward.\n",
    "\n",
    "The reward can negative, zero, or positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"As a general rule, it is better to design performance measures according to what one actually wants to be achieved in the environment, rather than according to how one thinks the agent should behave.\"\n",
    "\n",
    "> — Artificial Intelligence: A Modern Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>γ - The discount factor</h2></center>\n",
    "\n",
    "\n",
    "γ (gamma) - the discount factor. \n",
    "\n",
    "- Quantifies the difference between actual immediate rewards and possible future rewards.\n",
    "- Thus, how much influence the subsequent rewards have on the value of a particular state.\n",
    "- A real-valued number between 0 and 1. \n",
    "- When γ is close to 0, rewards in the distant future are viewed as insignificant. \n",
    "- When γ is 1, future rewards are exactly equivalent to current rewards.\n",
    "- Discounting is a good model of human preferences over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>\"A Bird in the Hand is Worth Two in the Bush\"</h2></center>\n",
    "\n",
    "<center><img src=\"https://www.tips4uk.com/wp-content/uploads/2018/12/A-BIRD-IN-THE-HAND-IS-WORTH-TWO-IN-THE-BUSH.jpg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>RL goal: The agent maximizes its expected discounted return</h2></center>\n",
    "\n",
    "$$R_t = \\sum_{k=0}^∞ γ^k r_{t+k+1}$$\n",
    "\n",
    "t is time.  \n",
    "k is steps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/cumulative expected (discounted) rewards.png\" width=\"75%\"/></center>\n",
    "\n",
    "Note: γ is exponential. Future rewards are heavily penalized!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>What happens if there are broken discounted rewards?</h2></center>\n",
    "\n",
    "Recently, oil prices went into negative territory. That means traders were actually paying people to take oil contracts off of their hands. \n",
    "\n",
    "https://www.npr.org/2020/04/21/839522390/u-s-oil-prices-fall-below-zero-for-the-first-time-in-history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "In small groups, define the 5 Elements (S, A, P, R, γ) of a MDP for a vacuum cleaning robot.\n",
    "\n",
    "\n",
    "1. State\n",
    "2. Action\n",
    "3. Probabilities of Transition \n",
    "4. Reward\n",
    "5. Discounting rate of future rewards\n",
    "\n",
    "Make a quick, complete pass through all 5. Then develop each one deeper. Try to be as complete as possible.\n",
    "\n",
    "The fastest typer should take notes and share their screen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. State - location in home\n",
    "    - Discrete or continuous\n",
    "2. Action\n",
    "    - Move to new location\n",
    "    - Clean\n",
    "    - Stop\n",
    "    - Dock to charge\n",
    "3. Probabilities of Transition \n",
    "    - Depends on - How dirty? How much battery? How far to move to clean new dirt?\n",
    "4. Reward\n",
    "    - Time to 100% clean\n",
    "    - Minimal hitting things\n",
    "    - Stay charged\n",
    "5. Discounting rate of future rewards\n",
    "    - Clean current area completely before moving (otherwise it would the floor would just kinda clean all over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For full treatment of vacuum cleaning robot see Artificial Intelligence: A Modern Approach, 4th edition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Putting the Markov in Markov decision process (MDP)</h2></center>\n",
    "\n",
    "What does it mean to be Markov in a Markov decision process (MDP)?\n",
    "\n",
    "Once Brownie is in a state, it does not matter how it got there.\n",
    "\n",
    "To maximize cumulative rewards, we only need to know current state and optimal State-Action pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Markov Assumption</h2></center>\n",
    "\n",
    "<center><img src=\"images/markov_quote.png \" width=\"75%\"/></center>\n",
    "\n",
    "<center>Tomorrow only depends on today. Yesterday has no influence.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Markov decision process (MDP) vs Markov Chain</h2></center>\n",
    "\n",
    "https://setosa.io/blog/2014/07/26/markov-chains/index.html\n",
    "\n",
    "A Markov Chain has no agent or decisions. It purely stochastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Markov Assumption helps overcome the Sunk Cost fallacy</h2></center>\n",
    "\n",
    "[Sunk cost](https://en.wikipedia.org/wiki/Sunk_cost) a cost that has already been incurred and cannot be recovered.\n",
    "\n",
    "For example, repairing a bike or car.\n",
    "\n",
    "The decision to keep fixing an old bike or car is about present and future value (Markov), not how much you have already put into the car (sunk cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Model-based vs model-free</h2></center>\n",
    "\n",
    "<center><img src=\"images/model_based__model_free.png\" width=\"75%\"/></center>\n",
    "\n",
    "Model-based means the agent knows (a priori) or learns direct knowledge of the probability transition function $p(s`|s, a). The probability transition function is the environment dynamics, aka \"the rules of the game.\" Agents try to understands the world and create a model to represent it. \n",
    "\n",
    "Model-free means does __not__ know or learns  direct knowledge of the probability transition function. Instead, the agents directly learns value or policy. Just do, no extra knowledge.\n",
    "\n",
    "\n",
    "</h2></center>\n",
    "\n",
    "In small groups, fill in this table with examples:\n",
    "\n",
    "\n",
    "|         |  |\n",
    "|:-------:|:------:|\n",
    "| Model-based | ? | \n",
    "| Model-free  | ? | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Check for understanding</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know the Transition Function before an action (a priori), which type of algorithm can you apply?\n",
    "\n",
    "1. Model-based\n",
    "2. Model-free\n",
    "\n",
    "Why?\n",
    "\n",
    "How can you change a model-free representation to a model-based representation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know the Transition Function before an action, you can apply model-based.\n",
    "\n",
    "You can change a model-free representation to a model-based representation by completely exploring the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What the relationship between multi-arm bandits (MAB) and MDPs?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "MAB only has a single state, the agent starts and ends in the same state.\n",
    "\n",
    "You think of a physical location. The agent is in a always in the same location.\n",
    "\n",
    "\n",
    "Each arm is an action with a reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Maximizing cumulative rewards is the goal of RL / MDP.  \n",
    "Minimizing regret is the goal of MAB.\n",
    "\n",
    "They are the very similar:\n",
    "Maximize cumulative reward = minimize total regret "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Markov Decision Process (MDP) is the most common Reinforcement Learning framework.\n",
    "- MDP's has 5 Elements (S,A,P,R,γ):\n",
    "    1. S - State space\n",
    "    1. A - Action space\n",
    "    1. P - Transition probability function\n",
    "    1. R - Reward function\n",
    "    1. γ - Discount factor \n",
    "- γ - Discount factor\n",
    "    - 0 - Maximizes current observed rewards\n",
    "    - 1 - Future possible rewards are as valuable as current rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>Bonus Material</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Types of Markov Decision Processes (MDPs) Environment and Actions\n",
    "\n",
    "- Fully Observable (Chess) vs Partially Observable (Poker)\n",
    "- Deterministic (Cart Pole) vs Stochastic (DeepTraffic)\n",
    "- Static (Chess) vs Dynamic (DeepTraffic)\n",
    "- Discrete (Chess) vs Continuous (Cart Pole)\n",
    "- Single Agent (Atari) vs Multi Agent (DeepTraffic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "------\n",
    "\n",
    "- Chapter 17 of [Artificial Intelligence: A Modern Approach, 4th edition](http://aima.cs.berkeley.edu/)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
