{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Hierarchical-Learning\" data-toc-modified-id=\"Hierarchical-Learning-1\">Hierarchical Learning</a></span></li><li><span><a href=\"#Beyond-MDPs\" data-toc-modified-id=\"Beyond-MDPs-2\">Beyond MDPs</a></span></li><li><span><a href=\"#Hierarchical-Learning\" data-toc-modified-id=\"Hierarchical-Learning-3\">Hierarchical Learning</a></span></li><li><span><a href=\"#Atari-Montezuma's-Revenge\" data-toc-modified-id=\"Atari-Montezuma's-Revenge-4\">Atari Montezuma's Revenge</a></span></li><li><span><a href=\"#Reinforcement-Learning-vs--Montezuma's-Revenge\" data-toc-modified-id=\"Reinforcement-Learning-vs--Montezuma's-Revenge-5\">Reinforcement Learning vs  Montezuma's Revenge</a></span></li><li><span><a href=\"#How-was-Montezuma's-Revenge-actually-solved?\" data-toc-modified-id=\"How-was-Montezuma's-Revenge-actually-solved?-6\">How was Montezuma's Revenge actually solved?</a></span></li><li><span><a href=\"#Why-is-expert-augmentation-this-important?\" data-toc-modified-id=\"Why-is-expert-augmentation-this-important?-7\">Why is expert augmentation this important?</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-8\">Sources of Inspiration</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Hierarchical Learning</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Beyond MDPs</h2></center>\n",
    "\n",
    "- Partially Observable Markov Decision Process (POMDP)\n",
    "- Hierarchical learning\n",
    "- Intrinsic rewards\n",
    "- Multi-agent\n",
    "- Real-world / production\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Hierarchical Learning</h2></center>\n",
    "\n",
    "- Define and accomplish subgoals that build to overall goal(s).\n",
    "- Globally non-greedy.\n",
    "- Better at handling credit assignment problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Atari Montezuma's Revenge</h2></center>\n",
    "\n",
    "<center><img src=\"images/1_UB4n5q9HWfUsLOA6R-u1rA.png\" width=\"75%\"/></center>\n",
    "\n",
    "[wikipedia's explanation](https://en.wikipedia.org/wiki/Montezuma%27s_Revenge_(video_game))\n",
    "\n",
    "Relatively sparse rewards - the agent only receives reward signals after completing specific series of actions over extended periods of time.\n",
    "\n",
    "How to get the first key in the first room of Montezumaâ€™s Revenge:\n",
    "\n",
    "1. Jump across an open space\n",
    "1. Descend a ladder\n",
    "1. Jump across an open space using a rope\n",
    "1. Descend another ladder\n",
    "1. Jump over a moving enemy\n",
    "1. Finally climbing another ladder\n",
    "\n",
    "If the agent is just learning from pixels, that means doing all of that just to get a binary change from `no key` to `key` on the screen.\n",
    "\n",
    "It is impossible for Q-learning with random exploration to learn it.\n",
    "\n",
    "[Try it online!](https://www.retrogames.cz/play_124-Atari2600.php?language=EN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/1535202184918.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Reinforcement Learning vs  Montezuma's Revenge</h2></center>\n",
    "\n",
    "Typical ways, aka DQN didn't work:\n",
    "\n",
    "<center><img src=\"images/dqn_results.png\" width=\"75%\"/></center>\n",
    "\n",
    "[Human-level control through deep reinforcement learning by Mnih et al](https://daiwk.github.io/assets/dqn.pdf)\n",
    "\n",
    "-------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>How was Montezuma's Revenge actually solved?</h2></center>\n",
    "\n",
    "\"Expert augmented\" strategies from OpenAI, DeepMind, and UberAI were able to beat human level performance.\n",
    "\n",
    "1. Humans played the game.\n",
    "1. That recording was feed in the Reinforcement Learning system.\n",
    "1. That bootstraped learning.\n",
    "\n",
    "\n",
    "\n",
    "[Source](https://openai.com/blog/learning-montezumas-revenge-from-a-single-demonstration/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Why is expert augmentation important?</h2></center>\n",
    "\n",
    "Bootstrapping Reinforcement Learning learning based on human knowledge is one promising direction for the future of applied Reinforcement Learning.\n",
    "\n",
    "Instead of Artificial Intelligence (AI), it will be Intelligence Augmented (IA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- https://github.com/yandexdataschool/Practical_RL/tree/spring19/week08_pomdp\n",
    "- Artificial Intelligence: A Modern Approach, 4th edition, section 17.4\n",
    "    - https://github.com/aimacode/aima-python/blob/master/mdp.ipynb\n",
    "- https://en.wikipedia.org/wiki/Particle_filter\n",
    "- https://www.researchgate.net/figure/Illustration-of-the-particle-filter-with-covariance-resampling-The-green-bars-show-the_fig1_331415631"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
