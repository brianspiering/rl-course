{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Actor-Critic-Model-for-Reinforcement-Learning\" data-toc-modified-id=\"Actor-Critic-Model-for-Reinforcement-Learning-1\">Actor-Critic Model for Reinforcement Learning</a></span></li><li><span><a href=\"#Actor-Critic-(AC)-Model-Overview\" data-toc-modified-id=\"Actor-Critic-(AC)-Model-Overview-2\">Actor-Critic (AC) Model Overview</a></span></li><li><span><a href=\"#AC-Concept-Map\" data-toc-modified-id=\"AC-Concept-Map-3\">AC Concept Map</a></span></li><li><span><a href=\"#-AC-Algorithm-\" data-toc-modified-id=\"-AC-Algorithm--4\"> AC Algorithm </a></span></li><li><span><a href=\"#AC-compared-to-other-RL-algorithms\" data-toc-modified-id=\"AC-compared-to-other-RL-algorithms-5\">AC compared to other RL algorithms</a></span></li><li><span><a href=\"#Actor-Critic-in-Gridworld\" data-toc-modified-id=\"Actor-Critic-in-Gridworld-6\">Actor-Critic in Gridworld</a></span></li><li><span><a href=\"#AC-Modern-(aka,-Bayesian)-Approach\" data-toc-modified-id=\"AC-Modern-(aka,-Bayesian)-Approach-7\">AC Modern (aka, Bayesian) Approach</a></span></li><li><span><a href=\"#AC-Formalism\" data-toc-modified-id=\"AC-Formalism-8\">AC Formalism</a></span></li><li><span><a href=\"#What-are-the-advantages-of-AC?\" data-toc-modified-id=\"What-are-the-advantages-of-AC?-9\">What are the advantages of AC?</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-10\">Takeaways</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-11\">Sources of Inspiration</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Actor-Critic Model for Reinforcement Learning</h2></center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Define actor-critic model in your own words.\n",
    "- Explain how the actor-critic model learns both the value and policy.\n",
    "- List the steps in applying actor-model algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Actor-Critic (AC) Model Overview</h2></center>\n",
    "\n",
    "<center><img src=\"images/ac.png\" width=\"45%\"/></center>\n",
    "\n",
    "- The actor\n",
    "    - Takes in a state, outputs the best action \n",
    "    - Parameterized / approximate policy\n",
    "- The critic\n",
    "    - Takes in an action and state, output the expected return from taking that action\n",
    "    - Parameterized / approximate value \n",
    "\n",
    "Actor and critic are explicit and separate both in representation and learning.\n",
    "\n",
    "The actor is typically trained with Policy Gradient (PG).\n",
    "\n",
    "The critic is typically trained with Temporal difference (TD) learning Value Function Approximation (VFA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>AC Concept Map</h2></center>\n",
    "\n",
    "<center><img src=\"images/ac_concept_map.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> AC Algorithm </h2></center>\n",
    "\n",
    "1. Produce the action $a_t$ for the current state $s_t$.\n",
    "1. Observe next state $s_{t+1}$ and the reward $r$.\n",
    "1. Update the utility of state $s_t$ (critic).\n",
    "1. Update the probability of the action $a_t$ (actor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>AC compared to other RL algorithms</h2></center>\n",
    "\n",
    "<center><img src=\"images/reinforcement_learning_model_free_active_actor_critic_scheme_actor_only_critic_only.png\" width=\"75%\"/></center>\n",
    "\n",
    "Critic-only: Learning based on value functions\n",
    "\n",
    "Actor-only: Direct policy search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Actor-Critic in Gridworld</h2></center>\n",
    "\n",
    "<center><img src=\"images/reinforcement_learning_model_free_active_actor_critic_neural_implementation_outcome.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><img src=\"images/reinforcement_learning_model_free_active_actor_critic_robot_actor_critic.png\" width=\"75%\"/></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>AC Modern (aka, Bayesian) Approach</h2></center>\n",
    "\n",
    "<center><img src=\"images/bayesian_actor_critic.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Image source](http://ai.stanford.edu/blog/acteach/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>AC Formalism</h2></center>\n",
    "\n",
    "\n",
    "<center><img src=\"images/formalism.png\" width=\"75%\"/></center>\n",
    "\n",
    "[Source](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_5_actor_critic_pdf) ðŸ‘ˆ Suggested reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are the advantages of AC?</h2></center>\n",
    "\n",
    "1. Variance reduction in function approximation\n",
    "\n",
    "    Similar to Double Q-learning, an external supervisor (critic) reduce the variance respect to pure actor-only methods. \n",
    "\n",
    "1. Computationally efficiency\n",
    "\n",
    "    AC methods can represent the policy in a separate structure and use it to find the best action. This is important to learning in continuous action spaces.\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Actor-critic (AC) models learns both the value and policy at the same time, but separately.\n",
    "- Since value and policy are separate, different function approximators / parameterization can be used for value and policy. \n",
    "- Given the flexibility of AC, it is very common for complex, applied problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_5_actor_critic_pdf\n",
    "- http://incompleteideas.net/book/first/ebook/node66.html\n",
    "- https://mpatacchiola.github.io/blog/2017/02/11/dissecting-reinforcement-learning-4.html\n",
    "- [Reinforcement Learning in a Nutshell by Heidrich-Meisner et al.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.9557&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
