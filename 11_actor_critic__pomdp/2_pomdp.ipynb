{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Partially-Observable-Markov-Decision-Process-(POMDP)\" data-toc-modified-id=\"Partially-Observable-Markov-Decision-Process-(POMDP)-1\">Partially Observable Markov Decision Process (POMDP)</a></span></li><li><span><a href=\"#Markov-decision-process-(MDP)\" data-toc-modified-id=\"Markov-decision-process-(MDP)-2\">Markov decision process (MDP)</a></span></li><li><span><a href=\"#Partially-Observable-Markov-Decision-Process-(POMDP)\" data-toc-modified-id=\"Partially-Observable-Markov-Decision-Process-(POMDP)-3\">Partially Observable Markov Decision Process (POMDP)</a></span></li><li><span><a href=\"#Example-of-POMDP\" data-toc-modified-id=\"Example-of-POMDP-4\">Example of POMDP</a></span></li><li><span><a href=\"#Model-free-vs-Partially-Observable\" data-toc-modified-id=\"Model-free-vs-Partially-Observable-5\">Model-free vs Partially Observable</a></span></li><li><span><a href=\"#Model-free-vs-Partially-Observable\" data-toc-modified-id=\"Model-free-vs-Partially-Observable-6\">Model-free vs Partially Observable</a></span></li><li><span><a href=\"#Real-world-problems-are-POMDPs-\" data-toc-modified-id=\"Real-world-problems-are-POMDPs--7\">Real-world problems are POMDPs </a></span></li><li><span><a href=\"#-POMDP-formalism\" data-toc-modified-id=\"-POMDP-formalism-8\"> POMDP formalism</a></span></li><li><span><a href=\"#Example-of-lidar-for-self-driving-cars-\" data-toc-modified-id=\"Example-of-lidar-for-self-driving-cars--9\">Example of lidar for self driving cars </a></span></li><li><span><a href=\"#Optimal-Policy-under-uncertainty\" data-toc-modified-id=\"Optimal-Policy-under-uncertainty-10\">Optimal Policy under uncertainty</a></span></li><li><span><a href=\"#-POMDP-are-naturally-continuous-&amp;-abstract-model\" data-toc-modified-id=\"-POMDP-are-naturally-continuous-&amp;-abstract-model-11\"> POMDP are naturally continuous &amp; abstract model</a></span></li><li><span><a href=\"#Particle-Filter-for-Belief-State-Estimation\" data-toc-modified-id=\"Particle-Filter-for-Belief-State-Estimation-12\">Particle Filter for Belief State Estimation</a></span></li><li><span><a href=\"#How-Particle-Filtering-Works\" data-toc-modified-id=\"How-Particle-Filtering-Works-13\">How Particle Filtering Works</a></span></li><li><span><a href=\"#Particle-Filter-Example\" data-toc-modified-id=\"Particle-Filter-Example-14\">Particle Filter Example</a></span></li><li><span><a href=\"#Limitations-of-POMDPs\" data-toc-modified-id=\"Limitations-of-POMDPs-15\">Limitations of POMDPs</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-16\">Takeaways</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-17\">Sources of Inspiration</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Partially Observable Markov Decision Process (POMDP)</h2></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Explain how a Partially Observable Markov Decision Process (POMDP) extend a traditional Markov decision process (MDP).\n",
    "- Explain the difference between model-free and partially observable.\n",
    "- Explain belief state in your own words.\n",
    "- List the steps in applying Particle Filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Markov decision process (MDP)</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/mdp.png\" width=\"75%\"/></center>\n",
    "\n",
    "2 kinds of state:\n",
    "\n",
    "1. Environmentâ€™s actual / internal state $s_{actual}$\n",
    "1. Agent's observation of state $s_{observed}$\n",
    "\n",
    "\n",
    "\n",
    "In an MDP, $s_{observed}$ == $s_{actual}$\n",
    "\n",
    "The same state information that is used to transition an environment into the next state is also made available to an agent. The environment is fully observable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Partially Observable Markov Decision Process (POMDP)</h2></center>\n",
    "\n",
    "The observed state may differ from the environmentâ€™s actual state, $s_{observed}$ â‰  $s_{actual}$. \n",
    "\n",
    "In this case, the environment is described as a partially observable MDP (POMDP) because the state observed by the agent only contains partial information about the state of the environment.\n",
    "\n",
    "__The agent only learns $s_{observed}$__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Example of POMDP</h2></center>\n",
    "\n",
    "<center><img src=\"images/breakout.png\" width=\"75%\"/></center>\n",
    "\n",
    "In  Atari Breakout, a single frame is a POMDP \n",
    "\n",
    "Observed:\n",
    "\n",
    "- object positions (bricks, paddle, ball)\n",
    "- Score\n",
    "- Lives\n",
    "\n",
    "Unobserved:\n",
    "\n",
    "- Ball velocity (direction and speed)\n",
    "\n",
    "Velocities would be included in the environmentâ€™s internal state since they are required to determine the next state given an action.\n",
    "\n",
    "The model needs to generate a belief about the ball's velocity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Model-free vs Partially Observable</h2></center>\n",
    "\n",
    "In small groups, fill in this table with examples:\n",
    "\n",
    "\n",
    "|         | Fully Observable | Partially Observable |  \n",
    "|:-------:|:------:|:------:|\n",
    "| Model-based | ? | ? |\n",
    "| Model-free  | ? | ? | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Model-free vs Partially Observable</h2></center>\n",
    "\n",
    "\n",
    "|         | Fully Observable | Partially Observable |  \n",
    "|:-------:|:------:|:------:|\n",
    "| Model-based | Chess & Go | Poker |\n",
    "| Model-free  | Warehouse robot | Self-driving cars| \n",
    "\n",
    "\n",
    "Model-based means the agent knows (a priori) or learns direct knowledge of the probability transition function $p(s`|s, a). The probability transition function is the environment dynamics, aka \"the rules of the game.\" Agents try to understands the world and create a model to represent it. \n",
    "\n",
    "Model-free means does __not__ know or learns  direct knowledge of the probability transition function. Instead, the agents directly learns value or policy. Just do, no extra knowledge.\n",
    "\n",
    "Warehouse robot needs to act correctly and can see all relevant data (fully instrumented). However, the environment dynamics might be too complex or change too frequently to back it valuable to learn the probability transition function.\n",
    "\n",
    "Whereas, partially observable is knowledge of current state $s$.\n",
    "\n",
    "Self-driving cars can not get all relevant data and the environment dynamics might be too complex or change too frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Real-world problems are POMDPs </h2></center>\n",
    "\n",
    "For example, any robot ðŸ¤– cannot perfectly observe its state:\n",
    "\n",
    "- Model error\n",
    "- Environment noise\n",
    "- Sensor limitations\n",
    "- Data representation limits (e.g., the world is analog and computers are digital and binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> POMDP formalism</h2></center>\n",
    "\n",
    "Take a regular MDP and add a sensor model:\n",
    "\n",
    "$$ P(e | s) $$\n",
    "\n",
    "Specify the probability of perceiving evidence $e$ in state $s$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Example of lidar for self driving cars </h2></center>\n",
    "<br>\n",
    "<center><img src=\"https://eijournal.com/wp-content/uploads/2012/01/VELODYNE-IMAGE.jpg\" width=\"75%\"/></center>\n",
    "\n",
    "Think about \n",
    "\"Is that thing a tree or very skinny person?\"\n",
    "\n",
    "The agent create a __belief state__, a set of actual possible state the agents could be in.\n",
    "\n",
    "The belief state $b$ is probability distribution overall possible states. The probability of some states can be zero.\n",
    "\n",
    "State moves from a scalar to a probability distribution (very Bayesian).\n",
    "\n",
    "The new belief vector $b'(s')$ after an action $a$ on the belief vector $b(s)$ and the noting of evidence $e$ is:\n",
    "$$ b'(s') = \\alpha P(e\\ |\\ s') \\sum_s P(s'\\ | s, a) b(s)$$ \n",
    "\n",
    "where $\\alpha$ is a normalizing constant to retain the interpretation of $b$ as a probability distribution.\n",
    "\n",
    "This equation is just counts the sum of likelihoods of going to a state $s'$ from every possible state $s$, times the initial likelihood of being in each $s$. This is multiplied by the likelihood that the known evidence actually implies the new state $s'$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Optimal Policy under uncertainty</h2></center>\n",
    "\n",
    "An agents's optimal action only depends on the agent's current belief state.\n",
    "\n",
    "An optimal policy $Ï€^*(b)$ is mapping from beliefs to actions.\n",
    "\n",
    "Decision Recipe:\n",
    "\n",
    "1. Given current belief state $b$, execute best action a = Ï€^*(b)\n",
    "2. Observe the percept $e$\n",
    "3. Update current belief\n",
    "4. Repeat\n",
    "\n",
    "\n",
    "That is a Bayesian learning problem:\n",
    "\n",
    "1. Define prior (say uniform to start)\n",
    "2. Collect empirical evidence under current model\n",
    "3. Calculate posterior\n",
    "1. Repeat (Yesterday's posterior is today's prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> POMDP are naturally continuous & abstract model</h2></center>\n",
    "\n",
    "Belief is a probability distribution over all states.\n",
    "\n",
    "POMDP are solved in the belief state, not in the physical state model. \n",
    "\n",
    "The better the information an agent receives the better the policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Particle Filter for Belief State Estimation</h2></center>\n",
    "\n",
    "The \"filtering problem\" estimates the actual states in a dynamical systems when only noisy, partial observations can be made. \n",
    "\n",
    "The algorithm needs to be robust to random perturbations are present in the sensors as well as in the dynamical system. \n",
    "\n",
    "The objective is to compute the posterior distributions of the states of a Markov process, given partial and noisy  observations.\n",
    "\n",
    "__Particle Filtering__ uses a set of particles (also called samples) to represent the posterior distribution of some stochastic process given partial and/or noisy observations. \n",
    "\n",
    "In Particle Filtering, provide a well-established methodology for generating samples from the required distribution without requiring assumptions about the state-space model or the state distributions. The state-space model can be nonlinear and the initial state and noise distributions can take any form required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>How Particle Filtering Works</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/pf.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Particle Filter Steps</h2></center>\n",
    "\n",
    "Let's try to estimate the belief of being in one of three states ${s_a, s_b, s_c}$.\n",
    "\n",
    "For simplicity, they are ordered on a univariate number line.\n",
    "\n",
    "Use n particles or samples (say n=10 or 1,000) to represent distribution over hidden states.\n",
    "\n",
    "1. Sampling: Sample at time $t$ the next state for each particle. If we have no idea where agent is, take uniform sample across the range.\n",
    "\n",
    "2. Evidence weighting: Take an empirical sample, an noisy indicator. Assign weights to each particle proportional to the likelihood of making that observation. The particles are weighted according to the observation. At this point, some particles might have already negligible weight. \n",
    "\n",
    "3. Resample to generate a new distribution of particles.\n",
    "\n",
    "4. Repeat. The process is repeat with the posterior particle location becoming the new prior particle location.\n",
    "\n",
    "This is similar to Expectation-Maximization (EM) algorithm and Gaussian Mixture Model (GMM). However non-parametric, recursive Bayes filters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Limitations of POMDPs</h2></center>\n",
    "\n",
    "The increased uncertainty makes credit assignment problem even more troublesome.\n",
    "\n",
    "For a given reward, which belief states should be updated.\n",
    "\n",
    "Especially over long time scales. \n",
    "\n",
    "There is a need for hierarchical planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2>Takeaways</h2></center>\n",
    " \n",
    " - Traditional MDPs can be extended with Partially Observable Markov Decision Process (POMDP) where there is uncertiinty about state\n",
    "- Partially Observable Markov Decision Process (POMDP) extend a traditional Markov decision process (MDP) by modeling uncertainty in state knowledge.\n",
    "- Model-free means there is no a priori knowledge of the probability transition function. Whereas, partially observable is knowledge of current state.\n",
    "- Belief state is a generalized solution to uncertainty for state perception.\n",
    "- Particle Filters is a method for iteratively updating model uncertain perception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " <center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- https://github.com/yandexdataschool/Practical_RL/tree/spring19/week08_pomdp\n",
    "- Artificial Intelligence: A Modern Approach, 4th edition, section 17.4\n",
    "    - https://github.com/aimacode/aima-python/blob/master/mdp.ipynb\n",
    "- https://en.wikipedia.org/wiki/Particle_filter\n",
    "- https://www.researchgate.net/figure/Illustration-of-the-particle-filter-with-covariance-resampling-The-green-bars-show-the_fig1_331415631\n",
    "- [Particle Filter Explained without Equations](https://www.youtube.com/watch?v=aUkBa1zMKv4)\n",
    "- [Particle Filters Basic Idea from Udacity](https://www.youtube.com/watch?v=_LjBba2hnfk)\n",
    "- https://medium.com/@jonathan_hui/tracking-a-self-driving-car-with-particle-filter-ef61f622a3e9\n",
    "- youtube.com/watch?v=lzN18y_z6HQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
