{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Temporal-difference-(TD)-learning\" data-toc-modified-id=\"Temporal-difference-(TD)-learning-1\">Temporal difference (TD) learning</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Temporal-difference-(TD)-learning\" data-toc-modified-id=\"Temporal-difference-(TD)-learning-3\">Temporal difference (TD) learning</a></span></li><li><span><a href=\"#TD(0)-Sampling-&amp;-Bootstrapping\" data-toc-modified-id=\"TD(0)-Sampling-&amp;-Bootstrapping-4\">TD(0) Sampling &amp; Bootstrapping</a></span></li><li><span><a href=\"#TD(0)-Algorithm\" data-toc-modified-id=\"TD(0)-Algorithm-5\">TD(0) Algorithm</a></span></li><li><span><a href=\"#Why-is-TD-so-popular?\" data-toc-modified-id=\"Why-is-TD-so-popular?-6\">Why is TD so popular?</a></span></li><li><span><a href=\"#TD-biggest-limitation\" data-toc-modified-id=\"TD-biggest-limitation-7\">TD biggest limitation</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-8\">Takeaways</a></span></li><li><span><a href=\"#-Bonus-Material\" data-toc-modified-id=\"-Bonus-Material-9\"> Bonus Material</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-10\">Sources of Inspiration</a></span></li><li><span><a href=\"#TD-λ-&amp;-TD-Gammon\" data-toc-modified-id=\"TD-λ-&amp;-TD-Gammon-11\">TD λ &amp; TD-Gammon</a></span></li><li><span><a href=\"#TD-Gammon---Sources-of-Inspiration:\" data-toc-modified-id=\"TD-Gammon---Sources-of-Inspiration:-12\">TD-Gammon - Sources of Inspiration:</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Temporal difference (TD) learning</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Explain Temporal difference (TD) learning in your own words.\n",
    "- Explain how TD uses sampling and bootstrapping.\n",
    "- List the pros and cons of TD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Temporal difference (TD) learning</h2></center>\n",
    "\n",
    "- Temporal difference (TD) learning is a combination of Monte Carlo (MC) and Dynamic Programming (DP).\n",
    "- Like MC, Temporal difference (TD) learning learns directly from environment without a model.\n",
    "    - The data itself is the best model of the world.\n",
    "- Unlike MC, Temporal difference (TD) learning does not need to wait to end of episode to update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Like DP, TD use the expected values of next state to enrich the prediction and update estimates based on current state-reward pairs. Thus, TD's approximation is slightly less precise and less sample-based. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Temporal difference (TD) learning improves the value function / estimated utilities towards a local equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>TD(0) Sampling & Bootstrapping</h2></center>\n",
    "\n",
    "Sampling - Use a single, empirical observation to improve current understanding.\n",
    "\n",
    "Bootstrapping - Uses previous approximations to inform current understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/td_fig.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>TD(0) Algorithm</h2></center>\n",
    "\n",
    "$$V^π (s_t) = V^π(s_t) + α([r_{t}+γV^π(s_{t+1})]-V^π(s_t))$$\n",
    "\n",
    "- Value Estimation is at the heart of TD\n",
    "- Update the value function for a policy - $V^π(S_t)$   \n",
    "- $[r_{t}+γV^π(s_{t+1})]$ is the TD target  \n",
    "    - Use a single sample. There is no summation across all possible futures. \n",
    "- α is the learning rate hyperparameter. Typically, decayed over episodes.\n",
    "- Learn at each and every step we take, thus also called one-step TD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Demo of TD cliffworld](https://distill.pub/2019/paths-perspective-on-value-learning/)\n",
    "\n",
    "[Demo of TD for gridworld](https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Why is TD so popular?</h2></center>\n",
    "\n",
    "- TD methods do not require a model of the environment, only experience (the real world almost never gives you a full model).\n",
    "- TD  methods can be fully incremental (less to store and less compute).\n",
    "- TD is typically more efficient than MC. The more Markovian a world, the more TD will out perform MC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>TD biggest limitation</h2></center>\n",
    "\n",
    "- Updating $V^π(s_t)$ depends on previous values. Thus is sensitive to priors. Need a reasonable initial guess and mostly stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Temporal difference (TD) learning improves the value function by finding successfully better steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Bonus Material</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- https://mpatacchiola.github.io/blog/2017/01/29/dissecting-reinforcement-learning-3.html\n",
    "- https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#dynamic-programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>TD λ & TD-Gammon</h2></center>\n",
    "\n",
    "TD(λ) (\"TD lambda\") algorithm to create TD-Gammon, a human-competitive Backgammon playing program. He wrote an article describing the approach, which you can find here.\n",
    "\n",
    "TD(λ) was later extended to TDLeaf(λ), specifically to better deal with Minimax searches. TDLeaf(λ) has been used, for example, in the chess program KnightCap. You can read about TDLeaf in this paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "TD-Gammon - Sources of Inspiration:\n",
    "-----\n",
    "\n",
    "- https://en.wikipedia.org/wiki/TD-Gammon\n",
    "- bkgm.com/articles/tesauro/tdl.html\n",
    "- http://www.research.ibm.com/massive/tdl.html\n",
    "- https://arxiv.org/pdf/cs/9901001.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
