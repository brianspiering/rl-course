{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Monte-Carlo\" data-toc-modified-id=\"Monte-Carlo-1\">Monte Carlo</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Monte-Carlo-(MC)-Methods\" data-toc-modified-id=\"Monte-Carlo-(MC)-Methods-3\">Monte Carlo (MC) Methods</a></span></li><li><span><a href=\"#Monte-Carlo-(MC)-for-RL-Recipe\" data-toc-modified-id=\"Monte-Carlo-(MC)-for-RL-Recipe-4\">Monte Carlo (MC) for RL Recipe</a></span></li><li><span><a href=\"#MC-Policy\" data-toc-modified-id=\"MC-Policy-5\">MC Policy</a></span></li><li><span><a href=\"#Monte-Carlo-(MC)-Demo\" data-toc-modified-id=\"Monte-Carlo-(MC)-Demo-6\">Monte Carlo (MC) Demo</a></span></li><li><span><a href=\"#Monte-Carlo-(MC)-Demo-Results\" data-toc-modified-id=\"Monte-Carlo-(MC)-Demo-Results-7\">Monte Carlo (MC) Demo Results</a></span></li><li><span><a href=\"#Compare-DP-vs.-MC\" data-toc-modified-id=\"Compare-DP-vs.-MC-8\">Compare DP vs. MC</a></span></li><li><span><a href=\"#Monte-Carlo-does-not-make-the-Markov-assumption\" data-toc-modified-id=\"Monte-Carlo-does-not-make-the-Markov-assumption-9\">Monte Carlo does not make the Markov assumption</a></span></li><li><span><a href=\"#Monte-Carlo-is-very-inefficient\" data-toc-modified-id=\"Monte-Carlo-is-very-inefficient-10\">Monte Carlo is very inefficient</a></span></li><li><span><a href=\"#Discussion-Question:-In-general,-what-are-the-reasons-to-choose-a-biased-estimator-over-a-unbiased-estimator?\" data-toc-modified-id=\"Discussion-Question:-In-general,-what-are-the-reasons-to-choose-a-biased-estimator-over-a-unbiased-estimator?-11\">Discussion Question: In general, what are the reasons to choose a biased estimator over a unbiased estimator?</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-12\">Takeaways</a></span></li><li><span><a href=\"#-Bonus-Material\" data-toc-modified-id=\"-Bonus-Material-13\"> Bonus Material</a></span></li><li><span><a href=\"#Sources-of-Inspiration\" data-toc-modified-id=\"Sources-of-Inspiration-14\">Sources of Inspiration</a></span></li><li><span><a href=\"#Monte-Carlo-method-to-estimate-3.14-(the-other-π)\" data-toc-modified-id=\"Monte-Carlo-method-to-estimate-3.14-(the-other-π)-15\">Monte Carlo method to estimate 3.14 (the other π)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Monte Carlo</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Monte Carlo (MC) in general\n",
    "- How Monte Carlo (MC) can be allow to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Monte Carlo (MC) Methods</h2></center>\n",
    "\n",
    "Able to directly learn from experience rather than having to rely on the prior knowledge of the environment.\n",
    "\n",
    "Thus a __model-free algorithm__, aka data-driven\n",
    "\n",
    "> \"Being data-driven is like navigating by watching the rearview mirror.   \n",
    "> &nbsp;Being model-driven is like using GPS.\"   \n",
    "> — Nick Elprin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Monte Carlo (MC) for RL Recipe</h2></center>\n",
    "\n",
    "Goal: Find a good approximation of the Environment by aggregation rewards from complete runs.\n",
    "\n",
    "1. Start at the start state $S_0$\n",
    "1. The Agent moves randomly until a termination state is achieved.\n",
    "1.  For each episode, save 4 values: \n",
    "    1. The initial state\n",
    "    2. The actions taken\n",
    "    4. The final state\n",
    "    3. Total rewards received.\n",
    "   \n",
    "The best policy is the sequence of empirical actions that gives us the highest reward on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>MC Policy</h2></center>\n",
    "\n",
    "<center><img src=\"images/mc_sample.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Monte Carlo (MC) Demo</h2></center>\n",
    "\n",
    "<center><img src=\"https://mpatacchiola.github.io/blog/images/reinforcement_learning_model_free_monte_carlo_three_episodes_fast.gif\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Monte Carlo (MC) Demo Results</h2></center>\n",
    "\n",
    "<center><img src=\"https://mpatacchiola.github.io/blog/images/reinforcement_learning_model_free_monte_carlo_three_episodes_linear.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Compare DP vs. MC</h2></center>\n",
    "\n",
    "<center><img src=\"images/dp_vs_mc.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The Agent starts in (0, 0) for both. There is a limited number of steps to illustrate the differences.\n",
    "\n",
    "The utility estimations for two states are zero. \n",
    "\n",
    "This can be considered one of the limitations and at the same time one of the advantage of MC methods. \n",
    "\n",
    "The policy we are using, the transition probabilities, and the fact that the robot always start from the same position (bottom-left corner) are responsible of the wrong estimation for those states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " [Source](https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Monte Carlo does not make the Markov assumption</h2></center>\n",
    "\n",
    "DP makes the Markov assumption to eliminate the need to track history.\n",
    "\n",
    "MC has to track history so it implicitly model the any influence history has reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Monte Carlo is very inefficient</h2></center>\n",
    "\n",
    "In order to explore most options and get a robust estimate, it requires __many, many__ episodes.\n",
    "\n",
    "There is a need for high-quality simulators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Discussion Question: In general, what are the reasons to choose a biased estimator over a unbiased estimator?</h2></center>\n",
    "\n",
    "MC estimator is biased. But that might be okay...\n",
    "\n",
    "1. The biased estimator is more efficient.\n",
    "2. The biased estimator is easier to implement.\n",
    "3. The biased estimator has lower variance. In other word, an unbiased estimator might have unacceptable high variance.\n",
    "\n",
    "In the messy real-world, it okay to be a bit wrong (bias) if you can get pretty good stable solution quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Monte Carlo (MC) finds the best possible policy by repeatedly running complete episodes. \n",
    "- Since MC is entirely empirical, it does need a priori model of the environment.\n",
    "- Since MC has to run complete episodes, it does not need to make the Markov assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Bonus Material</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Sources of Inspiration</h2></center>\n",
    "\n",
    "- https://mpatacchiola.github.io/blog/2017/01/15/dissecting-reinforcement-learning-2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Monte Carlo method to estimate 3.14 (the other π)</h2></center>\n",
    "\n",
    "[Simulation in Python code](https://gist.github.com/louismullie/3769218)\n",
    "\n",
    "[Walkthrough Video](https://www.youtube.com/watch?v=X5kdy29rX1U)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
