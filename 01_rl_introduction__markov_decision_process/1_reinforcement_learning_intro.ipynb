{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Types-of-Machine-Learning\" data-toc-modified-id=\"Types-of-Machine-Learning¶-1\"><a href=\"#Types-of-Machine-Learning\" data-toc-modified-id=\"Types-of-Machine-Learning-1\" class=\"toc-item-highlight-select\">Types of Machine Learning</a></a></span></li><li><span><a href=\"#Introduction-to-Reinforcement-Learning\" data-toc-modified-id=\"Introduction-to-Reinforcement-Learning-2\">Introduction to Reinforcement Learning</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-3\">Learning Outcomes</a></span></li><li><span><a href=\"#Reinforcement-Learning-(RL)-is-the-new-awesomeness\" data-toc-modified-id=\"Reinforcement-Learning-(RL)-is-the-new-awesomeness-4\">Reinforcement Learning (RL) is the new awesomeness</a></span></li><li><span><a href=\"#What-is-Reinforcement-Learning?\" data-toc-modified-id=\"What-is-Reinforcement-Learning?-5\">What is Reinforcement Learning?</a></span></li><li><span><a href=\"#Reinforcement-Learning:-Another-Definition\" data-toc-modified-id=\"Reinforcement-Learning:-Another-Definition-6\">Reinforcement Learning: Another Definition</a></span></li><li><span><a href=\"#The-promise-and-peril-of-RL:-State\" data-toc-modified-id=\"The-promise-and-peril-of-RL:-State-7\">The promise and peril of RL: State</a></span></li><li><span><a href=\"#Reinforcement-learning-(RL)\" data-toc-modified-id=\"Reinforcement-learning-(RL)-8\">Reinforcement learning (RL)</a></span></li><li><span><a href=\"#Types-of-Machine-Learning\" data-toc-modified-id=\"Types-of-Machine-Learning-9\">Types of Machine Learning</a></span></li><li><span><a href=\"#What-are-example-tasks-and-algorithms-for-each-type-of-Machine-Learning?\" data-toc-modified-id=\"What-are-example-tasks-and-algorithms-for-each-type-of-Machine-Learning?-10\">What are example tasks and algorithms for each type of Machine Learning?</a></span></li><li><span><a href=\"#What-are-example-tasks-for-each-type-of-Machine-Learning?\" data-toc-modified-id=\"What-are-example-tasks-for-each-type-of-Machine-Learning?-11\">What are example tasks for each type of Machine Learning?</a></span></li><li><span><a href=\"#What-are-the-differences-between-Supervised,-Unsupervised,-and-Reinforcement-Learning?\" data-toc-modified-id=\"What-are-the-differences-between-Supervised,-Unsupervised,-and-Reinforcement-Learning?-12\">What are the differences between Supervised, Unsupervised, and Reinforcement Learning?</a></span></li><li><span><a href=\"#Student-Activity:-Fill-in-the-following-table\" data-toc-modified-id=\"Student-Activity:-Fill-in-the-following-table-13\">Student Activity: Fill in the following table</a></span></li><li><span><a href=\"#Comparison-Shopping:-RL,-UL,-&amp;-RL-\" data-toc-modified-id=\"Comparison-Shopping:-RL,-UL,-&amp;-RL--14\">Comparison Shopping: RL, UL, &amp; RL </a></span></li><li><span><a href=\"#Real-World-Machine-Learning\" data-toc-modified-id=\"Real-World-Machine-Learning-15\">Real World Machine Learning</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-16\">Check for understanding</a></span></li><li><span><a href=\"#Reinforcement-Learning-can-handle-sparse-and-time-delayed-labels,-aka-rewards.\" data-toc-modified-id=\"Reinforcement-Learning-can-handle-sparse-and-time-delayed-labels,-aka-rewards.-17\">Reinforcement Learning can handle <b>sparse</b> and <b>time-delayed</b> labels, aka rewards.</a></span></li><li><span><a href=\"#Supervised-learning-is-“teach-by-example”.\" data-toc-modified-id=\"Supervised-learning-is-“teach-by-example”.-18\">Supervised learning is “teach by example”.</a></span></li><li><span><a href=\"#Reinforcement-learning-is-“teach-by-experience”.\" data-toc-modified-id=\"Reinforcement-learning-is-“teach-by-experience”.-19\">Reinforcement learning is “teach by experience”.</a></span></li><li><span><a href=\"#Where-do-rewards-come-from?\" data-toc-modified-id=\"Where-do-rewards-come-from?-20\">Where do rewards come from?</a></span></li><li><span><a href=\"#The-goal-of-Reinforcement-Learning\" data-toc-modified-id=\"The-goal-of-Reinforcement-Learning-21\">The goal of Reinforcement Learning</a></span></li><li><span><a href=\"#Current-Trends-in-Machine-Learning\" data-toc-modified-id=\"Current-Trends-in-Machine-Learning-22\">Current Trends in Machine Learning</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-23\">Check for understanding</a></span></li><li><span><a href=\"#Why-are-games-useful-in-Reinforcement-Learning?\" data-toc-modified-id=\"Why-are-games-useful-in-Reinforcement-Learning?-24\">Why are games useful in Reinforcement Learning?</a></span></li><li><span><a href=\"#Defining-the-Reinforcement-Learning-Problem\" data-toc-modified-id=\"Defining-the-Reinforcement-Learning-Problem-25\">Defining the Reinforcement Learning Problem</a></span></li><li><span><a href=\"#PacMan-as-RL-problem\" data-toc-modified-id=\"PacMan-as-RL-problem-26\">PacMan as RL problem</a></span></li><li><span><a href=\"#RL-Problem-Definition:-SEA-SAR\" data-toc-modified-id=\"RL-Problem-Definition:-SEA-SAR-27\">RL Problem Definition: SEA-SAR</a></span></li><li><span><a href=\"#Student-Discussion\" data-toc-modified-id=\"Student-Discussion-28\">Student Discussion</a></span></li><li><span><a href=\"#What-are-Reinforcement-Learning-limitations?-\" data-toc-modified-id=\"What-are-Reinforcement-Learning-limitations?--29\">What are Reinforcement Learning limitations? </a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-30\">Takeaways</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-31\">Bonus Material</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-32\">Check for understanding</a></span></li><li><span><a href=\"#Reinforcement-Learning-for-SQL-Query-Optimization\" data-toc-modified-id=\"Reinforcement-Learning-for-SQL-Query-Optimization-33\">Reinforcement Learning for SQL Query Optimization</a></span></li><li><span><a href=\"#SQL-Query-Optimization\" data-toc-modified-id=\"SQL-Query-Optimization-34\">SQL Query Optimization</a></span></li><li><span><a href=\"#SQL-Query-Optimization-as-RL-problem\" data-toc-modified-id=\"SQL-Query-Optimization-as-RL-problem-35\">SQL Query Optimization as RL problem</a></span></li><li><span><a href=\"#Learning-a-join-order-enumerator\" data-toc-modified-id=\"Learning-a-join-order-enumerator-36\">Learning a join order enumerator</a></span></li><li><span><a href=\"#Learning-a-join-order-enumerator\" data-toc-modified-id=\"Learning-a-join-order-enumerator-37\">Learning a join order enumerator</a></span></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-38\">Results</a></span></li><li><span><a href=\"#Temporal-Credit-Assignment-Problem\" data-toc-modified-id=\"Temporal-Credit-Assignment-Problem-39\">Temporal Credit Assignment Problem</a></span></li><li><span><a href=\"#Why-aren't-we-using-X-framework?\" data-toc-modified-id=\"Why-aren't-we-using-X-framework?-40\">Why aren't we using X framework?</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Introduction to Reinforcement Learning</h2></center>\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://www1.vizury.com/blog/wp-content/uploads/2017/06/RL-article-image.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Compare Reinforcement Learning to Supervised Learning and Unsupervised Machine Learning.\n",
    "- Define the elements of Reinforcement Learning problem formation:\n",
    "    - Agent\n",
    "    - Environment\n",
    "    - State\n",
    "    - Action\n",
    "    - Reward\n",
    "- Identify those elements in a variety of Reinforcement Learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Reinforcement Learning (RL) is the new awesomeness</h2></center>\n",
    "\n",
    "<center><img src=\"https://ichef.bbci.co.uk/news/660/cpsprodpb/11B23/production/_88738427_pic1go.jpg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><h2>What is Reinforcement Learning?</h2></center>\n",
    "\n",
    "<center><img src=\"https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2016/04/aeloop-300x183.png\" width=\"75%\"/></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><img src=\"images/mdp.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<center><img src=\"images/figtmp7.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/model of RL.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Reinforcement Learning: Another Definition</h2></center>\n",
    "\n",
    "<br>\n",
    "<center><img src=\"images/trial.jpg\" width=\"35%\"/></center>\n",
    "\n",
    "<center>Learn to make a sequence of decisions, by trial & error, in order to achieve a goal.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>The promise and peril of RL: State</h2></center>\n",
    "\n",
    "<center><img src=\"images/mdp.png\" width=\"75%\"/></center>\n",
    "\n",
    "<center>RL can learn state.</center>\n",
    "\n",
    "<center>State makes programming hard.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Reinforcement learning (RL)</h2></center>\n",
    "<br>\n",
    "<center>A collection of algorithms that enable an agent(s) to learn a sequences of actions in an uncertain environment in order to maximize cumulative rewards.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Types of Machine Learning</h2></center>\n",
    "\n",
    "<center><img src=\"images/types.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are example tasks and algorithms for each type of Machine Learning?</h2></center>\n",
    "\n",
    "- Supervised Learning (SL) \n",
    "- Unsupervised Learning (UL) \n",
    "- Reinforcement (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are example tasks for each type of Machine Learning?</h2></center>\n",
    "\n",
    "- Supervised Learning (SL) - Classification & Regression\n",
    "- Unsupervised Learning (UL)  - Clustering, dimensionality reduction, representation learning, density estimation, …\n",
    "- RL - Wait for the rest of the course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/mind_map.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are the differences between Supervised, Unsupervised, and Reinforcement Learning?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity: Fill in the following table</h2></center>\n",
    "\n",
    "|  Compare:      | Supervised  | Unsupervised  | Reinforcement Learning | \n",
    "| :----: | :-----------: | :-------------: | :------: |  \n",
    "| What is the goal?   |              |                |         |\n",
    "| What are the type of labels?  |              |                |  |\n",
    "| Batch or Stream |      |                |  |\n",
    "\n",
    "\n",
    "Batch - Analyze all the data at the same time.   \n",
    "Stream - Analyze data element-by-element.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "|  Compare:      | Supervised  | Unsupervised  | Reinforcement Learning |  \n",
    "|:-------:|:-----------:|:-------------:|:------:|  \n",
    "| What is the goal?    |    Learn a function that maps input to output         |    Learn the latent structure of the data           |   Learn to maximize rewards     |\n",
    "| What are the type of labels?   |    Every data point has label         |   None            | Sparse & Time delayed |\n",
    "| Batch or Stream | Batch    |    Batch           |Stream |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://blogs.sas.com/content/subconsciousmusings/2017/09/25/machine-learning-concepts-styles-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/types of ml.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/past present future of ML.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/rl_decision_making.png\" width=\"70%\"/></center>\n",
    "\n",
    "Source - [Micheal Jordan](https://en.wikipedia.org/wiki/Michael_I._Jordan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/another_table.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> “The AI Revolution Will Not be Supervised”   \n",
    "> — Yann LeCun\n",
    "\n",
    "[Source](https://engineering.nyu.edu/news/revolution-will-not-be-supervised-promises-facebooks-yann-lecun-kickoff-ai-seminar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Comparison Shopping: RL, UL, & RL </h2></center>\n",
    "<br>\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/800/1*KDvA9Fq3lm-eQOyGlcKAKg.png\" width=\"75%\"/></center>\n",
    "\n",
    "[Source](https://www.voicetube.com/videos/58708)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Real World Machine Learning</h2></center>\n",
    "\n",
    "The real world is often:\n",
    "\n",
    "- Not normally distributed\n",
    "- Not is not Independent and identically distributed (i.i.d.) random variables \n",
    "- Not linear relationship between variables\n",
    "- Not stationary through time\n",
    "\n",
    "Your machine learning algorithms should be robust to these circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://medium.com/intuitionmachine/predictive-learning-is-the-key-to-deep-learning-acceleration-93e063195fd0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "Supervised Learning (SL) and Reinforcement Learning (RL) both need labeled data. With respect to labels, what is the difference between SL and RL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Supervised learning __requires__ every data point have a label for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Reinforcement Learning can handle <b>sparse</b> and <b>time-delayed</b> labels, aka rewards.</h2></center>\n",
    "\n",
    "<center><img src=\"images/rewards.jpg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Supervised learning is “teach by example”.</h2></center>\n",
    "\n",
    "<center><h2>Reinforcement learning is “teach by experience”.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Where do rewards come from?</h2></center>\n",
    "\n",
    "1. Either provided by some external mechanism (e.g. a human supervisor) \n",
    "\n",
    "1. Can be predefined as a function of state (e.g. distance to the goal state).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>The goal of Reinforcement Learning</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>An agent learns to <b>maximize cumulative rewards</b> (aka, return)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](http://louiskirsch.com/maps/reinforcement-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Current Trends in Machine Learning</h2></center>\n",
    "\n",
    "Making progress on fundamental problems:\n",
    "\n",
    "1. Perception - vision & audio signals\n",
    "2. Language - NLP, NLU\n",
    "3. Action - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Focus on fundamental problems in your field](https://www.cs.virginia.edu/~robins/YouAndYourResearch.html) is career advice from Richard Hamming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "<center><img src=\"https://www.siliconrepublic.com/wp-content/uploads/2014/12/201305/breakout2600-718x523.png\" width=\"55%\"/></center>\n",
    "\n",
    "__Why is RL useful framework for video games?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Well-defined environment\n",
    "- Simple actions\n",
    "- Clear, delayed rewards "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__What other domains are well-suited for Reinforcement Learning?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Games \n",
    "    - Board Games \n",
    "        - Chess\n",
    "        - Backgammon\n",
    "        - Go\n",
    "- Robotics \n",
    "    - warehouse robots\n",
    "    - factories\n",
    "    - vehicles\n",
    "        - ships\n",
    "        - planes\n",
    "        - drones\n",
    "        - self-driving cars\n",
    "- Industrial Process\n",
    "    - Overall factory flow\n",
    "    - Supply chain\n",
    "- System Optimization\n",
    "    - HVAC systems\n",
    "    - Data center topology\n",
    "- Advertising / Recommenders\n",
    "    - Videos\n",
    "    - Complex / dynamic systems\n",
    "- Finance\n",
    "    - Cutting-edge, automated trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](http://karpathy.github.io/2016/05/31/rl/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Why are games useful in Reinforcement Learning?</h2></center>\n",
    "\n",
    "Individual and small group behavior are a lot like games. There are rewards and interactions.\n",
    "\n",
    "Thus, games are a good model for Reinforcement Learning.\n",
    "\n",
    "(Large groups behave like economies.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:  Artificial Intelligence: A Modern Approach, p 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Defining the Reinforcement Learning Problem</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>PacMan as RL problem</h2></center>\n",
    "<br>\n",
    "<center><img src=\"https://images-na.ssl-images-amazon.com/images/I/71ipTP6sGFL._SY679_.jpg\" width=\"35%\"/></center>\n",
    "\n",
    "<center><a href=\"https://www.google.com/search?q=pacman&rlz=1C5CHFA_enUS836US836&oq=pacman&aqs=chrome..69i57.879j0j9&sourceid=chrome&ie=UTF-8\">PAC-MAN demo</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>RL Problem Definition: SEA-SAR</h2></center>\n",
    "\n",
    "1. Sequential - Time component, aka not batch\n",
    "1. Environment - Current Board\n",
    "    \n",
    "1. Agent - Pacman or Ms. Pacman (my favorite)\n",
    "    - Operate autonomously\n",
    "    - Perceive their environment\n",
    "    - Persist over a prolonged time period\n",
    "    - Adapt to changes \n",
    "    - Create and pursue goals\n",
    "1. State \n",
    "    - We can ask ourselves what does the agent (Pac-Man) need to know in order to make a good move? It needs to know its own position, positions of all the ghosts and position of all the food. Right? So, a good representation of the state might be a tuple containing values of\n",
    "    - Where am I?\n",
    "    - Where are the walls and paths?\n",
    "    - Where are the ghosts?\n",
    "    - Where are the pellets?\n",
    "    - What could be an efficient way to represent a state in this game? \n",
    "1. Action \n",
    "    - Our agent can move only in four directions in this 2D grid so our actions are {Up,Down,Left,Right}\n",
    "1. Reward / Objective\n",
    "    - An agent learns to maximize cumulative reward in an environment.\n",
    "    - Our end goal is to obtain maximum possible score in the game. So, after performing an action, the increase in score can be a good reward signal for our agent. Or, if your intention is to survive in the game as long as possible, time spent in the game can be a good reward signal for the agent.\n",
    "    - Positive - eat stuff, points go up\n",
    "    - Negative - die\n",
    "    - Long term\n",
    "        - short term - eat closest pellet near ghost\n",
    "        - long term - eat farther away pellets, live longer\n",
    "\n",
    "[Source](https://medium.com/code-heroku/introduction-to-reinforcement-learning-67826ec177ea)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Discussion</h2></center>\n",
    "\n",
    "In small groups, pick a specific task to apply RL (not a board or video game). \n",
    "\n",
    "Define use the SEA-SAR model:\n",
    "\n",
    "- How is it Sequential? (If not sequential, pick new task.)\n",
    "- What is the Environment?\n",
    "- What is the Objective?\n",
    "- What is the Agent?\n",
    "- What are example States?\n",
    "- What are example Actions?\n",
    "- What are example Rewards?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are Reinforcement Learning limitations? </h2></center>\n",
    "\n",
    "In general, what makes RL difficult?\n",
    "\n",
    "Specifically, when would an agent not learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Violate one of the constraints__:\n",
    "\n",
    "- Agent\n",
    "    - Can not take action that would lead to a reward\n",
    "    - Can not process reward\n",
    "- Rewards\n",
    "    - Not enough rewards (very low rate or \"binary\")\n",
    "    - Rewards are not based on agent performance (random or biased)\n",
    "    - Rewards are too delayed or hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Reinforcement Learning is a sub-field of Machine Learning that is becoming increasingly popular.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Reinforcement Learning: An agent learns to maximize cumulative rewards in an environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- RL can learn sparse, time-delayed rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Every Reinforcement Learning problem has the following elements:\n",
    "    - Sequential\n",
    "    - Environment\n",
    "    - Agent\n",
    "    - State\n",
    "    - Action\n",
    "    - Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- One of the greatest challenges in applying RL to real-world problems is deciding how to represent those elements within the existing RL frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bonus Material</h2></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "In function optimization terms, what is \"the best reward\"?\n",
    "\n",
    "In function optimization terms, what if the best reward is hard to get to and you have settle for lower value rewards?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. __Global minimum__ is the best rewards.\n",
    "\n",
    "2. __Local minimum__ are easier to find but lower value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Reinforcement Learning for SQL Query Optimization</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```sql\n",
    "SELECT Customers.CustomerID, Customers.Name, Sales.LastSaleDate\n",
    "FROM Customers, Sales\n",
    "WHERE Customers.CustomerID = Sales.CustomerID\n",
    "```\n",
    "\n",
    "Is this an optimal query?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This type of join creates a Cartesian Join, also called a Cartesian Product or CROSS JOIN. In a Cartesian Join, all possible combinations of the variables are created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```sql\n",
    "SELECT Customers.CustomerID, Customers.Name, Sales.LastSaleDate\n",
    "FROM Customers\n",
    "   INNER JOIN Sales\n",
    "   ON Customers.CustomerID = Sales.CustomerID\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To prevent creating a Cartesian Join, INNER JOIN should be used instead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>SQL Query Optimization</h2></center>\n",
    "\n",
    "<center><img src=\"https://docs.oracle.com/database/121/TGSQL/img/GUID-22630970-B584-41C9-B104-200CEA2F4707-default.gif\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>SQL Query Optimization as RL problem</h2></center>\n",
    "\n",
    "__SEA-SAR__:\n",
    "\n",
    "- How is it Sequential? (If not sequential, pick new task)\n",
    "    - Yes. The query execute items in order and does it over-and-over\n",
    "- What is the Environment?\n",
    "    - The entire database system\n",
    "- What is the Agent?\n",
    "    - The optimizer \n",
    "- What are example States?\n",
    "    - The current query plan\n",
    "- What are example Actions?\n",
    "    - Represents an individual change to the query plan\n",
    "- What are example Rewards? What is objective?\n",
    "    - Effective and efficient SQL queries\n",
    "    - Reduce query steps\n",
    "    - Less data being moved from disk\n",
    "    - Lower execution time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning a join order enumerator</h2></center>\n",
    "\n",
    "<center><img src=\"https://adriancolyer.files.wordpress.com/2019/01/DL-Optimizer-Fig-1.jpeg?w=480\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>Learning a join order enumerator</h2></center>\n",
    "<br>\n",
    "<center><img src=\"https://adriancolyer.files.wordpress.com/2019/01/DL-Optimizer-Fig-2.jpeg?w=640\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Results</h2></center>\n",
    "<center><img src=\"https://adriancolyer.files.wordpress.com/2019/01/DL-Optimizer-Fig-3a.jpeg?w=420\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Please read the following:\n",
    "\n",
    "- [Blogpost](https://blog.acolyer.org/2019/01/18/towards-a-hands-free-query-optimizer-through-deep-learning/)\n",
    "- [Technical Paper](http://cidrdb.org/cidr2019/papers/p96-marcus-cidr19.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Temporal Credit Assignment Problem\n",
    "-----\n",
    "\n",
    "Which of the preceding actions was responsible for getting the reward and to what extent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The biggest problem facing a reinforcement-learning agent.\n",
    "\n",
    "One strategy is to wait until the \"end\" and reward the actions taken if the result was good and punish them if the result was bad. \n",
    "\n",
    "In ongoing tasks, it is difficult to know what the \"end\" is, and this might require a great deal of memory. \n",
    "\n",
    "Instead, we will use insights from value iteration to adjust the estimated value of a state based on the immediate reward and the estimated value of the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Why aren't we using X framework?</h2></center>\n",
    "\n",
    "There is a tension between building vs using using frameworks.\n",
    " \n",
    "This class is about building a deep understanding, thus using frameworks encourages you skip understanding.\n",
    "\n",
    "Why aren't we using Ray from UC Berkeley’s RISElab?\n",
    "\n",
    "1. See above\n",
    "1. Too new\n",
    "2. Designed for multi-agent reinforcement learning which is out-of-scope\n",
    "\n",
    "You can use any framework on your Final Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
